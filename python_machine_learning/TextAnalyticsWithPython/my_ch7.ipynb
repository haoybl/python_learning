{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exploring WordNet\n",
    "### Understanding Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term = 'fruit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "synsets = wn.synsets(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('fruit.n.01'),\n",
       " Synset('yield.n.03'),\n",
       " Synset('fruit.n.03'),\n",
       " Synset('fruit.v.01'),\n",
       " Synset('fruit.v.02')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset:Synset('fruit.n.01')\n",
      "Part of speech:noun.plant\n",
      "Definition:the ripened reproductive body of a seed plant\n",
      "Lemmas:['fruit']\n",
      "Examples:[]\n",
      "--------------------------------------------------\n",
      "Synset:Synset('yield.n.03')\n",
      "Part of speech:noun.artifact\n",
      "Definition:an amount of a product\n",
      "Lemmas:['yield', 'fruit']\n",
      "Examples:[]\n",
      "--------------------------------------------------\n",
      "Synset:Synset('fruit.n.03')\n",
      "Part of speech:noun.event\n",
      "Definition:the consequence of some effort or action\n",
      "Lemmas:['fruit']\n",
      "Examples:['he lived long enough to see the fruit of his policies']\n",
      "--------------------------------------------------\n",
      "Synset:Synset('fruit.v.01')\n",
      "Part of speech:verb.creation\n",
      "Definition:cause to bear fruit\n",
      "Lemmas:['fruit']\n",
      "Examples:[]\n",
      "--------------------------------------------------\n",
      "Synset:Synset('fruit.v.02')\n",
      "Part of speech:verb.creation\n",
      "Definition:bear fruit\n",
      "Lemmas:['fruit']\n",
      "Examples:['the trees fruited early this year']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets:\n",
    "    print(\"Synset:{}\\nPart of speech:{}\\nDefinition:{}\\nLemmas:{}\\nExamples:{}\".format(synset,\n",
    "                                                                                           synset.lexname(),\n",
    "                                                                                           synset.definition(),\n",
    "                                                                                           synset.lemma_names(),\n",
    "                                                                                           synset.examples()))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Lexical Semantic Relations\n",
    "### Entailments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('walk.v.01') --entails--> [Synset('step.v.01')]\n",
      "Synset('eat.v.01') --entails--> [Synset('chew.v.01'), Synset('swallow.v.01')]\n",
      "Synset('digest.v.01') --entails--> [Synset('consume.v.02')]\n"
     ]
    }
   ],
   "source": [
    "for action in ['walk', 'eat', 'digest']:\n",
    "    action_syn = wn.synsets(action, pos='v')[0]\n",
    "    print(\"{} --entails--> {}\".format(action_syn, action_syn.entailments()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homonyms and Homographs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank.n.01 - sloping land (especially the slope beside a body of water)\n",
      "depository_financial_institution.n.01 - a financial institution that accepts deposits and channels the money into lending activities\n",
      "bank.n.03 - a long ridge or pile\n",
      "bank.n.04 - an arrangement of similar objects in a row or in tiers\n",
      "bank.n.05 - a supply or stock held in reserve for future use (especially in emergencies)\n",
      "bank.n.06 - the funds held by a gambling house or the dealer in some gambling games\n",
      "bank.n.07 - a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "savings_bank.n.02 - a container (usually with a slot in the top) for keeping money at home\n",
      "bank.n.09 - a building in which the business of banking transacted\n",
      "bank.n.10 - a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "bank.v.01 - tip laterally\n",
      "bank.v.02 - enclose with a bank\n",
      "bank.v.03 - do business with a bank or keep an account at a bank\n",
      "bank.v.04 - act as the banker in a game or in gambling\n",
      "bank.v.05 - be in the banking business\n",
      "deposit.v.02 - put into a bank account\n",
      "bank.v.07 - cover with ashes so to control the rate of burning\n",
      "trust.v.01 - have confidence or faith in\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('bank'):\n",
    "    print(\"{} - {}\".format(synset.name(), synset.definition()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym: large.a.01\n",
      "Definition: above average in size or number or quantity or magnitude or extent\n",
      "Antonym: small.a.01\n",
      "Definition: limited or below average in number or quantity or magnitude or extent\n"
     ]
    }
   ],
   "source": [
    "term = 'large'\n",
    "synsets = wn.synsets(term, pos='a')\n",
    "\n",
    "synset_adj = synsets[0]\n",
    "adj_synonym = synset_adj.lemmas()[1].synset()\n",
    "adj_antonym = synset_adj.lemmas()[1].antonyms()[0].synset()\n",
    "\n",
    "print(\"Synonym: {}\".format(adj_synonym.name()))\n",
    "print(\"Definition: {}\".format(adj_synonym.definition()))\n",
    "print('Antonym: {}'.format(adj_antonym.name()))\n",
    "print(\"Definition: {}\".format(adj_antonym.definition()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('distrust.v.01.distrust'), Lemma('distrust.v.01.mistrust')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset.lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('large.a.01.large'), Lemma('large.a.01.big')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_adj.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym: rich_people.n.01\n",
      "Definition: people who have possessions and wealth (considered as a group)\n",
      "Synonym: poor_people.n.01\n",
      "Definition: people without possessions or wealth (considered as a group)\n",
      "--------------------------------------------------\n",
      "Synonym: rich.a.01\n",
      "Definition: possessing material wealth\n",
      "Synonym: poor.a.02\n",
      "Definition: having little money or few possessions\n",
      "--------------------------------------------------\n",
      "Synonym: rich.a.02\n",
      "Definition: having an abundant supply of desirable qualities or substances (especially natural resources)\n",
      "Synonym: poor.a.04\n",
      "Definition: lacking in specific resources, qualities or substances\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "term = 'rich'\n",
    "synsets = wn.synsets(term)[:3]\n",
    "\n",
    "for synset in synsets:\n",
    "    \n",
    "    lemma = synset.lemmas()[0]\n",
    "    synonym = lemma.synset()\n",
    "    antonym = lemma.antonyms()[0].synset()\n",
    "    \n",
    "    print(\"Synonym: {}\".format(synonym.name()))\n",
    "    print(\"Definition: {}\".format(synonym.definition()))\n",
    "    print(\"Synonym: {}\".format(antonym.name()))\n",
    "    print(\"Definition: {}\".format(antonym.definition()))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyponyms and Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tree.n.01\n",
      "Definition: a tall perennial woody plant having a main trunk and branches forming a distinct elevated crown; includes both gymnosperms and angiosperms\n"
     ]
    }
   ],
   "source": [
    "term = 'tree'\n",
    "ss = wn.synsets(term)[0]\n",
    "\n",
    "print(\"Name:\", ss.name())\n",
    "print(\"Definition:\", ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Hyponyms: 180\n",
      "\n",
      "aalii.n.01 - a small Hawaiian tree with hard dark wood\n",
      "\n",
      "acacia.n.01 - any of various spiny trees or shrubs of the genus Acacia\n",
      "\n",
      "african_walnut.n.01 - tropical African timber tree with wood that resembles mahogany\n",
      "\n",
      "albizzia.n.01 - any of numerous trees of the genus Albizia\n",
      "\n",
      "alder.n.02 - north temperate shrubs or trees having toothed leaves and conelike fruit; bark is used in tanning and dyeing and the wood is rot-resistant\n",
      "\n",
      "angelim.n.01 - any of several tropical American trees of the genus Andira\n",
      "\n",
      "angiospermous_tree.n.01 - any tree having seeds and ovules contained in the ovary\n",
      "\n",
      "anise_tree.n.01 - any of several evergreen shrubs and small trees of the genus Illicium\n",
      "\n",
      "arbor.n.01 - tree (as opposed to shrub)\n",
      "\n",
      "aroeira_blanca.n.01 - small resinous tree or shrub of Brazil\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hyponyms = ss.hyponyms()\n",
    "print(\"Total Hyponyms:\", len(hyponyms))\n",
    "print()\n",
    "for h in hyponyms[:10]:\n",
    "    print(\"{} - {}\".format(h.name(), h.definition()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypernym = ss.hypernyms()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('woody_plant.n.01')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entity.n.01 -> physical_entity.n.01 -> object.n.01 -> whole.n.02 -> living_thing.n.01 -> organism.n.01 -> plant.n.02 -> vascular_plant.n.01 -> woody_plant.n.01 -> tree.n.01'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" -> \".join(s.name() for s in ss.hypernym_paths()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holonyms and Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term = 'tree'\n",
    "tree = wn.synsets(term)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest.n.01 - the trees and other plants in a large densely wooded area\n"
     ]
    }
   ],
   "source": [
    "tree_holonym = tree.member_holonyms()[0]\n",
    "print(\"{} - {}\".format(tree_holonym.name(), tree_holonym.definition()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burl.n.02 - a large rounded outgrowth on the trunk or branch of a tree\n",
      "\n",
      "crown.n.07 - the upper branches and leaves of a tree or other plant\n",
      "\n",
      "limb.n.02 - any of the main branches arising from the trunk or a bough of a tree\n",
      "\n",
      "stump.n.01 - the base part of a tree that remains standing after the tree has been felled\n",
      "\n",
      "trunk.n.01 - the main stem of a tree; usually covered with bark; the bole is usually the part that is commercially useful for lumber\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_meronym = tree.part_meronyms()\n",
    "for m in tree_meronym:\n",
    "    print(\"{} - {}\".format(m.name(), m.definition()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total substance meronyms: 2\n",
      "\n",
      "heartwood.n.01 - the older inactive central wood of a tree or woody plant; usually darker and denser than the surrounding sapwood\n",
      "\n",
      "sapwood.n.01 - newly formed outer wood lying between the cambium and the heartwood of a tree or woody plant; usually light colored; active in water conduction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "substance_meronyms = tree.substance_meronyms()\n",
    "print('Total substance meronyms:', len(substance_meronyms))\n",
    "print()\n",
    "for s in substance_meronyms:\n",
    "    print(\"{} - {}\".format(s.name(), s.definition()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Relationships and Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = wn.synset('tree.n.01')\n",
    "lion = wn.synset('lion.n.01')\n",
    "tiger = wn.synset('tiger.n.02')\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities = [tree, lion, tiger, dog, cat]\n",
    "names = [e.name().split('.')[0] for e in entities]\n",
    "definitions = [e.definition() for e in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('tree.n.01'),\n",
       " Synset('lion.n.01'),\n",
       " Synset('tiger.n.02'),\n",
       " Synset('dog.n.01'),\n",
       " Synset('cat.n.01')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tree', 'lion', 'tiger', 'dog', 'cat']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a tall perennial woody plant having a main trunk and branches forming a distinct elevated crown; includes both gymnosperms and angiosperms',\n",
       " 'large gregarious predatory feline of Africa and India having a tawny coat with a shaggy mane in the male',\n",
       " 'large feline of forests in most of Asia having a tawny coat with black stripes; endangered',\n",
       " 'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds',\n",
       " 'feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_hypernyms = [[e1.lowest_common_hypernyms(e2)[0].name().split('.')[0] for e2 in entities]\n",
    "                      for e1 in entities ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tree', 'organism', 'organism', 'organism', 'organism'],\n",
       " ['organism', 'lion', 'big_cat', 'carnivore', 'feline'],\n",
       " ['organism', 'big_cat', 'tiger', 'carnivore', 'feline'],\n",
       " ['organism', 'carnivore', 'carnivore', 'dog', 'carnivore'],\n",
       " ['organism', 'feline', 'feline', 'carnivore', 'cat']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree</th>\n",
       "      <th>lion</th>\n",
       "      <th>tiger</th>\n",
       "      <th>dog</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>tree</td>\n",
       "      <td>organism</td>\n",
       "      <td>organism</td>\n",
       "      <td>organism</td>\n",
       "      <td>organism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>organism</td>\n",
       "      <td>lion</td>\n",
       "      <td>big_cat</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>feline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>organism</td>\n",
       "      <td>big_cat</td>\n",
       "      <td>tiger</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>feline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>organism</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>dog</td>\n",
       "      <td>carnivore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>organism</td>\n",
       "      <td>feline</td>\n",
       "      <td>feline</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tree       lion      tiger        dog        cat\n",
       "tree       tree   organism   organism   organism   organism\n",
       "lion   organism       lion    big_cat  carnivore     feline\n",
       "tiger  organism    big_cat      tiger  carnivore     feline\n",
       "dog    organism  carnivore  carnivore        dog  carnivore\n",
       "cat    organism     feline     feline  carnivore        cat"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(common_hypernyms, index=names, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarities = [[e1.path_similarity(e2) for e2 in entities] for e1 in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree</th>\n",
       "      <th>lion</th>\n",
       "      <th>tiger</th>\n",
       "      <th>dog</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tree  lion  tiger   dog   cat\n",
       "tree   1.00  0.07   0.07  0.12  0.08\n",
       "lion   0.07  1.00   0.33  0.17  0.25\n",
       "tiger  0.07  0.33   1.00  0.17  0.25\n",
       "dog    0.12  0.17   0.17  1.00  0.20\n",
       "cat    0.08  0.25   0.25  0.20  1.00"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(similarities, index=names, columns=names).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "    ('The fruits on that plant have ripened', 'n'),\n",
    "    ('He finally reaped the fruit of his hard word as he won the race', 'n')\n",
    "]\n",
    "\n",
    "word = 'fruit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The fruits on that plant have ripened\n",
      "Word synset: Synset('fruit.n.01')\n",
      "Corresponding definition: the ripened reproductive body of a seed plant\n",
      "--------------------------------------------------\n",
      "Sentence: He finally reaped the fruit of his hard word as he won the race\n",
      "Word synset: Synset('fruit.n.03')\n",
      "Corresponding definition: the consequence of some effort or action\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sent, tag in samples:\n",
    "    word_syn = lesk(word_tokenize(sent.lower()), word, pos=tag)\n",
    "    print(\"Sentence: {}\".format(sent))\n",
    "    print(\"Word synset: {}\".format(word_syn))\n",
    "    print(\"Corresponding definition: {}\".format(word_syn.definition()))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "    ('Lead is a very soft, malleable metal', 'n'),\n",
    "    ('John is the actor who plays the lead in that movie', 'n'),\n",
    "    ('This road leads to nowhere', 'v')\n",
    "]\n",
    "\n",
    "word = 'lead'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Lead is a very soft, malleable metal\n",
      "Word synset: Synset('lead.n.02')\n",
      "Corresponding definition: a soft heavy toxic malleable metallic element; bluish white when freshly cut but tarnishes readily to dull grey\n",
      "--------------------------------------------------\n",
      "Sentence: John is the actor who plays the lead in that movie\n",
      "Word synset: Synset('star.n.04')\n",
      "Corresponding definition: an actor who plays a principal role\n",
      "--------------------------------------------------\n",
      "Sentence: This road leads to nowhere\n",
      "Word synset: Synset('run.v.23')\n",
      "Corresponding definition: cause something to pass or lead somewhere\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sent, tag in samples:\n",
    "    word_syn = lesk(word_tokenize(sent.lower()), word, pos=tag)\n",
    "    print(\"Sentence: {}\".format(sent))\n",
    "    print(\"Word synset: {}\".format(word_syn))\n",
    "    print(\"Corresponding definition: {}\".format(word_syn.definition()))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "def parse_document(doc):\n",
    "    doc = re.sub('\\n', ' ', doc)\n",
    "    return [s.strip() for s in nltk.sent_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "text = \"\"\"\n",
    "Bayern Munich, or FC Bayern, is a German sports club based in Munich, \n",
    "Bavaria, Germany. It is best known for its professional football team, \n",
    "which plays in the Bundesliga, the top tier of the German football \n",
    "league system, and is the most successful club in German football \n",
    "history, having won a record 26 national titles and 18 national cups. \n",
    "FC Bayern was founded in 1900 by eleven football players led by Franz John. \n",
    "Although Bayern won its first national championship in 1932, the club \n",
    "was not selected for the Bundesliga at its inception in 1963. The club \n",
    "had its period of greatest success in the middle of the 1970s when, \n",
    "under the captaincy of Franz Beckenbauer, it won the European Cup three \n",
    "times in a row (1974-76). Overall, Bayern has reached ten UEFA Champions \n",
    "League finals, most recently winning their fifth title in 2013 as part \n",
    "of a continental treble. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "sentences = parse_document(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sentences = [nltk.pos_tag(tokens) for tokens in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ne_chunked_sents = [nltk.ne_chunk(tag_tokens) for tag_tokens in tagged_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('GPE', [('Bayern', 'NNP')]), Tree('ORGANIZATION', [('Munich', 'NNP')]), (',', ','), ('or', 'CC'), Tree('ORGANIZATION', [('FC', 'NNP'), ('Bayern', 'NNP')]), (',', ','), ('is', 'VBZ'), ('a', 'DT'), Tree('GPE', [('German', 'JJ')]), ('sports', 'NNS'), ('club', 'NN'), ('based', 'VBN'), ('in', 'IN'), Tree('GPE', [('Munich', 'NNP')]), (',', ','), Tree('GPE', [('Bavaria', 'NNP')]), (',', ','), Tree('GPE', [('Germany', 'NNP')]), ('.', '.')])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_chunked_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ne_chunked_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPE Bayern/NNP) True\n",
      "[('Bayern', 'NNP')] GPE\n",
      "(ORGANIZATION Munich/NNP) True\n",
      "[('Munich', 'NNP')] ORGANIZATION\n",
      "(',', ',') False\n",
      "('or', 'CC') False\n",
      "(ORGANIZATION FC/NNP Bayern/NNP) True\n",
      "[('FC', 'NNP'), ('Bayern', 'NNP')] ORGANIZATION\n",
      "(',', ',') False\n",
      "('is', 'VBZ') False\n",
      "('a', 'DT') False\n",
      "(GPE German/JJ) True\n",
      "[('German', 'JJ')] GPE\n",
      "('sports', 'NNS') False\n",
      "('club', 'NN') False\n",
      "('based', 'VBN') False\n",
      "('in', 'IN') False\n",
      "(GPE Munich/NNP) True\n",
      "[('Munich', 'NNP')] GPE\n",
      "(',', ',') False\n",
      "(GPE Bavaria/NNP) True\n",
      "[('Bavaria', 'NNP')] GPE\n",
      "(',', ',') False\n",
      "(GPE Germany/NNP) True\n",
      "[('Germany', 'NNP')] GPE\n",
      "('.', '.') False\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i, hasattr(i, 'label'))\n",
    "    if hasattr(i, 'label'):\n",
    "        print(i.leaves(), i.label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_entities = []\n",
    "\n",
    "for tagged_tree in ne_chunked_sents:\n",
    "    for node in tagged_tree:\n",
    "        if hasattr(node, 'label'):\n",
    "            name = \" \".join(c[0] for c in node.leaves())\n",
    "            label = node.label()\n",
    "            name_entities.append((name, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bayern', 'GPE'),\n",
       " ('Munich', 'ORGANIZATION'),\n",
       " ('FC Bayern', 'ORGANIZATION'),\n",
       " ('German', 'GPE'),\n",
       " ('Munich', 'GPE'),\n",
       " ('Bavaria', 'GPE'),\n",
       " ('Germany', 'GPE'),\n",
       " ('Bundesliga', 'ORGANIZATION'),\n",
       " ('German', 'GPE'),\n",
       " ('German', 'GPE'),\n",
       " ('Bayern', 'PERSON'),\n",
       " ('Franz John', 'PERSON'),\n",
       " ('Bayern', 'PERSON'),\n",
       " ('Bundesliga', 'ORGANIZATION'),\n",
       " ('Franz Beckenbauer', 'PERSON'),\n",
       " ('European', 'ORGANIZATION'),\n",
       " ('Overall', 'GPE'),\n",
       " ('Bayern', 'GPE'),\n",
       " ('UEFA', 'ORGANIZATION')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stanford_parser import stanford_ne_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sn = stanford_ne_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ne_annotated_sentences = [sn.tag(sent) for sent in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_entities = []\n",
    "\n",
    "for tag_sent in ne_annotated_sentences:\n",
    "    current_name = ''\n",
    "    current_name_tag = None\n",
    "    for name, tag in tag_sent:\n",
    "        if tag != 'O':\n",
    "            current_name = \" \".join([current_name, name]).strip()\n",
    "            current_name_tag = (current_name, tag)\n",
    "        else:\n",
    "            if current_name_tag:\n",
    "                name_entities.append(current_name_tag)\n",
    "                current_name_tag = None\n",
    "                current_name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity Name</th>\n",
       "      <th>Entity Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FC Bayern</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Munich</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bavaria</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FC Bayern</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Franz John</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bayern</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Franz Beckenbauer</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bayern</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Entity Name   Entity Type\n",
       "0      Bayern Munich  ORGANIZATION\n",
       "1          FC Bayern  ORGANIZATION\n",
       "2             Munich      LOCATION\n",
       "3            Bavaria      LOCATION\n",
       "4            Germany      LOCATION\n",
       "5          FC Bayern  ORGANIZATION\n",
       "6         Franz John        PERSON\n",
       "7             Bayern  ORGANIZATION\n",
       "8  Franz Beckenbauer        PERSON\n",
       "9             Bayern  ORGANIZATION"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(name_entities, columns=['Entity Name', 'Entity Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Semantic Representations\n",
    "\n",
    "#### Propositional Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P = 'P'\n",
    "Q = 'Q'\n",
    "P_prop = 'He is hungry'\n",
    "Q_prop = 'He will eat a sandwich'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_status = [True, False]\n",
    "q_status = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conjunction = 'P & Q'\n",
    "disjunction = \"P | Q\"\n",
    "implication = 'P -> Q'\n",
    "equivalence = 'P <-> Q'\n",
    "\n",
    "expressions = [conjunction, disjunction, implication, equivalence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for p_val, q_val in product(p_status, q_status):\n",
    "    \n",
    "    valu = nltk.Valuation([(P, p_val), (Q, q_val)])\n",
    "    dom = set()\n",
    "    assignment = nltk.Assignment(dom)\n",
    "    model = nltk.Model(dom, valu)\n",
    "    row = [p_val, q_val]\n",
    "    for expr in expressions:\n",
    "        row.append(model.evaluate(expr, assignment))\n",
    "    results.append(row)\n",
    "    \n",
    "cols = [P, Q] + expressions\n",
    "\n",
    "result_df = pd.DataFrame(results, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>P &amp; Q</th>\n",
       "      <th>P | Q</th>\n",
       "      <th>P -&gt; Q</th>\n",
       "      <th>P &lt;-&gt; Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       P      Q  P & Q  P | Q  P -> Q  P <-> Q\n",
       "0   True   True   True   True    True     True\n",
       "1   True  False  False   True   False    False\n",
       "2  False   True  False   True    True    False\n",
       "3  False  False  False  False    True     True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Order Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init theorem provers\n",
    "prover_path = \"/home/pongsakorn/Desktop/python_learning/TextAnalyticsWithPython/LADR-2009-11A/bin\"\n",
    "os.environ['PROVER9'] = prover_path\n",
    "\n",
    "prover = nltk.Prover9()\n",
    "\n",
    "# for reading FOL expressions\n",
    "read_expr = nltk.sem.Expression.fromstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# `If an entity jumps over another entity, the reverse cannot happen`\n",
    "\n",
    "# set the rule expression\n",
    "rule = read_expr('all x. all y. (jumps_over(x, y) -> -jumps_over(y, x))')\n",
    "# set the event occured\n",
    "event = read_expr('jumps_over(fox, dog)')\n",
    "\n",
    "# set the outcome to evaluate\n",
    "test_outcome = read_expr(\"jumps_over(dog, fox)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prover.prove(goal=test_outcome, assumptions=[event, rule], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "# aclimdb/[train|test\\]/[pos|neg]\n",
    "folder = './aclImdb/'\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "for d1 in ['train', 'test']:\n",
    "    for d2 in ['pos', 'neg']:\n",
    "        path = os.path.join(folder, d1, d2)\n",
    "        for txt_file in os.listdir(path):\n",
    "            with open(os.path.join(path, txt_file), 'r') as f:\n",
    "                review = f.read().strip()              \n",
    "                row = [review, d2]\n",
    "                dataset = dataset.append([row], ignore_index=True)\n",
    "\n",
    "idx = np.random.permutation(len(dataset))\n",
    "\n",
    "dataset.columns = ['review', 'sentiment']\n",
    "\n",
    "dataset = dataset.iloc[idx]\n",
    "\n",
    "dataset.reset_index(drop=True, inplace=True)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-573a86964f34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from html import unescape\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "contractions = pickle.load(open('contraction_map.pkl', 'rb'))\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    '''class to remove html tag and unescape html characters'''\n",
    "    \n",
    "    def __init__(self, convert_charrefs=False):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "        self.convert_charrefs = convert_charrefs\n",
    "        \n",
    "    def handle_data(self, txt):\n",
    "        self.fed.append(unescape(txt))\n",
    "        \n",
    "    def get_data(self):\n",
    "        return \" \".join(self.fed)\n",
    "\n",
    "def expand_contraction(text, contractions):\n",
    "    \n",
    "    pattern = re.compile(r\"({})\".format('|'.join(contractions.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand(match_obj):\n",
    "        \n",
    "        txt = match_obj.group(0)\n",
    "        new_txt = contractions.get(txt, contractions.get(txt.lower()))\n",
    "        return txt[0] + new_txt[1:]\n",
    "    \n",
    "    text = pattern.sub(expand, text)\n",
    "    text = re.sub(\"'\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_pos_tag(text):\n",
    "    \n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    return [(w, penn_to_wn_tags(tag)) for w,tag in tagged_tokens]\n",
    "\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    tagged_tokens = text_pos_tag(text)\n",
    "    \n",
    "    return \" \".join(wnl.lemmatize(w, tag) if tag else w for w, tag in tagged_tokens)\n",
    "\n",
    "\n",
    "def keep_only_text(text):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    for w in tokens:\n",
    "        if re.search(\"[a-zA-Z]\", w):\n",
    "            filtered_tokens.append(w.strip())\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "\n",
    "def remove_special_char(text):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pattern = re.compile(r'[{}]'.format(re.escape(string.punctuation)))\n",
    "    text = \" \".join(filter(None, [pattern.sub(' ', t).strip() for t in tokens]))\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join(w for w in tokens if w not in stop_words)\n",
    "\n",
    "def strip_html(text):\n",
    "    html_stripper = MLStripper()\n",
    "    html_stripper.feed(text)\n",
    "    return html_stripper.get_data()\n",
    "\n",
    "\n",
    "def normalize_accented_characters(text):\n",
    "    text = unicodedata.normalize('NFKD', \n",
    "                                 text.decode('utf-8')).encode('ascii', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, lemmanize=True, \n",
    "                     tokenize=False, only_text_chars=False):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        text = strip_html(text)\n",
    "        #text = normalize_accented_characters(text)\n",
    "        \n",
    "        # expand contraction\n",
    "        text = expand_contraction(text, contractions)\n",
    "        \n",
    "        if lemmanize:\n",
    "            text = lemmatize_text(text).lower()\n",
    "        else:\n",
    "            text = text.lower()\n",
    "            \n",
    "        if only_text_chars:\n",
    "            text = keep_only_text(text)\n",
    "        \n",
    "        # remove special char\n",
    "        text = remove_special_char(text)\n",
    "        # remove stopwords\n",
    "        text = remove_stopwords(text)\n",
    "        \n",
    "        if tokenize:\n",
    "            text = nltk.word_tokenize(text)\n",
    "        \n",
    "        normalized_corpus.append(text)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "def build_feature_matrix(corpus, feature_type='frequency', \n",
    "                         min_df=0.0, max_df=1.0, ngram=(1, 1)):\n",
    "    \n",
    "    if feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df, \n",
    "                                     max_df=max_df, ngram_range=ngram)\n",
    "    elif feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=min_df, \n",
    "                                     max_df=max_df, ngram_range=ngram)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram)\n",
    "    else:\n",
    "        raise Exception('Invalid feature type: possible choice [{}]'.format(\"|\".join(['frequency', \n",
    "                                                                                      'binary',\n",
    "                                                                                     'tfidf'])))\n",
    "    feature_matrix = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                                f1_score, recall_score, precision_score)\n",
    "\n",
    "\n",
    "def display_classification_report(y_true, y_pred, labels=None, target_names=None):\n",
    "    report = classification_report(y_true, y_pred, \n",
    "                                   labels=labels, target_names=target_names)\n",
    "    print(report)\n",
    "    \n",
    "    \n",
    "def display_confusion_matrix(y_true, y_pred, labels=[1, 0]):\n",
    "    con_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    idx_cols = pd.MultiIndex.from_product([['Prediction'], labels])\n",
    "    idx_rows = pd.MultiIndex.from_product([['Actual'], labels])\n",
    "    \n",
    "    return pd.DataFrame(con_mat, index=idx_rows, columns=idx_cols, dtype=int)\n",
    "\n",
    "\n",
    "def display_evaluation_metrics(y_true, y_pred, pos_label=1):\n",
    "    \n",
    "    result = \"Accuracy: {acc:.3f}\\nPrecision: {prec:.3f}\\n\\\n",
    "    Recall: {recall:.3f}\\nF1 Score: {f1:.3f}\"\n",
    "    \n",
    "    print(result.format(acc=accuracy_score(y_true, y_pred),\n",
    "                        prec=precision_score(y_true, y_pred),\n",
    "                        recall=recall_score(y_true, y_pred),\n",
    "                        f1=f1_score(y_true, y_pred, pos_label=pos_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset.to_csv('movie_reviews.csv', index=False)\n",
    "dataset = pd.read_csv('movie_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For me, it just didn't seem like GI Joe at all...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Back in August, '81 there was a country-ish bu...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This 1973 TV remake of the Billy Wilder classi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie really shows its age. The print I s...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As others have noted, this should have been an...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  For me, it just didn't seem like GI Joe at all...       neg\n",
       "1  Back in August, '81 there was a country-ish bu...       pos\n",
       "2  This 1973 TV remake of the Billy Wilder classi...       neg\n",
       "3  This movie really shows its age. The print I s...       neg\n",
       "4  As others have noted, this should have been an...       neg"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000,), (35000,), (15000,), (15000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = 35000\n",
    "\n",
    "train = dataset.iloc[:n_train]\n",
    "test = dataset.iloc[n_train:]\n",
    "\n",
    "review_train = train['review'].values\n",
    "review_test = test['review'].values\n",
    "sentiment_train = train['sentiment'].values\n",
    "sentiment_test = test['sentiment'].values\n",
    "\n",
    "review_train.shape, sentiment_train.shape, review_test.shape, sentiment_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8,), (8,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = 8\n",
    "\n",
    "sample_idx = np.random.permutation(range(len(test)))[:n_sample]\n",
    "\n",
    "review_sample = review_test[sample_idx]\n",
    "sentiment_sample = sentiment_test[sample_idx]\n",
    "\n",
    "review_sample.shape, sentiment_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30min 6s, sys: 3.39 s, total: 30min 9s\n",
      "Wall time: 30min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# normalized dataset\n",
    "\n",
    "norm_train_reviews = normalize_corpus(review_train, lemmanize=True, only_text_chars=True)\n",
    "norm_test_reviews = normalize_corpus(review_test, lemmanize=True, only_text_chars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.53 s, sys: 132 ms, total: 9.66 s\n",
      "Wall time: 9.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# feature extraction\n",
    "vectorizer, train_features = build_feature_matrix(norm_train_reviews, feature_type='tfidf',\n",
    "                                                 min_df=0.0, max_df=1.0, ngram=(1, 1))\n",
    "\n",
    "test_features = vectorizer.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SGDClassifier(loss='hinge', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=1000, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(train_features, sentiment_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.94      0.91      0.93     17504\n",
      "        pos       0.92      0.94      0.93     17496\n",
      "\n",
      "avg / total       0.93      0.93      0.93     35000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_classification_report(sentiment_train, svc.predict(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.90      0.88      0.89      7496\n",
      "        pos       0.88      0.91      0.89      7504\n",
      "\n",
      "avg / total       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_classification_report(sentiment_test, svc.predict(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Actual</th>\n",
       "      <th>pos</th>\n",
       "      <td>16443</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>1521</td>\n",
       "      <td>15983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Prediction       \n",
       "                  pos    neg\n",
       "Actual pos      16443   1053\n",
       "       neg       1521  15983"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_confusion_matrix(sentiment_train, svc.predict(train_features), labels=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Actual</th>\n",
       "      <th>pos</th>\n",
       "      <td>6815</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>936</td>\n",
       "      <td>6560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Prediction      \n",
       "                  pos   neg\n",
       "Actual pos       6815   689\n",
       "       neg        936  6560"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_confusion_matrix(sentiment_test, svc.predict(test_features), labels=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews:\n",
      "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!\n",
      "\n",
      "Predict: neg\n",
      "Actual: pos\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "I saw one of the stage performances in Denver and have never been less impressed. The word \"vagina\" says it all. A body part. Nothing shocking here. I could say to my doctor, \"My left arm has been hurting a bit after tennis\" or \"My vagina hurts after cycling\" with equal or more social commentary. It could be the \"Tricep Monologues\" for all the entertainment or radical comment I heard. The monologues were dull but delivered with drama, the topics were outdated, and I was alternately bored and annoyed. Once I think I laughed but apparently it wasn't when I was supposed to. Surely this isn't really a hit. Oh, and spoilers: there was a LESBIAN! - oh, wait, maybe not, come to think of it. And Inappropriate Fondling! And a Crack Mama! That about covers it.\n",
      "\n",
      "Predict: neg\n",
      "Actual: neg\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "Wow! Wow! Wow! I have never seen a non-preachy documentary on globalization until I saw MARDI GRAS: MADE IN CHINA. This film has zero narration and combines verite footage with sensitive interviews with four teenage workers in China who live inside a factory compound. They play with toys, jump rope, and dance. Yet, the majority of their days and nights consist of work, work, and work -- but the footage of their work is illuminating and mesmerizing to watch. The owner of the factory in China is amazingly open, so much so that he hits home the effects of globalization while he \"punishes\" the workers. Astutely following Mardi Gras beads from China to the Carnival, the film reveals how the local is connected to the global through humor and interesting, compelling footage from both cultures. One of the most interesting parts in this film is the cross cultural introduction of factory workers and Mardi Gras revelers to each other through pictures. Here, the film comes full circle and shows how images can be a point of communication and transformation. The film is never preachy, is not guilt driven, and allows everyone's point of view to be present. At the end, we -- the viewers -- make up our own conclusions about the complexity of the film, and globalization.\n",
      "\n",
      "Predict: pos\n",
      "Actual: pos\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "I remember when this came out it was the first kung fu film ever seen around our way and we were all excited about seeing it for sure .Although the action was mediocre at best it gave us our first taste of kung fu and our first taste of bad dubbing as well as bad film making or more precisely the way Chinese people were making films at the time . They were admittedly inferior wlthout question but there was entertainment value here and that caught on for sure . The kung fu craze had begun and Bruce Lee and ''The Chinese Connection'' would soon follow either that or ''The Chinese Boxer'' with Jimmy Wang Yu . In any case this film was chosen to lead the way .\n",
      "\n",
      "Predict: pos\n",
      "Actual: neg\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "Sheesh! What a dreadful movie. Dodgy camera work, a script with more corn than Kellogg's, and acting so hammy you could open a pig farm with it. <br /><br />To cap it all, it doesn't know which audience to aim at - we have Cornel Wilde - or is that Corny Wilde? - getting on his soap box about the hazards of smoking any time someone lights a cigarette, dear oh dear, and in another awkward scene we have the baddie, Lobo, forcing his, ahem, if you will, 'male friend' to do a striptease dressed in a bikini. Try explaining that one to the kids...<br /><br />Throw in an overly contrived Treasure Island-cum-Jaws type storyline, and the result is a film so unintentionally funny, it's enjoyable - I shouldn't expect a Special Edition DVD any time soon, though.\n",
      "\n",
      "Predict: neg\n",
      "Actual: neg\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "Actually, Goldie Hawn is from Washington (Takoma Park, Maryland), but I digress. This is sort of a Mr. Smith goes to Washington type of movie, with some variations but the same premise. I taped this movie off of cable years ago because I had a huge crush on Goldie Hawn. The story is interesting, but it's highly unlikely that some cocktail waitress will get an important job in the government just because she saved some big shot's life. It made me laugh and made me mad at the same time. It made me laugh because some of the situations she found herself in were so ridiculous, I had to laugh. (POSSIBLE SPOILER AHEAD). It made me mad to think that our government would set up an average citizen in the manner she was set up. And the speech she made at the end...beautiful. Too bad not many people have guts like that in real life.\n",
      "\n",
      "Predict: pos\n",
      "Actual: pos\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "I'm a bit spooked by some of these reviews praising A.K.A. Not only do they sound as if they were written by the same person, but they contain all kinds of insider information that surely you could only find by reading the press book from cover to cover. Please don't tell me that the director is writing his own reviews as that would just be too sad to contemplate.<br /><br /> Afraid I'm another one of those who hated the film and was surprised by its unapologetic amateurism. Great idea, shame about the execution. And it was most disconcerting to watch so many good actors (as well as some very bad ones including the leaden lead) all apparently thinking that they were appearing in a series of very different films.<br /><br /> I wish that A.K.A. had been audacious, innovative or just simply interesting. Sadly it was like watching an unintentionally hysterical home video with arty aspirations. A missed opportunity.\n",
      "\n",
      "Predict: neg\n",
      "Actual: neg\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "This must be the dumbest movie I've ever seen and therefor it deserves a special award. OK, so it begins pretty good, logical, humor and then rapidly changes to complete and utter uh f@rt,@ss, föck stupidity that makes total sense. The strange thing is that i hardly laughed at it, though i guess it is supposed to be funny, with a very serious message. After a moment of thought why i didn't laugh, the answer was pretty clear... this movie is a lot more realistic then i would like it to be. So it is extremely dumb, yet very wise if the object was to reach people which aren't 'blessed' with a whole lot of neurons and synapses and electrolytes (huh, electrolytes?). This movie might actually be understood at some level by a big audience!<br /><br />Although they probably just laugh.<br /><br />Ah well, nice try...<br /><br />I am now editing (well adding to) my comment, cause i read some of the other comments. First i noticed some people see racism in this movie. Well, that did not at all cross my mind when watching it. However, i can imagine if you are looking from that perspective, you will see some. We could of course debate the link between intelligence and race, but i think this is not the place to do so. I wonder, if we were to rate the Huxtable family here, would you complain about them all being black? I don't think so, but i guess you could... (aside from that, from the 2 wisest people in the world, half, the woman, doesn't seem totally caucasian to me, so 'what the problem is?')<br /><br />Secondly the movie gets very mixed ratings; from 1's to 10's and everything in between, but a lot of polarisation there, which means the movie does its job very well! It stirrs people up, makes them want to speak about it and judge it. That means it is a good movie, whether you like it or hate it. After 5 minutes it becomes very predictive and boring even, yet you keep watching this utter crap. I guess you keep hoping that it isn't so. Or maybe it's our sick way to look at terrible accidents. Well, this is one for sure.\n",
      "\n",
      "Predict: neg\n",
      "Actual: pos\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "reviews = review_test[sample_idx]\n",
    "preds = svc.predict(test_features)[sample_idx]\n",
    "trues = sentiment_test[sample_idx]\n",
    "\n",
    "for r, p, t in zip(reviews, preds, trues):\n",
    "    print('Reviews:\\n{}\\n\\nPredict: {}\\nActual: {}'.format(r, p, t))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sk = StratifiedKFold(n_splits=10, shuffle=True, random_state=123)\n",
    "gb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tr_idx, ts_idx in sk.split(train_features, sentiment_train):\n",
    "    data_X = train_features[ts_idx].toarray()\n",
    "    data_y = sentiment_train[ts_idx]\n",
    "    \n",
    "    gb.partial_fit(data_X, data_y, classes=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "for row in test_features:\n",
    "    y_preds.append(gb.predict(row.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.62      0.78      0.69      7496\n",
      "        pos       0.71      0.53      0.60      7504\n",
      "\n",
      "avg / total       0.67      0.66      0.65     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_classification_report(sentiment_test, np.array(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Actual</th>\n",
       "      <th>pos</th>\n",
       "      <td>3960</td>\n",
       "      <td>3544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>1631</td>\n",
       "      <td>5865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Prediction      \n",
       "                  pos   neg\n",
       "Actual pos       3960  3544\n",
       "       neg       1631  5865"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_confusion_matrix(sentiment_test, np.array(y_preds), labels=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pysrt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pysrt.open('../../Ted 2.srt', encoding='TIS-620')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt = a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'องคุณ\\nตอนนี้อยากพูดอะไรไหมคะ\\nในเมื่อคุณเป็นบุคคลตามกฎหมายแล้ว\\nมีครับ\\nแทมิลินน์ แม็คแคฟเฟอร์ตี้\\nคุณจะแต่งงานกับผมไหม\\nค่ะ\\n<i>และเรื่องราวของเรา ก็มาถึงตอนจบ</i>\\n<i>เท็ดกับแทมิลินน์ แต่งงานกันอีกครั้ง</i>\\n<i>และไม่นานหลังจากนั้น</i>\\n<i>พวกเขาก็รับเลี้ยงทารกน้อย น่ารักคนหนึ่ง</i>\\nจอห์นนี่ มาเจอลูกทูนหัวคนใหม่ของนายสิ\\nอะพอลโล คีร้ด คลับเบอร์ แลง\\nเท็ดดี้ เขาน่ารักมาก\\nและเรามีของขวัญจะให้เขาด้วย\\nจอห์น เอาให้เขาดูสิคะ\\nเอ้อ นั่นสิ เดี๋ยวนะ\\nเขามีอะไรให้หนูด้วยล่ะ\\nจอห์นนี่\\nจอห์นนี่ มันยอดมาก\\nเฮ้ รู้ไหม บางทีวันหนึ่ง\\nถ้าหนูอธิษฐานมากพอ\\nเขาอาจจะมีชีวิตขึ้นมาก็ได้นะ\\nและพวกหนูก็จะได้เล่นยาสารพัดอย่างด้วยกัน\\nเท่านั้นแหละที่ฉันอยากให้เขา\\nกลิ่นเหมือนมีใครอึใส่ผ้าอ้อม\\nเท็ดดี้ ตาคุณแล้วนะคะ\\nได้เลย ไม่มีปัญหา\\n- <i>ให้ตายสิ</i>\\n- <i>มีอะไรงั้นเหรอ</i>\\n<i>นี่ไม่ปกติแน่ เขาต้องป่วย</i>\\n<i>เขาไม่ได้ป่วย มันก็แค่อึเด็ก</i>\\n<i>ใช่ เด็กก็อย่างนี้แหละ เท็ดดี้</i>\\n- <i>ไม่คิดว่าผ้าอ้อมนี่น่าขยะแขยงเหรอ</i>\\n- <i>ไม่</i>\\n<i>ก็ได้ งั้นรับนะ</i>\\n<i>เท็ดดี้ อะไรฟะ</i>\\n<i>แม่เจ้า ไอ้สารเลว</i>\\n<i>แฮชแท็ก \"เรื่องขี้ๆ\"</i>'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pat = re.compile(r'<i>(.+)</i>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['มีคนบอกเราอยู่บ่อยครั้ง',\n",
       " 'ว่าความสุขชั่วนิรันดร์',\n",
       " 'มีอยู่แค่ในหนังสือนิทาน',\n",
       " 'และในใจที่ไร้เดียงสา',\n",
       " 'ของคนอ่อนต่อโลกที่ได้รับการปกป้อง',\n",
       " 'ซึ่งพิสูจน์ได้สองสิ่ง',\n",
       " 'ก้าวออกไปพร้อมหวานใจ',\n",
       " 'รู้สึกดีอย่างนี้ไม่มีทางเลวร้าย',\n",
       " 'ไม่เคยรู้สึกแจ่มใสเพียงนี้',\n",
       " 'ดังนั้นขอภาวนาให้วันนี้ดีเรื่อยไป',\n",
       " 'ใช่ ก้าวออกมาพร้อมกับยาหยี',\n",
       " 'ทำถูกขนาดนี้ ไม่มีทางพลาด',\n",
       " 'ถามสิว่าจะเป็นเมื่อไหร่',\n",
       " 'วันสำคัญอาจเป็นคืนนี้ไง',\n",
       " 'Downtown Abbey',\n",
       " 'ไปศาลกันเถอะ',\n",
       " 'ไปเขียนกฎหมายกันเถอะ',\n",
       " 'เย้ เย้ เย้ เย้',\n",
       " 'กฎหมายกันเถอะ',\n",
       " 'เฮ้ คุณทนายทั้งหลาย',\n",
       " 'คุณไม่รู้ว่าผมกับจอห์นนี่ดูอยู่',\n",
       " 'ระหว่างเมายา',\n",
       " 'I Am Legend',\n",
       " 'Sister, Sister',\n",
       " 'ทุกหน่วย มีเหตุ 3-17 บนถนนเมเปิลไดรฟ์',\n",
       " 'ทุกหน่วยในพื้นที่ตอบด้วย',\n",
       " 'กอดให้แนบแน่นอีกสิ ดร.แดนเซอร์',\n",
       " 'โมโนโพลี',\n",
       " 'Frozen',\n",
       " 'เดรด สก็อตต์กับแซนด์ฟอร์ด',\n",
       " 'เพลสซี่กับเฟอร์กูสัน',\n",
       " 'บราวน์กับคณะกรรมาธิการการศึกษา',\n",
       " 'Kramer vs. Kramer, Alien vs. Predator',\n",
       " 'เฟรดดี้กับเจสัน',\n",
       " 'Ernest Goes to Camp,',\n",
       " 'Ernest Goes to Jail',\n",
       " 'The Importance of Being Earnest',\n",
       " 'นายมีชื่อว่าโทบี้',\n",
       " 'นายต้องเรียนรู้ที่จะพูดชื่อตัวเอง',\n",
       " 'พูดให้ฟังซิ',\n",
       " 'นายชื่ออะไร',\n",
       " 'คุนตา คุนตา คินเต',\n",
       " 'Rocky III.',\n",
       " 'Rocky',\n",
       " 'Rocky',\n",
       " 'Rocky',\n",
       " 'Rocky',\n",
       " 'The Great Gatsby',\n",
       " 'คุณคิดว่า',\n",
       " 'ผมจะทำอะไร ณ เวลานี้',\n",
       " 'เมื่อคุณยืนอยู่ต่อหน้า',\n",
       " 'เมื่อน้ำตาเอ่อท้น ในดวงตาของคุณ',\n",
       " 'ฉันรักเธอ',\n",
       " 'แล้วก็ในข่าว คดีที่ยังค้างอยู่ในศาล',\n",
       " 'กำลังเริ่มได้รับความสนใจระดับชาติ',\n",
       " 'ในเรื่องการแตกแขนงของสิทธิพลเมือง',\n",
       " 'ตุ๊กตาหมีเท็ด ซึ่งพวกคุณอาจจำกันได้',\n",
       " 'มีชีวิตขึ้นมากลางยุค 1980',\n",
       " 'ที่เมืองบอสตันนี่เอง',\n",
       " 'โอเค นี่เรากำลังพูดเรื่องอะไรกันเหรอ',\n",
       " 'สิ่งที่เราจะพูดถึงคือประเด็นสิทธิพลเมือง',\n",
       " 'หมีตัวนี้มีสิทธิ์',\n",
       " 'โอ ไม่เอาน่า',\n",
       " 'เขาไม่มีหรอก เขาเป็นของเล่น',\n",
       " 'งั้นทำไมเรียกมันว่า \"เขา\" ล่ะ',\n",
       " 'ฟังนะ เราเรียกเทพีสันติภาพว่า \"เธอ\"',\n",
       " 'แต่เราก็รู้ว่ามันคือวัตถุทำจากทองแดงและเหล็ก',\n",
       " 'โอ พูดได้ดี',\n",
       " 'ใช่ แต่เธอไม่ได้มีสติหรือความรู้สึก',\n",
       " 'เขามี',\n",
       " 'โธ่ ไม่เอาน่า',\n",
       " 'พวกคุณที่นั่งอยู่ตรงนั้นจะบอกผม',\n",
       " 'ว่าตุ๊กตาตัวนี้เป็นคนงั้นเหรอ',\n",
       " 'ไม่',\n",
       " 'ไม่เลย',\n",
       " 'เราต่างเห็นพ้องตลอดเวลา',\n",
       " 'ผมไม่คิดว่าเขาควรถูกพิจารณาว่าเป็นคน',\n",
       " 'นั่นน่ารักเมื่อคุณเป็นตุ๊กตาสัตว์',\n",
       " 'แต่พอเป็นคนแล้ว นั่นเป็นความผิดร้ายแรง',\n",
       " 'เท็ด คุณมีอะไรจะพูดแก้ต่างให้ตัวเองไหม',\n",
       " 'ผมไม่ใช่สัตว์',\n",
       " 'เห็นไหมคะ ใต้เท้า',\n",
       " 'เขาไม่ใช่สัตว์',\n",
       " 'ขอโทษค่ะ ใต้เท้า ฉันมีประจำเดือน',\n",
       " 'และถ่ายทอดสดจากนิวยอร์ก',\n",
       " 'นี่คือรายการแซทเทอร์เดย์ไนท์',\n",
       " 'เท็ดกับรัฐแห่งแมสซาชูเสตส์',\n",
       " 'เท็ดกับแมสซาชูเสตส์',\n",
       " 'ชาร์ลี เฮบโด',\n",
       " 'เจ้าจันทร์ใจร้าย',\n",
       " 'สนุกหรือไร',\n",
       " 'ที่ได้ส่องแสงอาบไล้ยามที่ฉันหมองเศร้า',\n",
       " 'ต้องอยู่เดียวดายภายใต้ดวงตะวัน',\n",
       " 'เจ้าจันทร์ใจร้าย',\n",
       " 'ช่วยส่องแสงนำทาง',\n",
       " 'และพบคนรักที่ฉันนึกว่าจะอยู่คู่กัน',\n",
       " 'แล้วเธอก็ไป และพาคนรักฉันไปด้วย',\n",
       " 'เจ้าจันทร์ใจร้าย',\n",
       " 'โอ ดวงดาวใจมารเบื้องบน',\n",
       " 'เกมที่เธอปั่นหัวฉัน',\n",
       " 'ฉันจะพบความสุข',\n",
       " 'ถ้าราตรีทั้งหมดขุ่นมัว',\n",
       " 'เจ้าจันทร์ใจร้าย',\n",
       " 'หวังว่าเป็นความจริง',\n",
       " 'เธอพรากแสงทั้งหมดที่เหลือ',\n",
       " 'และเก็บไว้ให้ใครคนนั้น',\n",
       " 'เจ้าจันทร์ใจร้าย',\n",
       " 'เจ้าจันทร์ใจร้าย',\n",
       " 'สตาร์วอร์ส',\n",
       " 'สตาร์เทร็ค',\n",
       " 'เขาตามล่าฉัน นายต้องมาที่นี่นะ',\n",
       " 'มือสัมผัสมือ',\n",
       " 'เอื้อมออกไป',\n",
       " 'สัมผัสฉัน สัมผัสเธอ',\n",
       " 'แคโลไลน์ผู้แสนหวาน',\n",
       " 'เอนเทอร์ไพรซ์..',\n",
       " \"Where's Waldo\",\n",
       " 'และเรื่องราวของเรา ก็มาถึงตอนจบ',\n",
       " 'เท็ดกับแทมิลินน์ แต่งงานกันอีกครั้ง',\n",
       " 'และไม่นานหลังจากนั้น',\n",
       " 'พวกเขาก็รับเลี้ยงทารกน้อย น่ารักคนหนึ่ง',\n",
       " 'ให้ตายสิ',\n",
       " 'มีอะไรงั้นเหรอ',\n",
       " 'นี่ไม่ปกติแน่ เขาต้องป่วย',\n",
       " 'เขาไม่ได้ป่วย มันก็แค่อึเด็ก',\n",
       " 'ใช่ เด็กก็อย่างนี้แหละ เท็ดดี้',\n",
       " 'ไม่คิดว่าผ้าอ้อมนี่น่าขยะแขยงเหรอ',\n",
       " 'ไม่',\n",
       " 'ก็ได้ งั้นรับนะ',\n",
       " 'เท็ดดี้ อะไรฟะ',\n",
       " 'แม่เจ้า ไอ้สารเลว',\n",
       " 'แฮชแท็ก \"เรื่องขี้ๆ\"']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat.findall(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pysrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '../../python_learning/ted_subtitles/SaraSeager_2015.en.th.srt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subs = pysrt.open(filename, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = [sub.text for sub in subs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm here to tell you about the real search for alien life.\\nฉันมานี่เพื่อเล่าให้คุณฟัง เกี่ยวกับการค้นหาสิ่งมีชีวิตนอกโลกของจริง\",\n",
       " 'Not little green humanoids arriving in shiny UFOs,\\nไม่ใช่มนุษย์เขียวตัวเล็ก ที่มากับจานบินเรืองแสง',\n",
       " 'although that would be nice.\\nถึงถ้าเป็นอย่างนั้นก็ดีเหมือนกัน',\n",
       " \"But it's the search for planets orbiting stars far away.\\nแต่เป็นการเสาะหาดาวเคราะห์ ที่โคจรรอบดาวฤกษ์ซึ่งอยู่ไกลออกไป\",\n",
       " 'Every star in our sky is a sun.\\nดาวทุกดวงบนฟ้าของเราคือดวงอาทิตย์ค่ะ',\n",
       " 'And if our sun has planets --\\nและถ้าของเรามีดาวเคราะห์',\n",
       " 'Mercury, Venus, Earth, Mars, etc.,\\nดาวพุธ ดาวศุกร์ โลก ดาวอังคาร และอื่นๆ',\n",
       " 'surely those other stars should have planets also,\\nดาวฤกษ์ดวงอื่นๆ ก็ควรมีดาวเคราะห์ด้วยเช่นกัน',\n",
       " 'and they do.\\nและก็เป็นอย่างนั้นจริงๆ ค่ะ',\n",
       " 'And in the last two decades,\\nสองทศวรรษที่ผ่านมา',\n",
       " 'astronomers have found thousands of exoplanets.\\nนักดาราศาสตร์ได้ค้นพบ ดาวเคราะห์นอกระบบนับพันดวง',\n",
       " 'Our night sky is literally teeming with exoplanets.\\nท้องฟ้ายามค่ำของเรา แท้จริงแล้ว เต็มไปด้วยดาวเคราะห์นอกระบบ',\n",
       " 'We know, statistically speaking,\\nว่ากันตามสถิติแล้ว',\n",
       " 'that every star has at least one planet.\\nเรารู้ว่าดาวฤกษ์ทุกดวงจะมีดาวเคราะห์หนึ่งดวง',\n",
       " 'And in the search for planets,\\nการเสาะหาดาวเคราะห์อื่น',\n",
       " 'and in the future, planets that might be like Earth,\\nและในอนาคต ดาวเคราะห์ที่อาจเหมือนกับโลก',\n",
       " \"we're able to help address\\nอาจทำให้เราช่วยตอบคำถาม\",\n",
       " 'some of the most amazing and mysterious questions\\nที่น่าตื่นใจและเป็นปริศนาที่สุด',\n",
       " 'that have faced humankind for centuries.\\nที่มนุษย์เฝ้าสงสัยมาหลายศตวรรษได้',\n",
       " 'Why are we here?\\nทำไมเราถึงอยู่ที่นี่',\n",
       " 'Why does our universe exist?\\nทำไมจักรวาลของเราถึงอุบัติขึ้น',\n",
       " 'How did Earth form and evolve?\\nโลกก่อตัวและเปลี่ยนแปลงไปได้อย่างไร',\n",
       " 'How and why did life originate and populate our planet?\\nสิ่งมีชีวิตเกิดและอาศัยอยู่ บนดาวเคราะห์ของเราอย่างไรและทำไม',\n",
       " 'The second question that we often think about is:\\nคำถามต่อมาที่เรามักนึกสงสัยคือ',\n",
       " 'Are we alone?\\nมีแค่พวกเราหรือเปล่า',\n",
       " 'Is there life out there?\\nนอกโลกนั้นมีสิ่งมีชีวิตอยู่อีกไหม',\n",
       " 'Who is out there?\\nใครอยู่นอกโลกนั่น',\n",
       " 'You know, this question has been around for thousands of years,\\nคือคำถามนี้ก็มีคนถามกันมาเป็นพันปีค่ะ',\n",
       " 'since at least the time of the Greek philosophers.\\nอย่างน้อยก็ตั้งแต่ยุคของนักปราชญ์ชาวกรีก',\n",
       " \"But I'm here to tell you just how close we're getting\\nแต่ฉันมาที่นี่เพื่อจะบอกคุณ ว่าเราอยู่ใกล้แค่ไหนแล้ว\",\n",
       " 'to finding out the answer to this question.\\nกับคำตอบของคำถามนี้',\n",
       " \"It's the first time in human history that this really is within reach for us.\\nเป็นครั้งแรกในประวัติศาสตร์เลยนะคะ ที่คำตอบนั้นอยู่ใกล้เราแค่เอื้อม\",\n",
       " 'Now when I think about the possibilities for life out there,\\nตอนนี้พอฉันนึกถึงความเป็นได้ ที่จะมีสิ่งมีชีวิตอยู่นอกโลกนั้น',\n",
       " 'I think of the fact that our sun is but one of many stars.\\nฉันก็จะคิดถึงความจริงที่ว่า ดวงอาทิตย์ของเรานั้น เป็นเพียงหนึ่งในดาวฤกษ์จำนวนมากมาย',\n",
       " 'This is a photograph of a real galaxy,\\nนี่คือภาพถ่ายเสมือนจริงของกาแล็กซีหนึ่ง',\n",
       " 'we think our Milky Way looks like this galaxy.\\nซึ่งเราคิดว่าทางช้างเผือกของเรา มีหน้าตาประมาณนี้',\n",
       " \"It's a collection of bound stars.\\nเป็นกลุ่มดาวที่โคจรอยู่\",\n",
       " 'But our [sun] is one of hundreds of billions of stars\\nแต่ดวงอาทิตย์เป็นหนึ่งในดาวฤกษ์ หลายแสนล้านดวง',\n",
       " 'and our galaxy is one of upwards of hundreds of billions of galaxies.\\nและทางช้างเผือกของเรา ก็เป็นหนึ่งในอย่างน้อยแสนล้านกาแล็กซี',\n",
       " 'Knowing that small planets are very common,\\nเมื่อรู้ว่าดาวเคราะห์ดวงเล็กๆ นั้นมีอยู่ดาษดื่นทั่วไป',\n",
       " 'you can just do the math.\\nก็พอจะคำนวณได้แล้วนะคะ',\n",
       " 'And there are just so many stars and so many planets out there,\\nในเมื่อมีดาวฤกษ์และดาวเคราะห์ อยู่นอกโลกมากมาย',\n",
       " 'that surely, there must be life somewhere out there.\\nย่อมต้องมีสิ่งมีชีวิต อยู่ที่ไหนสักแห่งแน่นอน',\n",
       " 'Well, the biologists get furious with me for saying that,\\nนักชีววิทยาจะต้องโกรธฉันแน่ๆ ที่ฉันพูดแบบนี้',\n",
       " 'because we have absolutely no evidence for life beyond Earth yet.\\nเพราะตอนนี้เรายังไม่มีหลักฐานสักชิ้น ที่บ่งชี้ว่ามีสิ่งมีชีวิตนอกโลก',\n",
       " 'Well, if we were able to look at our galaxy from the outside\\nเอาล่ะค่ะ ถ้าเรามองกาแล็กซีของเรา จากข้างนอก',\n",
       " 'and zoom in to where our sun is,\\nแล้วขยายไปตรงที่ดวงอาทิตย์ของเราอยู่',\n",
       " 'we see a real map of the stars.\\nเราก็จะเห็นแผนที่ดาวเสมือนจริง',\n",
       " 'And the highlighted stars are those with known exoplanets.\\nและดาวดวงที่มีไฮไลต์ คือดวงที่เราพบดาวเคราะห์นอกระบบ',\n",
       " 'This is really just the tip of the iceberg.\\nนี่เป็นเพียงเสี้ยวหนึ่งจากทั้งหมดเท่านั้น',\n",
       " 'Here, this animation is zooming in onto our solar system.\\nคราวนี้ ภาพจะขยายไปที่ ระบบสุริยจักรวาลของเรา',\n",
       " \"And you'll see here the planets\\nคุณก็จะเห็นดาวเคราะห์ต่างๆ\",\n",
       " 'as well as some spacecraft that are also orbiting our sun.\\nกับยานอวกาศบางลำ ที่กำลังโคจรรอบดวงอาทิตย์เหมือนกัน',\n",
       " 'Now if we can imagine going to the West Coast of North America,\\nที่นี้ ถ้าเราจินตนาการว่ากำลัง อยู่ที่ชายฝั่งตะวันตกของอเมริกาเหนือ',\n",
       " 'and looking out at the night sky,\\nแล้วมองขึ้นไปบนท้องฟ้าตอนกลางคืน',\n",
       " \"here's what we'd see on a spring night.\\nนี่คือสิ่งที่เราจะเห็นในยามค่ำคืนของฤดูใบไม้ผลิ\",\n",
       " 'And you can see the constellations overlaid\\nคุณจะเห็นกลุ่มดาวจักราศีเรียงรายอยู่',\n",
       " 'and again, so many stars with planets.\\nและก็ดาวฤกษ์มากมายที่มีดาวเคราะห์ด้วย',\n",
       " \"There's a special patch of the sky where we have thousands of planets.\\nมีท้องฟ้าอยู่ปื้นหนึ่ง ที่มีดาวเคราะห์เป็นพันๆดวง\",\n",
       " 'This is where the Kepler Space Telescope focused for many years.\\nเป็นจุดที่กล้องโทรทรรศน์อวกาศเคปเลอร์ ให้ความสนใจมาหลายปีแล้ว',\n",
       " \"Let's zoom in and look at one of the favorite exoplanets.\\nเรามาลองซูมดูที่หนึ่งในดาวเคราะห์นอกระบบ ที่เป็นที่สนใจที่สุดกัน\",\n",
       " 'This star is called Kepler-186f.\\nตัวดาวฤกษ์นี้ชื่อว่า เคปเลอร์-186เอฟ',\n",
       " \"It's a system of about five planets.\\nเป็นระบบสุริยะที่มีดาวเคราะห์ประมาณห้าดวง\",\n",
       " \"And by the way, most of these exoplanets, we don't know too much about.\\nยังไงก็ตาม เราไม่ได้รู้อะไรมากมายเกี่ยวกับ ดาวเคราะห์นอกระบบพวกนี้สักเท่าไหร่\",\n",
       " 'We know their size, and their orbit and things like that.\\nเรารู้ขนาดและวงโคจร อะไรทำนองนั้น',\n",
       " \"But there's a very special planet here called Kepler-186f.\\nแต่มีดาวเคราะห์ดวงหนึ่งที่พิเศษกว่าใคร ชื่อว่า เคปเลอร์-186เอฟ\",\n",
       " 'This planet is in a zone that is not too far from the star,\\nอยู่ในโซนที่ไม่ไกลจากดาวฤกษ์เกินไป',\n",
       " 'so that the temperature may be just right for life.\\nทำให้อาจมีอุณหภูมิพอเหมาะสำหรับสิ่งมีชีวิต',\n",
       " \"Here, the artist's conception is just zooming in\\nที่เห็นอยู่นี้ เป็นภาพในจินตนาการของศิลปิน\",\n",
       " 'and showing you what that planet might be like.\\nกำลังขยายให้เห็นว่าดาวเคราะห์ อาจมีหน้าตายังไง',\n",
       " 'So, many people have this romantic notion of astronomers\\nหลายคนจินตนาการถึง นักดาราศาสตร์ไว้อย่างโรแมนติก',\n",
       " 'going to the telescope on a lonely mountaintop\\nว่าต้องไปส่องกล้องโทรทรรศน์ บนยอดเขาอันเงียบเหงา',\n",
       " 'and looking at the spectacular night sky through a big telescope.\\nแล้วเฝ้ามองท้องฟ้ายามค่ำคืนที่น่าตื่นตา ผ่านกล้องอันใหญ่',\n",
       " 'But actually, we just work on our computers like everyone else,\\nแต่จริงๆแล้ว เราก็แค่ทำงานอยู่ หน้าคอมพิวเตอร์เหมือนกับคนทั่วไป',\n",
       " 'and we get our data by email or downloading from a database.\\nแล้วมีข้อมูลส่งมาทางอีเมล  หรือไม่ก็ดาวน์โหลดมาจากฐานข้อมูล',\n",
       " 'So instead of coming here to tell you\\nเพราะงั้น แทนที่จะมาสาธยาย',\n",
       " 'about the somewhat tedious nature of the data and data analysis\\nเรื่องน่าเบื่อเกี่ยวกับข้อมูล กับการวิเคราะห์ข้อมูล',\n",
       " 'and the complex computer models we make,\\nและโมเดลคอมพิวเตอร์ซับซ้อนที่เราสร้าง',\n",
       " 'I have a different way to try to explain to you\\nฉันมีวิธีอื่นในการอธิบาย',\n",
       " \"some of the things that we're thinking about exoplanets.\\nบางสิ่งที่เรากำลังคิดกันอยู่ เกี่ยวกับดาวเคราะห์นอกระบบ\",\n",
       " \"Here's a travel poster:\\nนี่คือโปสเตอร์การท่องเที่ยว\",\n",
       " '\"Kepler-186f:\\nเคปเลอร์-186เอฟ',\n",
       " 'Where the grass is always redder on the other side.\"\\nที่ที่หญ้าของอีกบ้านแดงกว่าบ้านเรา  (เสียงหัวเราะ)',\n",
       " \"That's because Kepler-186f orbits a red star,\\nที่เป็นแบบนี้เพราะว่า เคปเลอร์-186เอฟ โคจรรอบดาวฤกษ์สีแดง\",\n",
       " \"and we're just speculating that perhaps the plants there,\\nเราจึงคาดกันว่า บางทีพืชบนดาวดวงนั้น\",\n",
       " 'if there is vegetation that does photosynthesis,\\nในกรณีที่มีพืชที่สังเคราะห์ด้วยแสง',\n",
       " 'it has different pigments and looks red.\\nก็จะมีรงควัตถุต่างออกไป มองดูเป็นสีแดง',\n",
       " '\"Enjoy the gravity on HD 40307g,\\nเพลิดเพลินกับแรงโน้มถ่วงบนดาวเอชดี 40307จี',\n",
       " 'a Super-Earth.\"\\nโลกเหนือโลกมนุษย์',\n",
       " 'This planet is more massive than Earth\\nดาวเคราะห์ดวงนี้มีมวลมากกว่าโลก',\n",
       " 'and has a higher surface gravity.\\nและมีแรงโน้มถ่วงที่พื้นผิวดาวมากกว่า',\n",
       " '\"Relax on Kepler-16b,\\nหย่อนใจที่ เคปเลอร์-16บี',\n",
       " 'where your shadow always has company.\"\\nที่ที่เงาของคุณไม่อยู่เดียวดาย',\n",
       " '(Laughter)\\n(เสียงหัวเราะ)',\n",
       " 'We know of a dozen planets that orbit two stars,\\nเราพบดาวเคราะห์ที่โคจร รอบดาวฤกษ์สองดวงเป็นโหลๆ',\n",
       " \"and there's likely many more out there.\\nและก็น่าจะมีมากกว่านี้อีกนอกจากนั้น\",\n",
       " 'If we could visit one of those planets,\\nถ้าเราอยู่บนดาวเคราะห์ประเภทนี้ได้',\n",
       " 'you literally would see two sunsets\\nคุณก็จะได้ดูพระอาทิตย์ตกดินสองรอบ',\n",
       " 'and have two shadows.\\nและมีสองเงาค่ะ',\n",
       " 'So actually, science fiction got some things right.\\nแสดงว่าจริงๆแล้วนิยายวิทยาศาสตร์ ก็พูดถูกเหมือนกัน',\n",
       " 'Tatooine from Star Wars.\\nอย่างดาวทาทูอีน จากสตาร์วอส์',\n",
       " 'And I have a couple of other favorite exoplanets\\nฉันอยากขอพูดถึง',\n",
       " 'to tell you about.\\nดาวเคราะห์นอกระบบอีกสองดวงที่ฉันชอบ',\n",
       " 'This one is Kepler-10b,\\nดวงนี้มีชื่อว่า เคปเลอร์-10บี',\n",
       " \"it's a hot, hot planet.\\nเป็นดาวที่ร้อน ร้อนมากๆ\",\n",
       " 'It orbits over 50 times closer to its star\\nมีวงโคจรใกล้กับดวงอาทิตย์ของตัวเอง',\n",
       " 'than our Earth does to our sun.\\nมากกว่าโลกของเราถึง 50 เท่า',\n",
       " \"And actually, it's so hot,\\nบนดาวจึงร้อนถึงขนาดที่\",\n",
       " \"we can't visit any of these planets, but if we could,\\nเราลงไปบนดาวดวงนี้ไม่ได้ แต่ถึงทำได้\",\n",
       " 'we would melt long before we got there.\\nกว่าจะไปถึงเราก็คงจะละลายไปก่อนแล้ว',\n",
       " 'We think the surface is hot enough to melt rock\\nคิดว่าพื้นผิวบนของดาวร้อนจนหินละลายได้เลย',\n",
       " 'and has liquid lava lakes.\\nและมีทะเลสาบลาวาอยู่ด้วย',\n",
       " 'Gliese 1214b.\\nกลีซ 1214 บี',\n",
       " 'This planet, we know the mass and the size\\nสำหรับดาวดวงนี้ เรารู้มวลกับขนาด',\n",
       " 'and it has a fairly low density.\\nและรู้ว่ามีความหนาแน่นค่อนข้างต่ำ',\n",
       " \"It's somewhat warm.\\nค่อนข้างจะอบอุ่น\",\n",
       " \"We actually don't know really anything about this planet,\\nจริงๆ แล้วพวกเราไม่รู้อะไรเลย เกี่ยวกับดาวดวงนี้\",\n",
       " \"but one possibility is that it's a water world,\\nแต่มีความเป็นไปได้อย่างหนึ่ง ว่าดาวดวงนี้เป็นโลกใต้น้ำ\",\n",
       " \"like a scaled-up version of one of Jupiter's icy moons\\nเหมือนกับเอาดวงจันทร์ที่เต็มไปด้วย น้ำแข็งของดาวพฤหัสมาขยายใหญ่\",\n",
       " 'that might be 50 percent water by mass.\\nซึ่งอาจมีปริมาณน้ำเทียบกับมวลถึงร้อยละ 50',\n",
       " 'And in this case, it would have a thick steam atmosphere\\nในกรณีนี้ชั้นบรรยากาศของดาวจะเป็นไอหนา',\n",
       " 'overlaying an ocean,\\nปกคลุมมหาสมุทร',\n",
       " 'not of liquid water,\\nที่ไม่ได้ประกอบด้วยน้ำ',\n",
       " 'but of an exotic form of water, a superfluid --\\nเป็นน้ำในรูปแบบที่ต่างออกไป เป็นของไหลยิ่งยวด',\n",
       " 'not quite a gas, not quite a liquid.\\nจะเป็นแก๊สก็ไม่ใช่ ของเหลวก็ไม่เชิง',\n",
       " \"And under that wouldn't be rock,\\nลึกลงไปข้างใต้ก็ไม่ใช่หิน\",\n",
       " 'but a form of high-pressure ice,\\nแต่เป็นน้ำแข็งความดันสูง',\n",
       " 'like ice IX.\\nเหมือนกับ ไอซ์ IX',\n",
       " 'So out of all these planets out there,\\nเพราะงั้น ในจำนวนดาวเคราะห์พวกนี้',\n",
       " 'and the variety is just simply astonishing,\\nที่เห็นว่ามีความหลากหลายน่าอัศจรรย์',\n",
       " 'we mostly want to find the planets that are Goldilocks planets, we call them.\\nเราหวังเป็นอย่างยิ่งว่าจะเจอดาวเคราะห์',\n",
       " 'Not too big, not too small,\\nซี่งไม่เล็กไม่ใหญ่จนเกินไป',\n",
       " 'not too hot, not too cold --\\nไม่ร้อนเกิน ไม่เย็นเกิน',\n",
       " 'but just right for life.\\nแต่เหมาะเจาะสำหรับสิ่งมีชีวิต',\n",
       " \"But to do that, we'd have to be able to look\\nแต่จะทำอย่างนั้นได้\",\n",
       " \"at the planet's atmosphere,\\nเราต้องมองเห็นชั้นบรรยากาศ ของดาวเคราะห์ได้\",\n",
       " 'because the atmosphere acts like a blanket trapping heat --\\nเพราะชั้นบรรยากาศเป็นเหมือนกับ ผ้าที่ห่มคลุมดักความร้อน',\n",
       " 'the greenhouse effect.\\nนั่นคือ ปรากฎการณ์เรือนกระจก',\n",
       " 'We have to be able to assess the greenhouse gases\\nเราต้องสามารถประเมินก๊าซเรือนกระจก',\n",
       " 'on other planets.\\nที่อยู่บนดาวเคราะห์ดวงอื่นได้',\n",
       " 'Well, science fiction got some things wrong.\\nนิยายวิทยาศาสตร์ก็พูดผิดได้บ้างค่ะ',\n",
       " 'The Star Trek Enterprise\\nยานเอนเตอร์ไพรส์ ในสตาร์เทร็ค',\n",
       " 'had to travel vast distances at incredible speeds\\nต้องเดินทางไกลลิบด้วยความเร็วสูง',\n",
       " 'to orbit other planets\\nเพื่อที่จะโคจรรอบดาวเคราะห์',\n",
       " 'so that First Officer Spock could analyze the atmosphere\\nรองกัปตันสป็อคจะได้วิเคราะห์ชั้นบรรยากาศ',\n",
       " 'to see if the planet was habitable\\nว่าดาวเคราะห์นั้นสามารถใช้อยู่อาศัยได้ไหม',\n",
       " 'or if there were lifeforms there.\\nหรือไม่ก็มีสิ่งมีชีวิตอยู่บนดาวไหม',\n",
       " \"Well, we don't need to travel at warp speeds\\nเราไม่จำเป็นต้องเคลื่อนที่ ถึงเร็วเหนือแสงก็ได้\",\n",
       " 'to see other planet atmospheres,\\nในการจะดูชั้นบรรยากาศของดาวเคราะห์อื่น',\n",
       " \"although I don't want to dissuade any budding engineers\\nถึงฉันไม่อยากจะหยุดวิศวกรรุ่นใหม่\",\n",
       " 'from figuring out how to do that.\\nไม่ให้หาวิธีเดินทางด้วยความเร็วขนาดนั้น',\n",
       " 'We actually can and do study planet atmospheres\\nจริงๆแล้วเราสามารถศึกษาชั้นบรรยากาศ ของดาวเคราะห์',\n",
       " 'from here, from Earth orbit.\\nจากที่นี่ จากวงโคจรของโลก',\n",
       " 'This is a picture, a photograph of the Hubble Space Telescope\\nภาพนี้คือภาพถ่ายของ กล้องโทรทรรศน์อวกาศฮับเบิล',\n",
       " 'taken by the shuttle Atlantis as it was departing\\nถ่ายโดยยานแอตแลนทิส ขณะออกเดินทาง',\n",
       " 'after the last human space flight to Hubble.\\nหลังครั้งสุดท้ายที่ส่ง นักบินอวกาศไปบนฮับเบิล',\n",
       " 'They installed a new camera, actually,\\nมีการติดตั้งกล้องตัวใหม่',\n",
       " 'that we use for exoplanet atmospheres.\\nไว้ใช้ดูชั้นบรรยากาศของดาวเคราะห์นอกระบบ',\n",
       " \"And so far, we've been able to study dozens of exoplanet atmospheres,\\nจนถึงตอนนี้เราสามารถศึกษาชั้นบรรยากาศ ของดาวเคราะห์นอกระบบหลายโหล\",\n",
       " 'about six of them in great detail.\\nมีหกดวงที่ได้ศึกษาอย่างละเอียด',\n",
       " 'But those are not small planets like Earth.\\nแต่ดาวพวกนี้ไม่ใช่ ดาวเคราะห์ดวงเล็กๆ อย่างโลก',\n",
       " \"They're big, hot planets that are easy to see.\\nเป็นดาวเคราะห์ดวงใหญ่และร้อน ซึ่งมองเห็นได้ง่าย\",\n",
       " \"We're not ready,\\nเรายังไม่พร้อม\",\n",
       " \"we don't have the right technology yet to study small exoplanets.\\nเรายังไม่มีเทคโนโลยีที่จะใช้ศึกษา ดาวเคราะห์นอกระบบขนาดเล็กได้\",\n",
       " 'But nevertheless,\\nแต่อย่างไรก็ดี',\n",
       " 'I wanted to try to explain to you how we study exoplanet atmospheres.\\nฉันอยากจะลองอธิบายว่าเราศึกษา ชั้นบรรยากาศของดาวเคราะห์นอกระบบยังไง',\n",
       " 'I want you to imagine, for a moment, a rainbow.\\nอยากให้ลองจินตนาการ นิดหนึ่งค่ะ ว่ามีสายรุ้ง',\n",
       " 'And if we could look at this rainbow closely,\\nและถ้ามองเข้าไปใกล้ๆสายรุ้งนี้ได้',\n",
       " 'we would see that some dark lines are missing.\\nก็จะเห็นว่าแถบสีบางช่วงนั้นหายไป',\n",
       " \"And here's our sun,\\nและนี่คือดวงอาทิตย์ของเรา\",\n",
       " 'the white light of our sun split up,\\nแสงสีขาวของดวงอาทิตย์จะถูกแยกออก',\n",
       " 'not by raindrops, but by a spectrograph.\\nไม่ใช่โดยน้ำฝน แต่โดยสเปกโทรกราฟ',\n",
       " 'And you can see all these dark, vertical lines.\\nคุณก็จะเห็นเส้นตั้งสีดำพวกนี้',\n",
       " 'Some are very narrow, some are wide,\\nบ้างก็แคบ บ้างก็กว้าง',\n",
       " 'some are shaded at the edges.\\nบ้างก็มีขอบมัว',\n",
       " 'And this is actually how astronomers have studied objects in the heavens,\\nและนี่คือวิธีการที่นักดาราศาสตร์ใช้ ศึกษาเหล่าวัตถุที่อยู่บนท้องฟ้า',\n",
       " 'literally, for over a century.\\nเป็นศตวรรษๆ',\n",
       " 'So here, each different atom and molecule\\nในภาพนี้ อะตอมและโมเลกุลที่แตกต่างกัน',\n",
       " 'has a special set of lines,\\nจะมีเส้นเป็นชุดๆ แตกต่างกัน',\n",
       " 'a fingerprint, if you will.\\nจะว่าเป็นเหมือนลายนิ้วมือก็ได้',\n",
       " \"And that's how we study exoplanet atmospheres.\\nและนี่คือวิธีที่ใช้ศึกษา ชั้นบรรยากาศของดาวเคราะห์นอกระบบ\",\n",
       " \"And I'll just never forget when I started working\\nและฉันจะลืมไม่ลงเลยค่ะ ว่าตอนที่เพิ่งเข้ามาทำงาน\",\n",
       " 'on exoplanet atmospheres 20 years ago,\\nศึกษาชั้นบรรยากาศนี้ เมื่อ 20 ปีที่แล้ว',\n",
       " 'how many people told me,\\nผู้คนมากมายบอกฉันว่า',\n",
       " '\"This will never happen.\\nงานนี้มันเป็นไปไม่ได้',\n",
       " 'We\\'ll never be able to study them. Why are you bothering?\"\\nคุณศึกษาของพวกนี้ไม่ได้หรอก จะเสียเวลาอยู่ทำไม',\n",
       " \"And that's why I'm pleased to tell you about all the atmospheres studied now,\\nเพราะงั้นฉันจึงยินดีที่จะบอกคุณเกี่ยวกับ ชั้นบรรยากาศต่างๆ ที่ศึกษาอยู่ตอนนี้\",\n",
       " 'and this is really a field of its own.\\nซึ่งกลายเป็นงานอีกสาขาหนึ่งไปแล้ว',\n",
       " 'So when it comes to other planets, other Earths,\\nพอพูดถึงดาวเคราะห์ดวงอื่น โลกอีกโลกหนึ่ง',\n",
       " 'in the future when we can observe them,\\nในอนาคตที่เราสามารถสำรวจบนนั้นได้',\n",
       " 'what kind of gases would we be looking for?\\nก๊าซชนิดไหนที่เราจะมองหา',\n",
       " 'Well, you know, our own Earth has oxygen in the atmosphere\\nแบบ โลกของเราเองมีออกซิเจน อยู่ในชั้นบรรยากาศ',\n",
       " 'to 20 percent by volume.\\nประมาณร้อยละ 20 ต่อปริมาตร',\n",
       " \"That's a lot of oxygen.\\nเป็นออกซิเจนจำนวนมหาศาลเลย\",\n",
       " 'But without plants and photosynthetic life,\\nแต่หากไม่มีพืช และสิ่งมีชีวิตที่สังเคราะห์แสง',\n",
       " 'there would be no oxygen,\\nก็จะไม่มีออกซิเจน',\n",
       " 'virtually no oxygen in our atmosphere.\\nไม่มีออกซิเจนเลย ในชั้นบรรยากาศของเรา',\n",
       " 'So oxygen is here because of life.\\nดังนั้นที่มีออกซิเจน ก็เพราะมีสิ่งมีชีวิต',\n",
       " 'And our goal then is to look for gases in other planet atmospheres,\\nและเป้าหมายของเราคือหาก๊าซ ในชั้นบรรยากาศของดาวเคราะห์อื่น',\n",
       " \"gases that don't belong,\\nก๊าซที่ปกติจะไม่มี\",\n",
       " 'that we might be able to attribute to life.\\nซึ่งเราอาจใช้แสดงว่ามีสิ่งชีวิตได้',\n",
       " 'But which molecules should we search for?\\nแต่โมเลกุลอันไหนล่ะคะที่เราควรเสาะหา',\n",
       " 'I actually told you how diverse exoplanets are.\\nฉันได้กล่าวไปแล้วว่าดาวเคราะห์นอกระบบ แตกต่างกันขนาดไหน',\n",
       " 'We expect that to continue in the future\\nเราคาดว่าก็จะเป็นอย่างนี้ในอนาคต',\n",
       " 'when we find other Earths.\\nเมื่อเราจะเจอโลกอื่นๆ',\n",
       " \"And that's one of the main things I'm working on now,\\nและนั่นเป็นหนึ่งในงานหลัก ของฉันในตอนนี้ค่ะ\",\n",
       " 'I have a theory about this.\\nฉันมีทฤษฎีเรื่องนี้อยู่',\n",
       " 'It reminds me that nearly every day,\\nว่าแล้วก็นึกขึ้นได้ว่าทุกๆ วัน',\n",
       " 'I receive an email or emails\\nฉันจะได้รับอีเมลอย่างน้อยฉบับนึง',\n",
       " 'from someone with a crazy theory about physics of gravity\\nจากใครสักคนเกี่ยวกับทฤษฎีประหลาดๆ เรื่องแรงโน้มถ่วง',\n",
       " 'or cosmology or some such.\\nหรือดาราศาสตร์ ไม่ก็อะไรทำนองนี้',\n",
       " \"So, please don't email me one of your crazy theories.\\nเพราะฉะนั้น โปรดอย่าส่งทฤษฎีสติเฟื่อง ของพวกคุณมาหาฉันเลยนะคะ\",\n",
       " '(Laughter)\\n(เสียงหัวเราะ)',\n",
       " 'Well, I had my own crazy theory.\\nฉันเคยมีทฤษฎีสติเฟื่องของตัวเอง',\n",
       " 'But, who does the MIT professor go to?\\nแต่ศาสตราจารย์เอ็มไอทีจะไปปรึกษาใครได้',\n",
       " 'Well, I emailed a Nobel Laureate in Physiology or Medicine\\nฉันก็เลยอีเมลไปหาผู้ได้รับรางวัลโนเบล สาขาสรีรวิทยาหรือแพทยศาสตร์',\n",
       " 'and he said, \"Sure, come and talk to me.\"\\nและเขาก็บอกว่า \"เอาสิ มาคุยกับผม\"',\n",
       " 'So I brought my two biochemistry friends\\nฉันก็เลยหนีบเพื่อนนักชีวเคมีไปด้วยสองคน',\n",
       " 'and we went to talk to him about our crazy theory.\\nแล้วเราก็พูดกับเขาเรื่องทฤษฎีบ้าๆ ของเรา',\n",
       " 'And that theory was that life produces all small molecules,\\nทฤษฎีนี้กล่าวว่าสิ่งมีชีวิตผลิต โมเลกุลเล็กๆ ทุกโมเลกุล',\n",
       " 'so many molecules.\\nเป็นโมเลกุลจำนวนมาก',\n",
       " 'Like, everything I could think of, but not being a chemist.\\nเท่าที่ฉันจะนึกออก แต่ในฐานะที่ไม่ใช่นักเคมี',\n",
       " 'Think about it:\\nคิดดูสิคะ',\n",
       " 'carbon dioxide, carbon monoxide,\\nคาร์บอนไดออกไซด์ คาร์บอนมอนอกไซด์',\n",
       " 'molecular hydrogen, molecular nitrogen,\\nโมเลกุลของไฮโดรเจน โมเลกุลของไนโตรเจน',\n",
       " 'methane, methyl chloride --\\nมีเทน เมทิลคลอไรด์',\n",
       " 'so many gases.\\nก๊าซมากมาย',\n",
       " 'They also exist for other reasons,\\nพวกนี้ก็มีอยู่เพื่อเหตุผลอื่นบ้าง',\n",
       " 'but just life even produces ozone.\\nแต่มีสิ่งมีชีวิตที่ผลิตแม้แต่โอโซน',\n",
       " 'So we go to talk to him about this,\\nเราจึงพูดกับเขาเรื่องนี้',\n",
       " 'and immediately, he shot down the theory.\\nและเขาก็เขวี้ยงทฤษฎีนี้ทิ้งทันที',\n",
       " \"He found an example that didn't exist.\\nเขาพบตัวอย่างที่ไม่มีอยู่จริง\",\n",
       " 'So, we went back to the drawing board\\nเราก็เลยต้องพับทฤษฎีนี้ไป',\n",
       " 'and we think we have found something very interesting in another field.\\nและคิดว่าได้เจอของน่าสนใจในสาขาอื่นแล้ว',\n",
       " 'But back to exoplanets,\\nกลับมาที่ดาวเคราะห์นอกระบบ',\n",
       " 'the point is that life produces so many different types of gases,\\nประเด็นอยู่ที่ว่าสิ่งมีชีวิต ผลิตก๊าซหลากหลายชนิด',\n",
       " 'literally thousands of gases.\\nพูดได้ว่าเป็นพันๆ ชนิด',\n",
       " \"And so what we're doing now is just trying to figure out\\nงานของเราในตอนนี้จึงกลายเป็นการหา\",\n",
       " 'on which types of exoplanets,\\nว่าบนดาวเคราะห์นอกระบบแบบไหน',\n",
       " 'which gases could be attributed to life.\\nก๊าซชนิดใดที่สามารถบ่งว่ามีสิ่งชีวิตได้',\n",
       " 'And so when it comes time when we find gases\\nพอเรารู้แล้วว่ามีก๊าซ',\n",
       " 'in exoplanet atmospheres\\nในชั้นบรรยากาศของดาวเคราะห์',\n",
       " \"that we won't know if they're being produced\\nซึ่งเราไม่รู้ได้ว่าผลิตโดย\",\n",
       " 'by intelligent aliens or by trees,\\nเอเลียนทรงปัญญา หรือโดยต้นไม้',\n",
       " 'or a swamp,\\nหรือหนองน้ำ',\n",
       " 'or even just by simple, single-celled microbial life.\\nหรือแค่จุลชีวันเซลล์เดียว',\n",
       " 'So working on the models\\nการทำงานไปตามโมเดล',\n",
       " 'and thinking about biochemistry,\\nและขบคิดโดยชีวเคมี',\n",
       " \"it's all well and good.\\nนั่นเป็นเรื่องที่ดี\",\n",
       " 'But a really big challenge ahead of us is: how?\\nแต่งานช้างของจริงรออยู่ใน คำถามที่ว่า ทำยังไง',\n",
       " 'How are we going to find these planets?\\nทำยังไงเราจะหาดาวเคราะห์พวกนี้ได้',\n",
       " 'There are actually many ways to find planets,\\nมีตั้งหลายวิธีที่จะค้นหาดาวเคราะห์',\n",
       " 'several different ways.\\nหลายวิธีแตกต่างกันไป',\n",
       " \"But the one that I'm most focused on is how can we open a gateway\\nแต่วิธีหนึ่งที่ฉันเน้นคือวิธีการ ที่เราจะเปิดประตู\",\n",
       " 'so that in the future,\\nเพื่อที่ในอนาคต',\n",
       " 'we can find hundreds of Earths.\\nเราจะสามารถค้นพบโลกเป็นร้อยๆ ดวงได้',\n",
       " 'We have a real shot at finding signs of life.\\nเรามีโอกาสไม่น้อยที่จะพบ ร่องรอยของสิ่งมีชีวิต',\n",
       " 'And actually, I just finished leading a two-year project\\nจริงๆ แล้ว ฉันเพิ่งจะทำโครงการสองปีเสร็จ',\n",
       " 'in this very special phase\\nในขั้นตอนสำคัญ',\n",
       " 'of a concept we call the starshade.\\nของแนวคิดที่เราเรียกกันว่า กำบังแสงดาว',\n",
       " 'And the starshade is a very specially shaped screen\\nซึ่งเป็นแผงที่มีรูปร่างพิเศษ',\n",
       " 'and the goal is to fly that starshade\\nมีเป้าหมายคือการปล่อยมันให้ลอยออกไป',\n",
       " 'so it blocks out the light of a star\\nบังแสงจากดาวฤกษ์',\n",
       " 'so that the telescope can see the planets directly.\\nกล้องโทรทรรศน์จะได้ ส่องเห็นดาวเคราะห์โดยตรง',\n",
       " 'Here, you can see myself and two team members\\nในรูป คุณจะเห็นฉันกับสมาชิกในทีมสองคน',\n",
       " 'holding up one small part of the starshade.\\nกำลังยกส่วนเล็กๆ ส่วนหนึ่งของกำบังแสงดาว',\n",
       " \"It's shaped like a giant flower,\\nมันมีรูปร่างเหมือนดอกไม้ขนาดยักษ์\",\n",
       " 'and this is one of the prototype petals.\\nและอันนี้คือหนึ่งในกลีบต้นแบบของมัน',\n",
       " 'The concept is that a starshade and telescope could launch together,\\nแนวคิดของเราคือ จะส่งกำบังแสงดาว กับกล้องโทรทรรศน์ไปด้วยกัน',\n",
       " 'with the petals unfurling from the stowed position.\\nโดยตัวกลีบจะคลี่ออกจากตำแหน่งที่เก็บไว้',\n",
       " 'The central truss would expand,\\nโครงยึดตรงส่วนกลางจะขยายออก',\n",
       " 'with the petals snapping into place.\\nพร้อมกับที่กลีบเลื่อนเข้าที่',\n",
       " 'Now, this has to be made very precisely,\\nนี่จะต้องทำอย่างละเอียดแม่นยำ',\n",
       " 'literally, the petals to microns\\nแม่นยำมากๆ ตัวกลีบคิดเป็นไมครอน',\n",
       " 'and they have to deploy to millimeters.\\nและต้องปล่อยออกไปแม่นยำเป็นมิลลิเมตร',\n",
       " 'And this whole structure would have to fly\\nโครงสร้างทั้งหมดนี้จะต้องลอย',\n",
       " 'tens of thousands of kilometers away from the telescope.\\nห่างเป็นหมื่นๆ กิโลเมตรจากกล้องโทรทรรศน์',\n",
       " \"It's about tens of meters in diameter.\\nเส้นผ่าศูนย์กลางยาวหลายสิบเมตร\",\n",
       " 'And the goal is to block out the starlight to incredible precision\\nโดยมีเป้าหมายเพื่อกั้นแสงจากดาวฤกษ์ ด้วยความแม่นยำเหลือเชื่อ',\n",
       " \"so that we'd be able to see the planets directly.\\nให้เราสามารถมองเห็นดาวเคราะห์ได้โดยตรง\",\n",
       " 'And it has to be a very special shape,\\nและมันต้องมีรูปร่างพิเศษ',\n",
       " 'because of the physics of defraction.\\nตามหลักฟิสิกส์ของการเลี้ยวเบนของแสง',\n",
       " 'Now this is a real project that we worked on,\\nนี้คือโครงการจริง ๆ ที่เรากำลังทำกันอยู่',\n",
       " 'literally, you would not believe how hard.\\nคุณจะไม่เชื่อเลยว่ามันยากขนาดไหน',\n",
       " \"Just so you believe it's not just in movie format,\\nเพื่อให้คุณไม่คิดว่ามันมีแต่ในหนัง\",\n",
       " \"here's a real photograph\\nนี่คือภาพจริง\",\n",
       " 'of a second-generation starshade deployment test bed in the lab.\\nของแท่นทดสอบการ ใช้กำบังแสงดาวรุ่นที่สองในแล็บ',\n",
       " 'And in this case, I just wanted you to know\\nกรณีนี้ฉันอยากให้คุณรู้ว่า',\n",
       " 'that that central truss has heritage left over\\nส่วนโครงยึดหลักเป็นมรดกตกทอด',\n",
       " 'from large radio deployables in space.\\nจากโครงสร้างวิทยุขนาดใหญ่ในอวกาศ',\n",
       " 'So after all of that hard work\\nหลังจากเราพยายามอย่างหนัก',\n",
       " 'where we try to think of all the crazy gases that might be out there,\\nเพื่อขบคิดถึงก๊าซทั้งหลายแหล่ ที่อาจอยู่นอกโลกนั้น',\n",
       " 'and we build the very complicated space telescopes\\nและสร้างกล้องโทรทรรศน์อวกาศที่ซับซ้อนนี้',\n",
       " 'that might be out there,\\nซึ่งจะไปอยู่นอกโลกนั้น',\n",
       " 'what are we going to find?\\nเราจะได้พบอะไรเหรอคะ',\n",
       " 'Well, in the best case,\\nอย่างดีที่สุด',\n",
       " 'we will find an image of another exo-Earth.\\nเราจะได้ภาพของโลกอื่น',\n",
       " 'Here is Earth as a pale blue dot.\\nนี่คือภาพโลกเป็นจุดสีฟ้าซีดๆ',\n",
       " 'And this is actually a real photograph of Earth\\nและนี่คือภาพถ่ายจริง ๆ ของโลก',\n",
       " 'taken by the Voyager 1 spacecraft,\\nถ่ายโดยยานวอยเอจเจอร์ 1',\n",
       " 'four billion miles away.\\nที่อยู่ห่างออกไปสี่พันล้านไมล์',\n",
       " 'And that red light is just scattered light in the camera optics.\\nแสงสีแดงนั้นเป็นเแค่แสงกระเจิงในกล้อง',\n",
       " \"But what's so awesome to consider\\nแต่จะเจ๋งมากถ้าเราลองนึกดูว่า\",\n",
       " 'is that if there are intelligent aliens\\nถ้ามีพวกเอเลียนทรงปัญญา',\n",
       " 'orbiting on a planet around a star near to us\\nอยู่บนดาวเคราะห์ที่โคจร รอบดาวฤกษ์ใกล้ๆ กับพวกเรา',\n",
       " 'and they build complicated space telescopes\\nและได้สร้างกล้องโทรทรรศน์อวกาศอันซับซ้อน',\n",
       " \"of the kind that we're trying to build,\\nแบบเดียวกับที่เราพยายามจะสร้างกันอยู่\",\n",
       " \"all they'll see is this pale blue dot,\\nพวกเขาก็จะเห็นแค่จุดสีฟ้าซีดๆ นี้\",\n",
       " 'a pinprick of light.\\nเป็นจุดแสงอันกระจึ๋งเดียว',\n",
       " 'And so sometimes, when I pause to think\\nเพราะงั้น บางครั้งพอฉันหยุดคิด',\n",
       " 'about my professional struggle and huge ambition,\\nถึงอุปสรรคในงาน กับความทะเยอทะยานอันยิ่งใหญ่',\n",
       " \"it's hard to think about that\\nมันยากที่จะคิด\",\n",
       " 'in contrast to the vastness of the universe.\\nเทียบว่าจักรวาลของเรากว้างใหญ่กว่าเพียงใด',\n",
       " 'But nonetheless, I am devoting the rest of my life\\nยังไงก็ตามฉันจะอุทิศชีวิต ทั้งหมดที่เหลือ',\n",
       " 'to finding another Earth.\\nเพื่อการตามหาโลกอีกโลกหนึ่ง',\n",
       " 'And I can guarantee\\nและฉันขอรับประกันเลยค่ะ',\n",
       " 'that in the next generation of space telescopes,\\nว่ากล้องโทรทรรศน์อวกาศรุ่นต่อไป',\n",
       " 'in the second generation,\\nในรุ่นที่สอง',\n",
       " 'we will have the capability to find and identity other Earths.\\nเราจะสามารถค้นหาและระบุ โลกอื่นๆ ได้',\n",
       " 'And the capability to split up the starlight\\nและมีความสามารถแยกแสงจากดาวฤกษ์',\n",
       " 'so that we can look for gases\\nเพื่อให้เราสามารถมองดูก๊าซต่างๆ',\n",
       " 'and assess the greenhouse gases in the atmosphere,\\nและระบุก๊าซเรือนกระจกในชั้นบรรยากาศ',\n",
       " 'estimate the surface temperature,\\nประมาณค่าอุณหภูมิบนพื้นผิว',\n",
       " 'and look for signs of life.\\nและหาร่องรอยของสิ่งมีชีวิต',\n",
       " \"But there's more.\\nแต่ไม่ใช่แค่นั้น\",\n",
       " 'In this case of searching for other planets like Earth,\\nการค้นหาดาวเคราะห์ดวงอื่นๆ ที่เหมือนกับโลกนี้',\n",
       " 'we are making a new kind of map\\nเรายังได้สร้างแผนที่ชนิดใหม่',\n",
       " 'of the nearby stars and of the planets orbiting them,\\nของดาวฤกษ์ใกล้ๆ กับเรา และดาวเคราะห์ที่โคจรรอบๆ',\n",
       " 'including [planets] that actually might be inhabitable by humans.\\nรวมไปถึงดาวเคราะห์ที่ มนุษย์อาจจะอาศัยอยู่ได้',\n",
       " 'And so I envision that our descendants,\\nฉันจึงคาดหวังว่ารุ่นลูกรุ่นหลานของพวกเรา',\n",
       " 'hundreds of years from now,\\nในอีกหลายร้อยปีข้างหน้า',\n",
       " 'will embark on an interstellar journey to other worlds.\\nจะออกเดินทางข้ามดวงดาว ไปยังโลกอื่น',\n",
       " 'And they will look back at all of us\\nและจะย้อนกลับมามองพวกเรา',\n",
       " 'as the generation who first found the Earth-like worlds.\\nในฐานะคนรุ่นแรกที่พบโลกใหม่ๆ',\n",
       " 'Thank you.\\nขอบคุณค่ะ',\n",
       " '(Applause)\\n(เสียงปรบมือ)',\n",
       " 'June Cohen: And I give you, for a question,\\nจูน โคเฮน : สำหรับคำถาม ขอเชิญ',\n",
       " 'Rosetta Mission Manager Fred Jansen.\\nผู้ควบคุมฝ่ายปฏิบัติงานโครงการโรเซตตา เฟรด แจนเซนค่ะ',\n",
       " 'Fred Jansen: You mentioned halfway through\\nเฟรด แจนเซน : คุณได้กล่าวไว้ตอนกลางๆ',\n",
       " 'that the technology to actually look at the spectrum\\nว่าเรายังไม่มีเทคโนโลยี ที่ใช้ในการสังเกตสเปกตรัม',\n",
       " 'of an exoplanet like Earth is not there yet.\\nของดาวเคราะห์ที่เหมือนกับโลก',\n",
       " 'When do you expect this will be there,\\nคุณคาดว่าเมื่อไหร่เราจะมีเทคโนโลยีอันนี้',\n",
       " \"and what's needed?\\nและอะไรบ้างที่จำเป็นครับ\",\n",
       " 'Actually, what we expect is what we call our next-generation Hubble telescope.\\nจริงๆ เราคาดว่าจะเห็นสิ่งที่เราเรียกว่า กล้องโทรทรรศ์ฮับเบิลรุ่นต่อไป',\n",
       " 'And this is called the James Webb Space Telescope,\\nมีชื่อว่ากล้องโทรทรรศน์อวกาศเจมส์ เวบบ์',\n",
       " 'and that will launch in 2018,\\nจะมีการส่งออกไปในปี ค.ศ. 2018',\n",
       " \"and that's what we're going to do,\\nนั่นเป็นสิ่งที่เราจะทำ\",\n",
       " \"we're going to look at a special kind of planet\\nเราจะส่องดูดาวเคราะห์ ชนิดพิเศษ ชนิดหนึ่ง\",\n",
       " 'called transient exoplanets,\\nเรียกว่า ดาวเคราะห์นอกแบบแทรนเซียน (transient exoplanet)',\n",
       " 'and that will be our first shot at studying small planets\\nและนั่นจะเป็นก้าวแรกของเรา ในการศึกษาดาวเคราะห์ขนาดเล็ก',\n",
       " 'for gases that might indicate the planet is habitable.\\nหาก๊าซที่จะบอกได้ว่า ดาวนั้นสามารถใช้อยู่ได้',\n",
       " \"JC: I'm going to ask you one follow-up question, too, Sara,\\nจ.ค. : ฉันอยากถามคุณเหมือนกันค่ะ ซารา  เป็นคำถามสืบเนื่อง\",\n",
       " 'as the generalist.\\nในฐานะผู้มีความรู้ทั่วไป',\n",
       " 'So I am really struck by the notion in your career\\nฉันนี่ตะลึงไปเลย ที่คุณพูดถึงอาชีพของคุณ',\n",
       " 'of the opposition you faced,\\nเรื่องฝ่ายตรงข้ามที่คุณเผชิญ',\n",
       " 'that when you began thinking about exoplanets,\\nว่าเมื่อคุณเริ่มคิดเรื่อง ดาวเคราะห์นอกระบบ',\n",
       " 'there was extreme skepticism in the scientific community\\nก็มีข้อกังขารุนแรงใน แวดวงวิทยาศาสตร์',\n",
       " 'that they existed,\\nว่าดาวพวกนี้มีจริงไหม',\n",
       " 'and you proved them wrong.\\nและคุณก็พิสูจน์ว่าพวกเขาผิด',\n",
       " 'What did it take to take that on?\\nคุณทำอย่างนั้นได้ยังไงคะ',\n",
       " 'SS: Well, the thing is that as scientists,\\nเรื่องของเรื่องคือในฐานะนักวิทยาศาสตร์',\n",
       " \"we're supposed to be skeptical,\\nเราจะต้องเป็นคนขี้สงสัยค่ะ\",\n",
       " 'because our job to make sure that what the other person is saying\\nเพราะงานของเราคือการพิสูจน์ว่า สิ่งที่คนอื่นพูดมานั้น',\n",
       " 'actually makes sense or not.\\nเข้าทีหรือเปล่า',\n",
       " 'But being a scientist,\\nแต่การเป็นนักวิทยาศาสตร์',\n",
       " \"I think you've seen it from this session,\\nที่ฉันว่าคุณได้เห็นแล้ว\",\n",
       " \"it's like being an explorer.\\nก็เหมือนกับเป็นนักสำรวจ\",\n",
       " 'You have this immense curiosity,\\nคุณมีความอยากรู้อยากเห็นเป็นที่สุด',\n",
       " 'this stubbornness,\\nมีความรั้น',\n",
       " 'this sort of resolute will that you will go forward\\nมีความตั้งใจแน่วแน่ที่จะก้าวไปข้างหน้า',\n",
       " 'no matter what other people say.\\nไม่ว่าคนอื่นจะพูดว่าอย่างไรค่ะ',\n",
       " 'JC: I love that. Thank you, Sara.\\nจ.ค. : นั่นวิเศษมาก ขอบคุณค่ะ คุณซารา',\n",
       " '(Applause)\\n(เสียงปรบมือ)']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
