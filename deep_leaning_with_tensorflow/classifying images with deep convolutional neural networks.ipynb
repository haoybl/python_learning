{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a CNN in the TensorFlow low-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data = mnist.input_data.read_data_sets(train_dir='MNIST_data/', \n",
    "                                       one_hot=True, reshape=False, validation_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train.num_examples, data.test.num_examples, data.validation.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 28, 28, 1), (50000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train.images.shape, data.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input_tensor, kernel_shape, n_out_channels, scope_name, \n",
    "               padding_mode='SAME', strides=(1, 1, 1, 1)):\n",
    "    \n",
    "    with tf.variable_scope(scope_name):\n",
    "        m, n1, n2, nc = input_tensor.get_shape().as_list()\n",
    "        p1, p2 = kernel_shape\n",
    "        \n",
    "        kernel = tf.get_variable('kernel', shape=(p1, p2, nc, n_out_channels))\n",
    "        bias = tf.get_variable('bias', shape=(n_out_channels), initializer=tf.zeros_initializer)\n",
    "        \n",
    "        output = tf.nn.conv2d(input_tensor, kernel, padding=padding_mode, strides=strides)\n",
    "        output = tf.nn.bias_add(output, bias, name='net_input')\n",
    "        output = tf.nn.relu(output, name='activation')\n",
    "        return output\n",
    "    \n",
    "def fc_layer(input_tensor, n_output, scope_name, activation_func=None):\n",
    "    with tf.variable_scope(scope_name):\n",
    "        input_shape = input_tensor.get_shape().as_list()[1:]\n",
    "        if len(input_shape) > 1:\n",
    "            # flatten to vector\n",
    "            n_input = np.prod(input_shape)\n",
    "            input_tensor = tf.reshape(input_tensor, shape=(-1, n_input))\n",
    "        else:\n",
    "            n_input = input_shape[0]\n",
    "            \n",
    "        # create W and b\n",
    "        weight = tf.get_variable('weight', shape=(n_input, n_output))\n",
    "        bias = tf.get_variable('bias', shape=(n_output), initializer=tf.zeros_initializer)\n",
    "        \n",
    "        # operation\n",
    "        output = tf.nn.bias_add(tf.matmul(input_tensor, weight), bias, name='net_input')\n",
    "        if activation_func is None:\n",
    "            return output\n",
    "        return activation_func(output, name='activation')\n",
    "    \n",
    "    \n",
    "def build_cnn():\n",
    "    \n",
    "    # create placeholder for X, Y\n",
    "    tf_X = tf.placeholder(dtype=tf.float32, shape=(None, 28, 28, 1), name='tf_X')\n",
    "    tf_Y = tf.placeholder(dtype=tf.float32, shape=(None, 10), name='tf_Y')\n",
    "    \n",
    "    # conv_1\n",
    "    h1 = conv_layer(tf_X, kernel_shape=(5, 5), n_out_channels=32, scope_name='conv1', \n",
    "                       padding_mode='SAME', strides=(1,1,1,1))\n",
    "    # max_pool_1\n",
    "    h1_maxpool = tf.nn.max_pool(h1, ksize=(1, 2, 2, 1), \n",
    "                                strides=(1, 2, 2, 1), padding='VALID')\n",
    "    # conv_2\n",
    "    h2 = conv_layer(h1_maxpool, kernel_shape=(5, 5), n_out_channels=64, scope_name='conv2',\n",
    "                   padding_mode='SAME', strides=(1, 1, 1, 1))\n",
    "    # max_pool_2\n",
    "    h2_maxpool = tf.nn.max_pool(h2, ksize=(1, 2, 2, 1), \n",
    "                                strides=(1, 2, 2, 1), padding='VALID')\n",
    "    # fc_1\n",
    "    h3 = fc_layer(h2_maxpool, n_output=1024, \n",
    "                  scope_name='fc1', activation_func=tf.nn.relu)\n",
    "    # dropout\n",
    "    dropout = tf.placeholder(dtype=tf.float32, shape=(), name='keep_prob')\n",
    "    h3_dropout = tf.nn.dropout(h3, keep_prob=dropout, name='dropout_layer')\n",
    "    # fc_2\n",
    "    logits = fc_layer(h3_dropout, n_output=10, \n",
    "                      scope_name='fc2', activation_func=None)\n",
    "    \n",
    "    # predictions: {probabilities, labels}\n",
    "    predictions = {\n",
    "        'probabilities': tf.nn.softmax(logits, name='probabilities'),\n",
    "        'labels': tf.argmax(logits, axis=1, name='labels')\n",
    "    }\n",
    "    # loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_Y, logits=logits), \n",
    "                          name='cross_entropy_loss')\n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, name='train_op')\n",
    "    \n",
    "    # metric: accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(tf_Y, axis=1), predictions['labels'])\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32), \n",
    "                              name='accuracy')\n",
    "    \n",
    "def save(saver, sess, epoch, path='./model/'):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    print('Saving model in', path)\n",
    "    saver.save(sess, save_path=os.path.join(path, 'cnn-model.ckpt'), \n",
    "               global_step=epoch)\n",
    "\n",
    "def load(saver, sess, path, epoch):\n",
    "    print(\"Loading model from\", path)\n",
    "    saver.restore(sess, path=os.path.join(path, \n",
    "                                          'cnn-model.ckpt-{}'.format(epoch)))\n",
    "\n",
    "def train(sess, training_set, validation_set=None,\n",
    "          initialize=True, epochs=20, shuffle=True,\n",
    "          dropout=.5, random_seed=None):\n",
    "    n_batch = training_set.num_examples // 64 + 1\n",
    "    \n",
    "    if initialize:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    training_loss = []\n",
    "    for i in range(epochs):\n",
    "        avg_loss = 0.0\n",
    "        for j in range(n_batch):\n",
    "            x_batch, y_batch = training_set.next_batch(batch_size=64)\n",
    "            feed = {'tf_X:0':x_batch, \n",
    "                    'tf_Y:0':y_batch, \n",
    "                    'keep_prob:0':dropout}\n",
    "            loss, _ = sess.run(['cross_entropy_loss:0', 'train_op'], feed_dict=feed)\n",
    "            avg_loss += loss\n",
    "        training_loss.append(avg_loss / (j+1))\n",
    "        print(\"Epoch {}: Training Loss {:.4f}\".format(i+1, training_loss[-1]), end=' ')\n",
    "        \n",
    "        if validation_set is not None:\n",
    "            feed = {'tf_X:0': validation_set.images,\n",
    "                    'tf_Y:0': validation_set.labels,\n",
    "                    'keep_prob:0': 1.0}\n",
    "            loss = sess.run('cross_entropy_loss:0', feed_dict=feed)\n",
    "            print(\"Validation Loss {:.4f}\".format(loss))\n",
    "        else:\n",
    "            print()\n",
    "        \n",
    "def predict(sess, X_test, return_proba=False):\n",
    "    feed = {'tf_X:0': X_test.images,\n",
    "            'tf_Y:0': X_test.labels,\n",
    "            'keep_prob:0': 1.0}\n",
    "    if return_proba:\n",
    "        return sess.run('probabilities', feed_dict=feed)\n",
    "    return sess.run('labels', feed_dict=feed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "random_seed = 123\n",
    "batch_size = 64\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    build_cnn()\n",
    "    \n",
    "    #file_writer = tf.summary.FileWriter(logdir='./logs/cnn', graph=g)\n",
    "    \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss 0.3705 Validation Loss 0.1171\n",
      "Epoch 2: Training Loss 0.1000 Validation Loss 0.0726\n",
      "Epoch 3: Training Loss 0.0696 Validation Loss 0.0571\n",
      "Epoch 4: Training Loss 0.0551 Validation Loss 0.0516\n",
      "Epoch 5: Training Loss 0.0457 Validation Loss 0.0440\n",
      "Epoch 6: Training Loss 0.0375 Validation Loss 0.0396\n",
      "Epoch 7: Training Loss 0.0343 Validation Loss 0.0443\n",
      "Epoch 8: Training Loss 0.0290 Validation Loss 0.0379\n",
      "Epoch 9: Training Loss 0.0255 Validation Loss 0.0394\n",
      "Epoch 10: Training Loss 0.0239 Validation Loss 0.0353\n",
      "Epoch 11: Training Loss 0.0199 Validation Loss 0.0331\n",
      "Epoch 12: Training Loss 0.0179 Validation Loss 0.0346\n",
      "Epoch 13: Training Loss 0.0164 Validation Loss 0.0392\n",
      "Epoch 14: Training Loss 0.0142 Validation Loss 0.0347\n",
      "Epoch 15: Training Loss 0.0133 Validation Loss 0.0330\n",
      "Epoch 16: Training Loss 0.0113 Validation Loss 0.0321\n",
      "Epoch 17: Training Loss 0.0102 Validation Loss 0.0364\n",
      "Epoch 18: Training Loss 0.0108 Validation Loss 0.0365\n",
      "Epoch 19: Training Loss 0.0088 Validation Loss 0.0329\n",
      "Epoch 20: Training Loss 0.0077 Validation Loss 0.0362\n",
      "Saving model in ./model/\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    train(sess, data.train, validation_set=data.validation, \n",
    "          initialize=True, epochs=20)\n",
    "    \n",
    "    save(saver, sess, epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl_no_gpu]",
   "language": "python",
   "name": "conda-env-dl_no_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
