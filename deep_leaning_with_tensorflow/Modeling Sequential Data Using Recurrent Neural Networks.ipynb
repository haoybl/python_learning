{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_path = 'C:/Users/p0ng5/OneDrive/movie_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reviews_path = 'C:/Users/LENOVO/Desktop/movie_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv(reviews_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nothing is fantastic! Simple as that! It's a f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This really was a waste of time...the movie ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This might sound weird, but I only got to see ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Considering this film was released 8 years bef...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very rarely does one come across an indie come...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Nothing is fantastic! Simple as that! It's a f...          1\n",
       "1  This really was a waste of time...the movie ha...          0\n",
       "2  This might sound weird, but I only got to see ...          0\n",
       "3  Considering this film was released 8 years bef...          1\n",
       "4  Very rarely does one come across an indie come...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I Sell the Dead is a big, sloppy horror comedy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>I know this sounds odd coming from someone bor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>OK I had higher hopes for this Carnosaur movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Laurence Fishburne is a fine actor, and deserv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I lived in Tokyo for 7 months. Knowing the rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  I Sell the Dead is a big, sloppy horror comedy...          1\n",
       "49996  I know this sounds odd coming from someone bor...          1\n",
       "49997  OK I had higher hopes for this Carnosaur movie...          0\n",
       "49998  Laurence Fishburne is a fine actor, and deserv...          1\n",
       "49999  I lived in Tokyo for 7 months. Knowing the rea...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count unique words\n",
    "counter = Counter()\n",
    "\n",
    "# clean text\n",
    "for i, review in enumerate(movie_data['review']):\n",
    "    \n",
    "    text = \"\".join(c if c not in punctuation else \" {} \".format(c) for c in review).lower()\n",
    "    movie_data.iloc[i, 0] = text\n",
    "    \n",
    "    counter.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2int dictionary\n",
    "word2int = {w:i for i, w in enumerate(sorted(counter, key=counter.get, reverse=True), start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# int-list reviews\n",
    "mapped_reviews = []\n",
    "for review in movie_data['review']:\n",
    "    mapped_reviews.append([word2int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 200\n",
    "\n",
    "# if length < sequence_length : left padd with zeros\n",
    "# if length > sequence_length : take last 'sequence_length' elements\n",
    "\n",
    "# padded sequence\n",
    "sequences = np.zeros(shape=(len(movie_data), sequence_length), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, mapped_review in enumerate(mapped_reviews):\n",
    "    n = len(mapped_review)\n",
    "    if n < sequence_length:\n",
    "        sequences[i, -n:] = mapped_review\n",
    "    else:\n",
    "        sequences[i, :] = mapped_review[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 200), (25000,), (25000, 200), (25000,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = sequences[:25000, :]\n",
    "y_train = movie_data.iloc[:25000, 1].values\n",
    "\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = movie_data.iloc[25000:, 1].values\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12500, 200), (12500,), (12500, 200), (12500,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = X_test[:12500]\n",
    "y_val = y_test[:12500]\n",
    "X_test = X_test[12500:]\n",
    "y_test = y_test[12500:]\n",
    "\n",
    "X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y=None, batch_size=64):\n",
    "    if y is not None:\n",
    "        assert len(X) == len(y)\n",
    "    n_batch = len(X) // batch_size\n",
    "    for i in range(n_batch):\n",
    "        a = i*batch_size\n",
    "        b = (i+1)*batch_size\n",
    "        \n",
    "        if y is not None:\n",
    "            yield X[a:b], y[a:b]\n",
    "        else:\n",
    "            yield X[a:b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentRNN:\n",
    "    \n",
    "    def __init__(self, word_size, embed_size=200, lstm_size=256, num_layer=1,\n",
    "                seq_length=200, learning_rate=1e-4, batch_size=32):\n",
    "        # model hyper parameters\n",
    "        self.word_size = word_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layer = num_layer\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # build model graph\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            #tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    def build(self):\n",
    "        # placeholder for inputs\n",
    "        tf_x = tf.placeholder(dtype=tf.int32, \n",
    "                              shape=(self.batch_size, self.seq_length), \n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(dtype=tf.float32, \n",
    "                              shape=(self.batch_size), \n",
    "                              name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(dtype=tf.float32, shape=(), \n",
    "                                     name='tf_keepprob')\n",
    "        # embedding vector\n",
    "        W_embedding = tf.Variable(\n",
    "            tf.random_uniform(shape=(self.word_size, self.embed_size), minval=-1, maxval=1),\n",
    "            name='W_embedding')\n",
    "        \n",
    "        embed_x = tf.nn.embedding_lookup(W_embedding, tf_x, name='embed_x')\n",
    "        \n",
    "        # create rnn cell\n",
    "        cells = tf.contrib.rnn.MultiRNNCell([\n",
    "            tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                                         output_keep_prob=tf_keepprob)\n",
    "                                          for i in range(self.num_layer)\n",
    "        ])\n",
    "        \n",
    "        # define the initial state/ rnn steps\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print(\"  << initial state > \", self.initial_state)\n",
    "        \n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cell=cells, inputs=embed_x, \n",
    "                                                           initial_state=self.initial_state)\n",
    "        print(\"\\n << lstm_output >> \", lstm_outputs)\n",
    "        print(\"\\n << final state >> \", self.final_state)\n",
    "        \n",
    "        # dense layer -> logits\n",
    "        logits = tf.layers.dense(inputs=lstm_outputs[:, -1], units=1, \n",
    "                                 activation=None, name='logits')\n",
    "        \n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print('\\n  << logits     >> ', logits)\n",
    "        \n",
    "        # predictions -> prob. | labels\n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        y_labels = tf.cast(tf.round(y_proba), dtype=tf.int32, name='labels')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels': y_labels\n",
    "        }\n",
    "        print(\"\\n << predictions  >> \", predictions)\n",
    "        \n",
    "        # cost function\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, \n",
    "                                                                      logits=logits), \n",
    "                              name='cost')\n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "    \n",
    "    def train(self, X, y, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            \n",
    "            batch_total = 0\n",
    "            for epoch in range(num_epochs):\n",
    "                # reset cell&hidden states\n",
    "                state = sess.run(self.initial_state)\n",
    "                for x_batch, y_batch in batch_generator(X, y, batch_size=self.batch_size):\n",
    "                    feed = {'tf_x:0': x_batch,\n",
    "                            'tf_y:0': y_batch,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state: state}\n",
    "                    loss, _, state = sess.run(['cost:0', 'train_op', self.final_state], \n",
    "                                              feed_dict=feed)\n",
    "                    \n",
    "                    # update training every 20 batches\n",
    "                    batch_total += 1\n",
    "                    if (batch_total+1) % 20 == 0:\n",
    "                        print(\"Epoch {:3d}, Iterations {:4d} | Train loss: {:.4f}\".format(epoch+1, \n",
    "                                                                                            batch_total+1,\n",
    "                                                                                            loss))\n",
    "                # save every 10 epochs                                                                          \n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    self.saver.save(sess, \n",
    "                                    'model/sentiment-rnn-{}.ckpt'.format(epoch+1))\n",
    "    \n",
    "    def predict(self, X, predict_proba=False):\n",
    "        pred = []\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            # restore latest model\n",
    "            self.saver.restore(sess, \n",
    "                               tf.train.latest_checkpoint('./model/'))\n",
    "            \n",
    "            # init model states\n",
    "            state = sess.run(self.initial_state)\n",
    "            \n",
    "            for x_batch in batch_generator(X, y=None, batch_size=self.batch_size):\n",
    "                feed = {'tf_x:0': x_batch,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: state}\n",
    "                \n",
    "                if predict_proba:\n",
    "                    y_pred, state = sess.run(['probabilities:0', self.final_state], \n",
    "                                           feed_dict=feed)\n",
    "                else:\n",
    "                    y_pred, state = sess.run(['labels:0', self.final_state], \n",
    "                                            feed_dict=feed)\n",
    "                pred.append(y_pred)\n",
    "                \n",
    "        return np.concatenate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2int' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5d252e7a0607>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mword_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2int\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0membed_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mn_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word2int' is not defined"
     ]
    }
   ],
   "source": [
    "sequence_length = 200\n",
    "word_size = max(word2int.values()) + 1\n",
    "embed_size = 256\n",
    "hidden_size = 128\n",
    "n_layer = 1\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = SentimentRNN(word_size=word_size, embed_size=embed_size, lstm_size=hidden_size, \n",
    "                     num_layer=n_layer, seq_length=sequence_length, batch_size=batch_size, \n",
    "                     learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1, Iterations   20 | Train loss: 0.6689\n",
      "Epoch   1, Iterations   40 | Train loss: 0.5714\n",
      "Epoch   1, Iterations   60 | Train loss: 0.6164\n",
      "Epoch   1, Iterations   80 | Train loss: 0.6584\n",
      "Epoch   1, Iterations  100 | Train loss: 0.5446\n",
      "Epoch   1, Iterations  120 | Train loss: 0.4987\n",
      "Epoch   1, Iterations  140 | Train loss: 0.4725\n",
      "Epoch   1, Iterations  160 | Train loss: 0.4996\n",
      "Epoch   1, Iterations  180 | Train loss: 0.5622\n",
      "Epoch   1, Iterations  200 | Train loss: 0.5063\n",
      "Epoch   1, Iterations  220 | Train loss: 0.4509\n",
      "Epoch   1, Iterations  240 | Train loss: 0.4027\n",
      "Epoch   2, Iterations  260 | Train loss: 0.5039\n",
      "Epoch   2, Iterations  280 | Train loss: 0.3833\n",
      "Epoch   2, Iterations  300 | Train loss: 0.4192\n",
      "Epoch   2, Iterations  320 | Train loss: 0.4525\n",
      "Epoch   2, Iterations  340 | Train loss: 0.3851\n",
      "Epoch   2, Iterations  360 | Train loss: 0.2664\n",
      "Epoch   2, Iterations  380 | Train loss: 0.2420\n",
      "Epoch   2, Iterations  400 | Train loss: 0.3306\n",
      "Epoch   2, Iterations  420 | Train loss: 0.4068\n",
      "Epoch   2, Iterations  440 | Train loss: 0.2590\n",
      "Epoch   2, Iterations  460 | Train loss: 0.2982\n",
      "Epoch   2, Iterations  480 | Train loss: 0.3188\n",
      "Epoch   2, Iterations  500 | Train loss: 0.2969\n",
      "Epoch   3, Iterations  520 | Train loss: 0.2957\n",
      "Epoch   3, Iterations  540 | Train loss: 0.2063\n",
      "Epoch   3, Iterations  560 | Train loss: 0.2427\n",
      "Epoch   3, Iterations  580 | Train loss: 0.1807\n",
      "Epoch   3, Iterations  600 | Train loss: 0.2052\n",
      "Epoch   3, Iterations  620 | Train loss: 0.2329\n",
      "Epoch   3, Iterations  640 | Train loss: 0.1974\n",
      "Epoch   3, Iterations  660 | Train loss: 0.2323\n",
      "Epoch   3, Iterations  680 | Train loss: 0.2184\n",
      "Epoch   3, Iterations  700 | Train loss: 0.0925\n",
      "Epoch   3, Iterations  720 | Train loss: 0.1635\n",
      "Epoch   3, Iterations  740 | Train loss: 0.2146\n",
      "Epoch   4, Iterations  760 | Train loss: 0.3312\n",
      "Epoch   4, Iterations  780 | Train loss: 0.2712\n",
      "Epoch   4, Iterations  800 | Train loss: 0.2500\n",
      "Epoch   4, Iterations  820 | Train loss: 0.2972\n",
      "Epoch   4, Iterations  840 | Train loss: 0.2335\n",
      "Epoch   4, Iterations  860 | Train loss: 0.1219\n",
      "Epoch   4, Iterations  880 | Train loss: 0.2511\n",
      "Epoch   4, Iterations  900 | Train loss: 0.1651\n",
      "Epoch   4, Iterations  920 | Train loss: 0.2093\n",
      "Epoch   4, Iterations  940 | Train loss: 0.0833\n",
      "Epoch   4, Iterations  960 | Train loss: 0.1050\n",
      "Epoch   4, Iterations  980 | Train loss: 0.1646\n",
      "Epoch   4, Iterations 1000 | Train loss: 0.1692\n",
      "Epoch   5, Iterations 1020 | Train loss: 0.3880\n",
      "Epoch   5, Iterations 1040 | Train loss: 0.1531\n",
      "Epoch   5, Iterations 1060 | Train loss: 0.1094\n",
      "Epoch   5, Iterations 1080 | Train loss: 0.1156\n",
      "Epoch   5, Iterations 1100 | Train loss: 0.0811\n",
      "Epoch   5, Iterations 1120 | Train loss: 0.1029\n",
      "Epoch   5, Iterations 1140 | Train loss: 0.1153\n",
      "Epoch   5, Iterations 1160 | Train loss: 0.1198\n",
      "Epoch   5, Iterations 1180 | Train loss: 0.1971\n",
      "Epoch   5, Iterations 1200 | Train loss: 0.0472\n",
      "Epoch   5, Iterations 1220 | Train loss: 0.0997\n",
      "Epoch   5, Iterations 1240 | Train loss: 0.1078\n",
      "Epoch   6, Iterations 1260 | Train loss: 0.1904\n",
      "Epoch   6, Iterations 1280 | Train loss: 0.1304\n",
      "Epoch   6, Iterations 1300 | Train loss: 0.1088\n",
      "Epoch   6, Iterations 1320 | Train loss: 0.1487\n",
      "Epoch   6, Iterations 1340 | Train loss: 0.0859\n",
      "Epoch   6, Iterations 1360 | Train loss: 0.0600\n",
      "Epoch   6, Iterations 1380 | Train loss: 0.0749\n",
      "Epoch   6, Iterations 1400 | Train loss: 0.2307\n",
      "Epoch   6, Iterations 1420 | Train loss: 0.1084\n",
      "Epoch   6, Iterations 1440 | Train loss: 0.0221\n",
      "Epoch   6, Iterations 1460 | Train loss: 0.0541\n",
      "Epoch   6, Iterations 1480 | Train loss: 0.0610\n",
      "Epoch   6, Iterations 1500 | Train loss: 0.1082\n",
      "Epoch   7, Iterations 1520 | Train loss: 0.0614\n",
      "Epoch   7, Iterations 1540 | Train loss: 0.0430\n",
      "Epoch   7, Iterations 1560 | Train loss: 0.0963\n",
      "Epoch   7, Iterations 1580 | Train loss: 0.0732\n",
      "Epoch   7, Iterations 1600 | Train loss: 0.1442\n",
      "Epoch   7, Iterations 1620 | Train loss: 0.0456\n",
      "Epoch   7, Iterations 1640 | Train loss: 0.0486\n",
      "Epoch   7, Iterations 1660 | Train loss: 0.1843\n",
      "Epoch   7, Iterations 1680 | Train loss: 0.0579\n",
      "Epoch   7, Iterations 1700 | Train loss: 0.0193\n",
      "Epoch   7, Iterations 1720 | Train loss: 0.0657\n",
      "Epoch   7, Iterations 1740 | Train loss: 0.0229\n",
      "Epoch   8, Iterations 1760 | Train loss: 0.1392\n",
      "Epoch   8, Iterations 1780 | Train loss: 0.0941\n",
      "Epoch   8, Iterations 1800 | Train loss: 0.0624\n",
      "Epoch   8, Iterations 1820 | Train loss: 0.0382\n",
      "Epoch   8, Iterations 1840 | Train loss: 0.0512\n",
      "Epoch   8, Iterations 1860 | Train loss: 0.0371\n",
      "Epoch   8, Iterations 1880 | Train loss: 0.0656\n",
      "Epoch   8, Iterations 1900 | Train loss: 0.1075\n",
      "Epoch   8, Iterations 1920 | Train loss: 0.0511\n",
      "Epoch   8, Iterations 1940 | Train loss: 0.0230\n",
      "Epoch   8, Iterations 1960 | Train loss: 0.0330\n",
      "Epoch   8, Iterations 1980 | Train loss: 0.0138\n",
      "Epoch   8, Iterations 2000 | Train loss: 0.0339\n",
      "Epoch   9, Iterations 2020 | Train loss: 0.0189\n",
      "Epoch   9, Iterations 2040 | Train loss: 0.0208\n",
      "Epoch   9, Iterations 2060 | Train loss: 0.0467\n",
      "Epoch   9, Iterations 2080 | Train loss: 0.0128\n",
      "Epoch   9, Iterations 2100 | Train loss: 0.0269\n",
      "Epoch   9, Iterations 2120 | Train loss: 0.0125\n",
      "Epoch   9, Iterations 2140 | Train loss: 0.0117\n",
      "Epoch   9, Iterations 2160 | Train loss: 0.0078\n",
      "Epoch   9, Iterations 2180 | Train loss: 0.0150\n",
      "Epoch   9, Iterations 2200 | Train loss: 0.0038\n",
      "Epoch   9, Iterations 2220 | Train loss: 0.0188\n",
      "Epoch   9, Iterations 2240 | Train loss: 0.0445\n",
      "Epoch  10, Iterations 2260 | Train loss: 0.3463\n",
      "Epoch  10, Iterations 2280 | Train loss: 0.1023\n",
      "Epoch  10, Iterations 2300 | Train loss: 0.0405\n",
      "Epoch  10, Iterations 2320 | Train loss: 0.1190\n",
      "Epoch  10, Iterations 2340 | Train loss: 0.0401\n",
      "Epoch  10, Iterations 2360 | Train loss: 0.0867\n",
      "Epoch  10, Iterations 2380 | Train loss: 0.0572\n",
      "Epoch  10, Iterations 2400 | Train loss: 0.0195\n",
      "Epoch  10, Iterations 2420 | Train loss: 0.0215\n",
      "Epoch  10, Iterations 2440 | Train loss: 0.0304\n",
      "Epoch  10, Iterations 2460 | Train loss: 0.0065\n",
      "Epoch  10, Iterations 2480 | Train loss: 0.0201\n",
      "Epoch  10, Iterations 2500 | Train loss: 0.0957\n",
      "Epoch  11, Iterations 2520 | Train loss: 0.0110\n",
      "Epoch  11, Iterations 2540 | Train loss: 0.0108\n",
      "Epoch  11, Iterations 2560 | Train loss: 0.0401\n",
      "Epoch  11, Iterations 2580 | Train loss: 0.0084\n",
      "Epoch  11, Iterations 2600 | Train loss: 0.0130\n",
      "Epoch  11, Iterations 2620 | Train loss: 0.0226\n",
      "Epoch  11, Iterations 2640 | Train loss: 0.0074\n",
      "Epoch  11, Iterations 2660 | Train loss: 0.0082\n",
      "Epoch  11, Iterations 2680 | Train loss: 0.0050\n",
      "Epoch  11, Iterations 2700 | Train loss: 0.0029\n",
      "Epoch  11, Iterations 2720 | Train loss: 0.0303\n",
      "Epoch  11, Iterations 2740 | Train loss: 0.0026\n",
      "Epoch  12, Iterations 2760 | Train loss: 0.0838\n",
      "Epoch  12, Iterations 2780 | Train loss: 0.0924\n",
      "Epoch  12, Iterations 2800 | Train loss: 0.0153\n",
      "Epoch  12, Iterations 2820 | Train loss: 0.0120\n",
      "Epoch  12, Iterations 2840 | Train loss: 0.0210\n",
      "Epoch  12, Iterations 2860 | Train loss: 0.0094\n",
      "Epoch  12, Iterations 2880 | Train loss: 0.0072\n",
      "Epoch  12, Iterations 2900 | Train loss: 0.0061\n",
      "Epoch  12, Iterations 2920 | Train loss: 0.0074\n",
      "Epoch  12, Iterations 2940 | Train loss: 0.0027\n",
      "Epoch  12, Iterations 2960 | Train loss: 0.0089\n",
      "Epoch  12, Iterations 2980 | Train loss: 0.0014\n",
      "Epoch  12, Iterations 3000 | Train loss: 0.0064\n",
      "Epoch  13, Iterations 3020 | Train loss: 0.0146\n",
      "Epoch  13, Iterations 3040 | Train loss: 0.0070\n",
      "Epoch  13, Iterations 3060 | Train loss: 0.0805\n",
      "Epoch  13, Iterations 3080 | Train loss: 0.0057\n",
      "Epoch  13, Iterations 3100 | Train loss: 0.0045\n",
      "Epoch  13, Iterations 3120 | Train loss: 0.0353\n",
      "Epoch  13, Iterations 3140 | Train loss: 0.0122\n",
      "Epoch  13, Iterations 3160 | Train loss: 0.0062\n",
      "Epoch  13, Iterations 3180 | Train loss: 0.0064\n",
      "Epoch  13, Iterations 3200 | Train loss: 0.0009\n",
      "Epoch  13, Iterations 3220 | Train loss: 0.0217\n",
      "Epoch  13, Iterations 3240 | Train loss: 0.0005\n",
      "Epoch  14, Iterations 3260 | Train loss: 0.1401\n",
      "Epoch  14, Iterations 3280 | Train loss: 0.0209\n",
      "Epoch  14, Iterations 3300 | Train loss: 0.0090\n",
      "Epoch  14, Iterations 3320 | Train loss: 0.0113\n",
      "Epoch  14, Iterations 3340 | Train loss: 0.0009\n",
      "Epoch  14, Iterations 3360 | Train loss: 0.0015\n",
      "Epoch  14, Iterations 3380 | Train loss: 0.0010\n",
      "Epoch  14, Iterations 3400 | Train loss: 0.0011\n",
      "Epoch  14, Iterations 3420 | Train loss: 0.0069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14, Iterations 3440 | Train loss: 0.0023\n",
      "Epoch  14, Iterations 3460 | Train loss: 0.0097\n",
      "Epoch  14, Iterations 3480 | Train loss: 0.0064\n",
      "Epoch  14, Iterations 3500 | Train loss: 0.0044\n",
      "Epoch  15, Iterations 3520 | Train loss: 0.0028\n",
      "Epoch  15, Iterations 3540 | Train loss: 0.0016\n",
      "Epoch  15, Iterations 3560 | Train loss: 0.0060\n",
      "Epoch  15, Iterations 3580 | Train loss: 0.0148\n",
      "Epoch  15, Iterations 3600 | Train loss: 0.0145\n",
      "Epoch  15, Iterations 3620 | Train loss: 0.0009\n",
      "Epoch  15, Iterations 3640 | Train loss: 0.0011\n",
      "Epoch  15, Iterations 3660 | Train loss: 0.0178\n",
      "Epoch  15, Iterations 3680 | Train loss: 0.0008\n",
      "Epoch  15, Iterations 3700 | Train loss: 0.0006\n",
      "Epoch  15, Iterations 3720 | Train loss: 0.0062\n",
      "Epoch  15, Iterations 3740 | Train loss: 0.0729\n",
      "Epoch  16, Iterations 3760 | Train loss: 0.0598\n",
      "Epoch  16, Iterations 3780 | Train loss: 0.0007\n",
      "Epoch  16, Iterations 3800 | Train loss: 0.0042\n",
      "Epoch  16, Iterations 3820 | Train loss: 0.0054\n",
      "Epoch  16, Iterations 3840 | Train loss: 0.0128\n",
      "Epoch  16, Iterations 3860 | Train loss: 0.0031\n",
      "Epoch  16, Iterations 3880 | Train loss: 0.0539\n",
      "Epoch  16, Iterations 3900 | Train loss: 0.0007\n",
      "Epoch  16, Iterations 3920 | Train loss: 0.0104\n",
      "Epoch  16, Iterations 3940 | Train loss: 0.0007\n",
      "Epoch  16, Iterations 3960 | Train loss: 0.0060\n",
      "Epoch  16, Iterations 3980 | Train loss: 0.0850\n",
      "Epoch  16, Iterations 4000 | Train loss: 0.0106\n",
      "Epoch  17, Iterations 4020 | Train loss: 0.2108\n",
      "Epoch  17, Iterations 4040 | Train loss: 0.0200\n",
      "Epoch  17, Iterations 4060 | Train loss: 0.0061\n",
      "Epoch  17, Iterations 4080 | Train loss: 0.0091\n",
      "Epoch  17, Iterations 4100 | Train loss: 0.0039\n",
      "Epoch  17, Iterations 4120 | Train loss: 0.0053\n",
      "Epoch  17, Iterations 4140 | Train loss: 0.0019\n",
      "Epoch  17, Iterations 4160 | Train loss: 0.0011\n",
      "Epoch  17, Iterations 4180 | Train loss: 0.0043\n",
      "Epoch  17, Iterations 4200 | Train loss: 0.1066\n",
      "Epoch  17, Iterations 4220 | Train loss: 0.0276\n",
      "Epoch  17, Iterations 4240 | Train loss: 0.0032\n",
      "Epoch  18, Iterations 4260 | Train loss: 0.0814\n",
      "Epoch  18, Iterations 4280 | Train loss: 0.0027\n",
      "Epoch  18, Iterations 4300 | Train loss: 0.0012\n",
      "Epoch  18, Iterations 4320 | Train loss: 0.0036\n",
      "Epoch  18, Iterations 4340 | Train loss: 0.0063\n",
      "Epoch  18, Iterations 4360 | Train loss: 0.0011\n",
      "Epoch  18, Iterations 4380 | Train loss: 0.0041\n",
      "Epoch  18, Iterations 4400 | Train loss: 0.0027\n",
      "Epoch  18, Iterations 4420 | Train loss: 0.0091\n",
      "Epoch  18, Iterations 4440 | Train loss: 0.0006\n",
      "Epoch  18, Iterations 4460 | Train loss: 0.0014\n",
      "Epoch  18, Iterations 4480 | Train loss: 0.0043\n",
      "Epoch  18, Iterations 4500 | Train loss: 0.0085\n",
      "Epoch  19, Iterations 4520 | Train loss: 0.0044\n",
      "Epoch  19, Iterations 4540 | Train loss: 0.0013\n",
      "Epoch  19, Iterations 4560 | Train loss: 0.0402\n",
      "Epoch  19, Iterations 4580 | Train loss: 0.0026\n",
      "Epoch  19, Iterations 4600 | Train loss: 0.0054\n",
      "Epoch  19, Iterations 4620 | Train loss: 0.0170\n",
      "Epoch  19, Iterations 4640 | Train loss: 0.0186\n",
      "Epoch  19, Iterations 4660 | Train loss: 0.0012\n",
      "Epoch  19, Iterations 4680 | Train loss: 0.0086\n",
      "Epoch  19, Iterations 4700 | Train loss: 0.0005\n",
      "Epoch  19, Iterations 4720 | Train loss: 0.1440\n",
      "Epoch  19, Iterations 4740 | Train loss: 0.0176\n",
      "Epoch  20, Iterations 4760 | Train loss: 0.0838\n",
      "Epoch  20, Iterations 4780 | Train loss: 0.0026\n",
      "Epoch  20, Iterations 4800 | Train loss: 0.0018\n",
      "Epoch  20, Iterations 4820 | Train loss: 0.0526\n",
      "Epoch  20, Iterations 4840 | Train loss: 0.0012\n",
      "Epoch  20, Iterations 4860 | Train loss: 0.0132\n",
      "Epoch  20, Iterations 4880 | Train loss: 0.0013\n",
      "Epoch  20, Iterations 4900 | Train loss: 0.0236\n",
      "Epoch  20, Iterations 4920 | Train loss: 0.0021\n",
      "Epoch  20, Iterations 4940 | Train loss: 0.0006\n",
      "Epoch  20, Iterations 4960 | Train loss: 0.0087\n",
      "Epoch  20, Iterations 4980 | Train loss: 0.0003\n",
      "Epoch  20, Iterations 5000 | Train loss: 0.0224\n",
      "Epoch  21, Iterations 5020 | Train loss: 0.0137\n",
      "Epoch  21, Iterations 5040 | Train loss: 0.0016\n",
      "Epoch  21, Iterations 5060 | Train loss: 0.0008\n",
      "Epoch  21, Iterations 5080 | Train loss: 0.0079\n",
      "Epoch  21, Iterations 5100 | Train loss: 0.0205\n",
      "Epoch  21, Iterations 5120 | Train loss: 0.0057\n",
      "Epoch  21, Iterations 5140 | Train loss: 0.0007\n",
      "Epoch  21, Iterations 5160 | Train loss: 0.0003\n",
      "Epoch  21, Iterations 5180 | Train loss: 0.0939\n",
      "Epoch  21, Iterations 5200 | Train loss: 0.0006\n",
      "Epoch  21, Iterations 5220 | Train loss: 0.0019\n",
      "Epoch  21, Iterations 5240 | Train loss: 0.0029\n",
      "Epoch  22, Iterations 5260 | Train loss: 0.0724\n",
      "Epoch  22, Iterations 5280 | Train loss: 0.0020\n",
      "Epoch  22, Iterations 5300 | Train loss: 0.0046\n",
      "Epoch  22, Iterations 5320 | Train loss: 0.0005\n",
      "Epoch  22, Iterations 5340 | Train loss: 0.0803\n",
      "Epoch  22, Iterations 5360 | Train loss: 0.0165\n",
      "Epoch  22, Iterations 5380 | Train loss: 0.0021\n",
      "Epoch  22, Iterations 5400 | Train loss: 0.0564\n",
      "Epoch  22, Iterations 5420 | Train loss: 0.0016\n",
      "Epoch  22, Iterations 5440 | Train loss: 0.0018\n",
      "Epoch  22, Iterations 5460 | Train loss: 0.0009\n",
      "Epoch  22, Iterations 5480 | Train loss: 0.0008\n",
      "Epoch  22, Iterations 5500 | Train loss: 0.0002\n",
      "Epoch  23, Iterations 5520 | Train loss: 0.0003\n",
      "Epoch  23, Iterations 5540 | Train loss: 0.0005\n",
      "Epoch  23, Iterations 5560 | Train loss: 0.0020\n",
      "Epoch  23, Iterations 5580 | Train loss: 0.0002\n",
      "Epoch  23, Iterations 5600 | Train loss: 0.0050\n",
      "Epoch  23, Iterations 5620 | Train loss: 0.0014\n",
      "Epoch  23, Iterations 5640 | Train loss: 0.0004\n",
      "Epoch  23, Iterations 5660 | Train loss: 0.0004\n",
      "Epoch  23, Iterations 5680 | Train loss: 0.0001\n",
      "Epoch  23, Iterations 5700 | Train loss: 0.0001\n",
      "Epoch  23, Iterations 5720 | Train loss: 0.0007\n",
      "Epoch  23, Iterations 5740 | Train loss: 0.0001\n",
      "Epoch  24, Iterations 5760 | Train loss: 0.0362\n",
      "Epoch  24, Iterations 5780 | Train loss: 0.0001\n",
      "Epoch  24, Iterations 5800 | Train loss: 0.0006\n",
      "Epoch  24, Iterations 5820 | Train loss: 0.0006\n",
      "Epoch  24, Iterations 5840 | Train loss: 0.0002\n",
      "Epoch  24, Iterations 5860 | Train loss: 0.0004\n",
      "Epoch  24, Iterations 5880 | Train loss: 0.0014\n",
      "Epoch  24, Iterations 5900 | Train loss: 0.0003\n",
      "Epoch  24, Iterations 5920 | Train loss: 0.0002\n",
      "Epoch  24, Iterations 5940 | Train loss: 0.0001\n",
      "Epoch  24, Iterations 5960 | Train loss: 0.0002\n",
      "Epoch  24, Iterations 5980 | Train loss: 0.0005\n",
      "Epoch  24, Iterations 6000 | Train loss: 0.0001\n",
      "Epoch  25, Iterations 6020 | Train loss: 0.0003\n",
      "Epoch  25, Iterations 6040 | Train loss: 0.0002\n",
      "Epoch  25, Iterations 6060 | Train loss: 0.0004\n",
      "Epoch  25, Iterations 6080 | Train loss: 0.0001\n",
      "Epoch  25, Iterations 6100 | Train loss: 0.0003\n",
      "Epoch  25, Iterations 6120 | Train loss: 0.0001\n",
      "Epoch  25, Iterations 6140 | Train loss: 0.0002\n",
      "Epoch  25, Iterations 6160 | Train loss: 0.0014\n",
      "Epoch  25, Iterations 6180 | Train loss: 0.0001\n",
      "Epoch  25, Iterations 6200 | Train loss: 0.0004\n",
      "Epoch  25, Iterations 6220 | Train loss: 0.0003\n",
      "Epoch  25, Iterations 6240 | Train loss: 0.0000\n",
      "Epoch  26, Iterations 6260 | Train loss: 0.0011\n",
      "Epoch  26, Iterations 6280 | Train loss: 0.0000\n",
      "Epoch  26, Iterations 6300 | Train loss: 0.0000\n",
      "Epoch  26, Iterations 6320 | Train loss: 0.0004\n",
      "Epoch  26, Iterations 6340 | Train loss: 0.0001\n",
      "Epoch  26, Iterations 6360 | Train loss: 0.0001\n",
      "Epoch  26, Iterations 6380 | Train loss: 0.0004\n",
      "Epoch  26, Iterations 6400 | Train loss: 0.0002\n",
      "Epoch  26, Iterations 6420 | Train loss: 0.0008\n",
      "Epoch  26, Iterations 6440 | Train loss: 0.0000\n",
      "Epoch  26, Iterations 6460 | Train loss: 0.0001\n",
      "Epoch  26, Iterations 6480 | Train loss: 0.0001\n",
      "Epoch  26, Iterations 6500 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6520 | Train loss: 0.0000\n",
      "Epoch  27, Iterations 6540 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6560 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6580 | Train loss: 0.0002\n",
      "Epoch  27, Iterations 6600 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6620 | Train loss: 0.0000\n",
      "Epoch  27, Iterations 6640 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6660 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6680 | Train loss: 0.0000\n",
      "Epoch  27, Iterations 6700 | Train loss: 0.0000\n",
      "Epoch  27, Iterations 6720 | Train loss: 0.0005\n",
      "Epoch  27, Iterations 6740 | Train loss: 0.0000\n",
      "Epoch  28, Iterations 6760 | Train loss: 0.0343\n",
      "Epoch  28, Iterations 6780 | Train loss: 0.0000\n",
      "Epoch  28, Iterations 6800 | Train loss: 0.0003\n",
      "Epoch  28, Iterations 6820 | Train loss: 0.0071\n",
      "Epoch  28, Iterations 6840 | Train loss: 0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  28, Iterations 6860 | Train loss: 0.0004\n",
      "Epoch  28, Iterations 6880 | Train loss: 0.1193\n",
      "Epoch  28, Iterations 6900 | Train loss: 0.0111\n",
      "Epoch  28, Iterations 6920 | Train loss: 0.0125\n",
      "Epoch  28, Iterations 6940 | Train loss: 0.0076\n",
      "Epoch  28, Iterations 6960 | Train loss: 0.1508\n",
      "Epoch  28, Iterations 6980 | Train loss: 0.0128\n",
      "Epoch  28, Iterations 7000 | Train loss: 0.0163\n",
      "Epoch  29, Iterations 7020 | Train loss: 0.0052\n",
      "Epoch  29, Iterations 7040 | Train loss: 0.0028\n",
      "Epoch  29, Iterations 7060 | Train loss: 0.0023\n",
      "Epoch  29, Iterations 7080 | Train loss: 0.0222\n",
      "Epoch  29, Iterations 7100 | Train loss: 0.0033\n",
      "Epoch  29, Iterations 7120 | Train loss: 0.0136\n",
      "Epoch  29, Iterations 7140 | Train loss: 0.0175\n",
      "Epoch  29, Iterations 7160 | Train loss: 0.0210\n",
      "Epoch  29, Iterations 7180 | Train loss: 0.0033\n",
      "Epoch  29, Iterations 7200 | Train loss: 0.0223\n",
      "Epoch  29, Iterations 7220 | Train loss: 0.0043\n",
      "Epoch  29, Iterations 7240 | Train loss: 0.0011\n",
      "Epoch  30, Iterations 7260 | Train loss: 0.0226\n",
      "Epoch  30, Iterations 7280 | Train loss: 0.0024\n",
      "Epoch  30, Iterations 7300 | Train loss: 0.0300\n",
      "Epoch  30, Iterations 7320 | Train loss: 0.0009\n",
      "Epoch  30, Iterations 7340 | Train loss: 0.0018\n",
      "Epoch  30, Iterations 7360 | Train loss: 0.0003\n",
      "Epoch  30, Iterations 7380 | Train loss: 0.0008\n",
      "Epoch  30, Iterations 7400 | Train loss: 0.0039\n",
      "Epoch  30, Iterations 7420 | Train loss: 0.0005\n",
      "Epoch  30, Iterations 7440 | Train loss: 0.0002\n",
      "Epoch  30, Iterations 7460 | Train loss: 0.0005\n",
      "Epoch  30, Iterations 7480 | Train loss: 0.0002\n",
      "Epoch  30, Iterations 7500 | Train loss: 0.0004\n",
      "Epoch  31, Iterations 7520 | Train loss: 0.0006\n",
      "Epoch  31, Iterations 7540 | Train loss: 0.0001\n",
      "Epoch  31, Iterations 7560 | Train loss: 0.0005\n",
      "Epoch  31, Iterations 7580 | Train loss: 0.0005\n",
      "Epoch  31, Iterations 7600 | Train loss: 0.0019\n",
      "Epoch  31, Iterations 7620 | Train loss: 0.0003\n",
      "Epoch  31, Iterations 7640 | Train loss: 0.0007\n",
      "Epoch  31, Iterations 7660 | Train loss: 0.0007\n",
      "Epoch  31, Iterations 7680 | Train loss: 0.0005\n",
      "Epoch  31, Iterations 7700 | Train loss: 0.0001\n",
      "Epoch  31, Iterations 7720 | Train loss: 0.0002\n",
      "Epoch  31, Iterations 7740 | Train loss: 0.0001\n",
      "Epoch  32, Iterations 7760 | Train loss: 0.0021\n",
      "Epoch  32, Iterations 7780 | Train loss: 0.0004\n",
      "Epoch  32, Iterations 7800 | Train loss: 0.0001\n",
      "Epoch  32, Iterations 7820 | Train loss: 0.0003\n",
      "Epoch  32, Iterations 7840 | Train loss: 0.0002\n",
      "Epoch  32, Iterations 7860 | Train loss: 0.0008\n",
      "Epoch  32, Iterations 7880 | Train loss: 0.0002\n",
      "Epoch  32, Iterations 7900 | Train loss: 0.0008\n",
      "Epoch  32, Iterations 7920 | Train loss: 0.0001\n",
      "Epoch  32, Iterations 7940 | Train loss: 0.0001\n",
      "Epoch  32, Iterations 7960 | Train loss: 0.0002\n",
      "Epoch  32, Iterations 7980 | Train loss: 0.0001\n",
      "Epoch  32, Iterations 8000 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8020 | Train loss: 0.0002\n",
      "Epoch  33, Iterations 8040 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8060 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8080 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8100 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8120 | Train loss: 0.0002\n",
      "Epoch  33, Iterations 8140 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8160 | Train loss: 0.0005\n",
      "Epoch  33, Iterations 8180 | Train loss: 0.0000\n",
      "Epoch  33, Iterations 8200 | Train loss: 0.0000\n",
      "Epoch  33, Iterations 8220 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8240 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8260 | Train loss: 0.0002\n",
      "Epoch  34, Iterations 8280 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8300 | Train loss: 0.0000\n",
      "Epoch  34, Iterations 8320 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8340 | Train loss: 0.0000\n",
      "Epoch  34, Iterations 8360 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8380 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8400 | Train loss: 0.0000\n",
      "Epoch  34, Iterations 8420 | Train loss: 0.0000\n",
      "Epoch  34, Iterations 8440 | Train loss: 0.0000\n",
      "Epoch  34, Iterations 8460 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8480 | Train loss: 0.0002\n",
      "Epoch  34, Iterations 8500 | Train loss: 0.0001\n",
      "Epoch  35, Iterations 8520 | Train loss: 0.0001\n",
      "Epoch  35, Iterations 8540 | Train loss: 0.0002\n",
      "Epoch  35, Iterations 8560 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8580 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8600 | Train loss: 0.0001\n",
      "Epoch  35, Iterations 8620 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8640 | Train loss: 0.0001\n",
      "Epoch  35, Iterations 8660 | Train loss: 0.0001\n",
      "Epoch  35, Iterations 8680 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8700 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8720 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8740 | Train loss: 0.0001\n",
      "Epoch  36, Iterations 8760 | Train loss: 0.0003\n",
      "Epoch  36, Iterations 8780 | Train loss: 0.0000\n",
      "Epoch  36, Iterations 8800 | Train loss: 0.0000\n",
      "Epoch  36, Iterations 8820 | Train loss: 0.0002\n",
      "Epoch  36, Iterations 8840 | Train loss: 0.0001\n",
      "Epoch  36, Iterations 8860 | Train loss: 0.0001\n",
      "Epoch  36, Iterations 8880 | Train loss: 0.0001\n",
      "Epoch  36, Iterations 8900 | Train loss: 0.0001\n",
      "Epoch  36, Iterations 8920 | Train loss: 0.0004\n",
      "Epoch  36, Iterations 8940 | Train loss: 0.0000\n",
      "Epoch  36, Iterations 8960 | Train loss: 0.0000\n",
      "Epoch  36, Iterations 8980 | Train loss: 0.0000\n",
      "Epoch  36, Iterations 9000 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9020 | Train loss: 0.0001\n",
      "Epoch  37, Iterations 9040 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9060 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9080 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9100 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9120 | Train loss: 0.0001\n",
      "Epoch  37, Iterations 9140 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9160 | Train loss: 0.0002\n",
      "Epoch  37, Iterations 9180 | Train loss: 0.0001\n",
      "Epoch  37, Iterations 9200 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9220 | Train loss: 0.0001\n",
      "Epoch  37, Iterations 9240 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9260 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9280 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9300 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9320 | Train loss: 0.0001\n",
      "Epoch  38, Iterations 9340 | Train loss: 0.0001\n",
      "Epoch  38, Iterations 9360 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9380 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9400 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9420 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9440 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9460 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9480 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9500 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9520 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9540 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9560 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9580 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9600 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9620 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9640 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9660 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9680 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9700 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9720 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9740 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9760 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9780 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9800 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9820 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9840 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9860 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9880 | Train loss: 0.0002\n",
      "Epoch  40, Iterations 9900 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9920 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9940 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9960 | Train loss: 0.0001\n",
      "Epoch  40, Iterations 9980 | Train loss: 0.0001\n",
      "Epoch  40, Iterations 10000 | Train loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/sentiment-rnn-40.ckpt\n"
     ]
    }
   ],
   "source": [
    "# val accuracy\n",
    "val_preds = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SentimentRNN(word_size=word_size, embed_size=embed_size, lstm_size=hidden_size, \n",
    "                     num_layer=n_layer, seq_length=sequence_length, batch_size=batch_size, \n",
    "                     learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Project 2 - implementing an RNN for character-level language modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pg2265.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178707"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text[15858:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_set = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char2int = {c:i for i,c in enumerate(char_set)}\n",
    "int2char = {i:c for i,c in enumerate(char_set)}\n",
    "\n",
    "assert len(char2int) == len(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_ints = np.array([char2int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162849,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape_date(sequence, batch_size, num_steps):\n",
    "    '''generate x, y  sequences from the given sequence\n",
    "    according to batch_size and num_steps\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    sequence : 1-d array\n",
    "        a sequence of integers (converted from char2int)\n",
    "    batch_size : int\n",
    "        size of batch in each iteration\n",
    "    num_steps : int\n",
    "        length of sequence in each batch\n",
    "    '''\n",
    "    total_batch_length = batch_size * num_steps\n",
    "    n_batch = len(sequence) // total_batch_length\n",
    "    \n",
    "    # trim original sequence fit to batches\n",
    "    x = sequence[0:n_batch*total_batch_length]\n",
    "    y = sequence[1:n_batch*total_batch_length+1]\n",
    "    assert len(x) == len(y)\n",
    "    \n",
    "    # reshape to [batch_size, n_batch*num_steps] matrix\n",
    "    x = np.asarray(x).reshape(batch_size, n_batch*num_steps)\n",
    "    y = np.asarray(y).reshape(batch_size, n_batch*num_steps)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, total_length = data_x.shape\n",
    "    n_batch = total_length // num_steps\n",
    "    for i in range(n_batch):\n",
    "        a, b = i*num_steps, (i+1)*num_steps\n",
    "        yield data_x[:, a:b], data_y[:, a:b]\n",
    "                  \n",
    "    \n",
    "def get_top_char(proba, char_size, top_n=5):\n",
    "    p = np.squeeze(proba)\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p /= np.sum(p)\n",
    "    idx = np.random.choice(char_size, size=1, p=p)[0]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRnn:\n",
    "    \n",
    "    def __init__(self, n_classes, lstm_size=128, n_layers=1, \n",
    "                 batch_size=64, n_steps=100, learning_rate=1e-3,\n",
    "                keep_prob=.5, grad_clip=5, sampling=False, random_state=None):\n",
    "        # model parameters\n",
    "        self.n_classes = n_classes\n",
    "        self.lstm_size = lstm_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_steps = n_steps\n",
    "        self.n_layers = n_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            self.build(sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            # sampling mode\n",
    "            batch_size, n_step = 1, 1\n",
    "        else:\n",
    "            # traing mode\n",
    "            batch_size, n_step = self.batch_size, self.n_steps\n",
    "        \n",
    "        # init placeholders\n",
    "        tf_x = tf.placeholder(dtype=tf.int32, shape=(batch_size, n_step), name='tf_x')\n",
    "        tf_y = tf.placeholder(dtype=tf.int32, shape=(batch_size, n_step), name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(dtype=tf.float32, shape=(), name='tf_keepprob')\n",
    "        \n",
    "        # one-hot encoding\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.n_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.n_classes)\n",
    "        print(\" << y_onehot >> \", y_onehot)\n",
    "        # build multi-layers RNN cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.BasicLSTMCell(num_units=self.lstm_size), \n",
    "            input_keep_prob=tf_keepprob) for i in range(self.n_layers)])\n",
    "        \n",
    "        # define initial state\n",
    "        self.initial_state = cells.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "        \n",
    "        # run sequence step through the RNN\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, \n",
    "                                                      inputs=x_onehot, \n",
    "                                                      initial_state=self.initial_state)\n",
    "        print(\"  << lstm_outputs  >> \", lstm_outputs)\n",
    "        # reshape output 3-d [batch_size, steps, lstm_size] -> 2-d [batch_size*steps, lstm_size]\n",
    "        seq_outputs = tf.reshape(lstm_outputs, shape=[-1, self.lstm_size], name='seq_outputs_reshape')\n",
    "        \n",
    "        # dense layer -> out shape [seq_length, n_classes]\n",
    "        logits = tf.layers.dense(seq_outputs, units=self.n_classes, \n",
    "                                 activation=None, name='logits')\n",
    "        print(\" << logits >> \", logits)\n",
    "        proba = tf.nn.softmax(logits, name='probabilities')\n",
    "        \n",
    "        y_reshaped = tf.reshape(y_onehot, shape=(-1, self.n_classes), name='y_reshaped')\n",
    "        \n",
    "        # cost function\n",
    "        cost = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                                    labels=y_reshaped, \n",
    "                                                    logits=logits), \n",
    "                    name='cost')\n",
    "        \n",
    "        # gradient clipping to avoiding exploding gradients\n",
    "        train_vars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(cost, train_vars), \n",
    "                                          clip_norm=self.grad_clip)\n",
    "        # optimizer \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars=zip(grads, train_vars), \n",
    "                                             name='train_op')\n",
    "        \n",
    "    def train(self, x_train, y_train, n_epochs, ckpt_dir='C:\\\\Users\\\\LENOVO\\\\OneDrive\\\\tmp\\\\model'):\n",
    "        \n",
    "        # check valid ckpt_dir\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.best_loss_ = float('inf')\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            \n",
    "            n_batch = x_train.shape[1]//self.n_steps\n",
    "            total_iterations = 0\n",
    "            for i in range(n_epochs):\n",
    "                # init zero_state\n",
    "                state = sess.run(self.initial_state)\n",
    "                avg_loss = 0.0\n",
    "                for j, (x_batch, y_batch) in enumerate(batch_generator(x_train, y_train, \n",
    "                                                                       num_steps=self.n_steps)):\n",
    "                    feed = {'tf_x:0': x_batch,\n",
    "                            'tf_y:0': y_batch,\n",
    "                            'tf_keepprob:0': self.keep_prob, \n",
    "                             self.initial_state: state}\n",
    "                    loss, _, state = sess.run(['cost:0', 'train_op', self.final_state], \n",
    "                                              feed_dict=feed)\n",
    "                    \n",
    "                    avg_loss += loss            \n",
    "                    # display every 20 iterations\n",
    "                    total_iterations += 1\n",
    "                    if (total_iterations % 20 == 0):\n",
    "                        print(\"Epoch {:2d}/{}, Iterations {:4d} | Train loss {:.5f}\".format(i, n_epochs, \n",
    "                                                                                            total_iterations,\n",
    "                                                                                            loss))\n",
    "                # one pass finish\n",
    "                avg_loss /= (j+1)\n",
    "                # save model if better than best_loss\n",
    "                if avg_loss < self.best_loss_:\n",
    "                    self.best_model_path = os.path.join(self.ckpt_dir, \n",
    "                                                       'char_model-{}.ckpt'.format(total_iterations))\n",
    "                    self.saver.save(sess, self.best_model_path)\n",
    "    \n",
    "                        \n",
    "    def sample(self, output_length, ckpt_dir, starter_seq='The '):\n",
    "        \n",
    "        observed_seq = list(starter_seq)\n",
    "        \n",
    "        # restore model\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, \n",
    "                               tf.train.latest_checkpoint(ckpt_dir) )\n",
    "            \n",
    "            # run model using the starter sequence\n",
    "            state = sess.run(self.initial_state)\n",
    "            for char in observed_seq:\n",
    "                x = np.zeros(shape=(1, 1))\n",
    "                x[0, 0] = char2int[char]\n",
    "                \n",
    "                feed = {'tf_x:0': x, \n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: state}\n",
    "                proba, state = sess.run(['probabilities:0', self.final_state], \n",
    "                                        feed_dict=feed)\n",
    "            \n",
    "            next_idx = get_top_char(proba, self.n_classes)\n",
    "            observed_seq.append(int2char[next_idx])\n",
    "            \n",
    "            # run the model using the updated observed_seq\n",
    "            for _ in range(output_length):\n",
    "                x[0, 0] = next_idx\n",
    "                feed = {'tf_x:0': x, 'tf_keepprob:0':1.0, \n",
    "                        self.initial_state: state}\n",
    "                proba, state = sess.run(['probabilities:0', self.final_state], \n",
    "                                        feed_dict=feed)\n",
    "                \n",
    "                next_idx = get_top_char(proba, self.n_classes)\n",
    "                observed_seq.append(int2char[next_idx])\n",
    "            \n",
    "        return \"\".join(observed_seq)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " << y_onehot >>  Tensor(\"one_hot_1:0\", shape=(32, 100, 65), dtype=float32)\n",
      "  << lstm_outputs  >>  Tensor(\"rnn/transpose:0\", shape=(32, 100, 128), dtype=float32)\n",
      " << logits >>  Tensor(\"logits/BiasAdd:0\", shape=(3200, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_steps = 100\n",
    "\n",
    "train_x, train_y = reshape_date(text_ints, batch_size, num_steps)\n",
    "\n",
    "model = CharRnn(n_classes=len(char_set), batch_size=batch_size, n_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0/1000, Iterations   20 | Train loss 3.23091\n",
      "Epoch  0/1000, Iterations   40 | Train loss 3.14201\n",
      "Epoch  1/1000, Iterations   60 | Train loss 3.15066\n",
      "Epoch  1/1000, Iterations   80 | Train loss 3.14746\n",
      "Epoch  1/1000, Iterations  100 | Train loss 3.09701\n",
      "Epoch  2/1000, Iterations  120 | Train loss 3.09089\n",
      "Epoch  2/1000, Iterations  140 | Train loss 3.02612\n",
      "Epoch  3/1000, Iterations  160 | Train loss 3.00430\n",
      "Epoch  3/1000, Iterations  180 | Train loss 2.98151\n",
      "Epoch  3/1000, Iterations  200 | Train loss 2.89937\n",
      "Epoch  4/1000, Iterations  220 | Train loss 2.89092\n",
      "Epoch  4/1000, Iterations  240 | Train loss 2.83746\n",
      "Epoch  5/1000, Iterations  260 | Train loss 2.83303\n",
      "Epoch  5/1000, Iterations  280 | Train loss 2.84107\n",
      "Epoch  5/1000, Iterations  300 | Train loss 2.76052\n",
      "Epoch  6/1000, Iterations  320 | Train loss 2.79072\n",
      "Epoch  6/1000, Iterations  340 | Train loss 2.75286\n",
      "Epoch  7/1000, Iterations  360 | Train loss 2.74247\n",
      "Epoch  7/1000, Iterations  380 | Train loss 2.72635\n",
      "Epoch  7/1000, Iterations  400 | Train loss 2.70219\n",
      "Epoch  8/1000, Iterations  420 | Train loss 2.71649\n",
      "Epoch  8/1000, Iterations  440 | Train loss 2.65942\n",
      "Epoch  9/1000, Iterations  460 | Train loss 2.68497\n",
      "Epoch  9/1000, Iterations  480 | Train loss 2.66975\n",
      "Epoch  9/1000, Iterations  500 | Train loss 2.64220\n",
      "Epoch 10/1000, Iterations  520 | Train loss 2.68864\n",
      "Epoch 10/1000, Iterations  540 | Train loss 2.61864\n",
      "Epoch 11/1000, Iterations  560 | Train loss 2.67971\n",
      "Epoch 11/1000, Iterations  580 | Train loss 2.64661\n",
      "Epoch 11/1000, Iterations  600 | Train loss 2.59283\n",
      "Epoch 12/1000, Iterations  620 | Train loss 2.65183\n",
      "Epoch 12/1000, Iterations  640 | Train loss 2.62091\n",
      "Epoch 13/1000, Iterations  660 | Train loss 2.61516\n",
      "Epoch 13/1000, Iterations  680 | Train loss 2.61641\n",
      "Epoch 13/1000, Iterations  700 | Train loss 2.54873\n",
      "Epoch 14/1000, Iterations  720 | Train loss 2.65223\n",
      "Epoch 14/1000, Iterations  740 | Train loss 2.58715\n",
      "Epoch 15/1000, Iterations  760 | Train loss 2.60363\n",
      "Epoch 15/1000, Iterations  780 | Train loss 2.62649\n",
      "Epoch 15/1000, Iterations  800 | Train loss 2.58682\n",
      "Epoch 16/1000, Iterations  820 | Train loss 2.62334\n",
      "Epoch 16/1000, Iterations  840 | Train loss 2.58327\n",
      "Epoch 17/1000, Iterations  860 | Train loss 2.59448\n",
      "Epoch 17/1000, Iterations  880 | Train loss 2.61483\n",
      "Epoch 17/1000, Iterations  900 | Train loss 2.55799\n",
      "Epoch 18/1000, Iterations  920 | Train loss 2.61054\n",
      "Epoch 18/1000, Iterations  940 | Train loss 2.57575\n",
      "Epoch 19/1000, Iterations  960 | Train loss 2.58684\n",
      "Epoch 19/1000, Iterations  980 | Train loss 2.58919\n",
      "Epoch 19/1000, Iterations 1000 | Train loss 2.55069\n",
      "Epoch 20/1000, Iterations 1020 | Train loss 2.60132\n",
      "Epoch 20/1000, Iterations 1040 | Train loss 2.54406\n",
      "Epoch 21/1000, Iterations 1060 | Train loss 2.60831\n",
      "Epoch 21/1000, Iterations 1080 | Train loss 2.57201\n",
      "Epoch 21/1000, Iterations 1100 | Train loss 2.52476\n",
      "Epoch 22/1000, Iterations 1120 | Train loss 2.58494\n",
      "Epoch 22/1000, Iterations 1140 | Train loss 2.58161\n",
      "Epoch 23/1000, Iterations 1160 | Train loss 2.54610\n",
      "Epoch 23/1000, Iterations 1180 | Train loss 2.58241\n",
      "Epoch 23/1000, Iterations 1200 | Train loss 2.51400\n",
      "Epoch 24/1000, Iterations 1220 | Train loss 2.55869\n",
      "Epoch 24/1000, Iterations 1240 | Train loss 2.54094\n",
      "Epoch 25/1000, Iterations 1260 | Train loss 2.54867\n",
      "Epoch 25/1000, Iterations 1280 | Train loss 2.52038\n",
      "Epoch 25/1000, Iterations 1300 | Train loss 2.48737\n",
      "Epoch 26/1000, Iterations 1320 | Train loss 2.55466\n",
      "Epoch 26/1000, Iterations 1340 | Train loss 2.50883\n",
      "Epoch 27/1000, Iterations 1360 | Train loss 2.54526\n",
      "Epoch 27/1000, Iterations 1380 | Train loss 2.56230\n",
      "Epoch 27/1000, Iterations 1400 | Train loss 2.48927\n",
      "Epoch 28/1000, Iterations 1420 | Train loss 2.54629\n",
      "Epoch 28/1000, Iterations 1440 | Train loss 2.51285\n",
      "Epoch 29/1000, Iterations 1460 | Train loss 2.55015\n",
      "Epoch 29/1000, Iterations 1480 | Train loss 2.55627\n",
      "Epoch 29/1000, Iterations 1500 | Train loss 2.48676\n",
      "Epoch 30/1000, Iterations 1520 | Train loss 2.51922\n",
      "Epoch 30/1000, Iterations 1540 | Train loss 2.54151\n",
      "Epoch 31/1000, Iterations 1560 | Train loss 2.52191\n",
      "Epoch 31/1000, Iterations 1580 | Train loss 2.51191\n",
      "Epoch 31/1000, Iterations 1600 | Train loss 2.49065\n",
      "Epoch 32/1000, Iterations 1620 | Train loss 2.54046\n",
      "Epoch 32/1000, Iterations 1640 | Train loss 2.51198\n",
      "Epoch 33/1000, Iterations 1660 | Train loss 2.53158\n",
      "Epoch 33/1000, Iterations 1680 | Train loss 2.52185\n",
      "Epoch 33/1000, Iterations 1700 | Train loss 2.46271\n",
      "Epoch 34/1000, Iterations 1720 | Train loss 2.52611\n",
      "Epoch 34/1000, Iterations 1740 | Train loss 2.53448\n",
      "Epoch 35/1000, Iterations 1760 | Train loss 2.51122\n",
      "Epoch 35/1000, Iterations 1780 | Train loss 2.54075\n",
      "Epoch 35/1000, Iterations 1800 | Train loss 2.47141\n",
      "Epoch 36/1000, Iterations 1820 | Train loss 2.53288\n",
      "Epoch 36/1000, Iterations 1840 | Train loss 2.47583\n",
      "Epoch 37/1000, Iterations 1860 | Train loss 2.53630\n",
      "Epoch 37/1000, Iterations 1880 | Train loss 2.51218\n",
      "Epoch 37/1000, Iterations 1900 | Train loss 2.47267\n",
      "Epoch 38/1000, Iterations 1920 | Train loss 2.52065\n",
      "Epoch 38/1000, Iterations 1940 | Train loss 2.50707\n",
      "Epoch 39/1000, Iterations 1960 | Train loss 2.47764\n",
      "Epoch 39/1000, Iterations 1980 | Train loss 2.49436\n",
      "Epoch 39/1000, Iterations 2000 | Train loss 2.48485\n",
      "Epoch 40/1000, Iterations 2020 | Train loss 2.48221\n",
      "Epoch 40/1000, Iterations 2040 | Train loss 2.51254\n",
      "Epoch 41/1000, Iterations 2060 | Train loss 2.53985\n",
      "Epoch 41/1000, Iterations 2080 | Train loss 2.53000\n",
      "Epoch 41/1000, Iterations 2100 | Train loss 2.42628\n",
      "Epoch 42/1000, Iterations 2120 | Train loss 2.49886\n",
      "Epoch 42/1000, Iterations 2140 | Train loss 2.48768\n",
      "Epoch 43/1000, Iterations 2160 | Train loss 2.52227\n",
      "Epoch 43/1000, Iterations 2180 | Train loss 2.48015\n",
      "Epoch 43/1000, Iterations 2200 | Train loss 2.44962\n",
      "Epoch 44/1000, Iterations 2220 | Train loss 2.50177\n",
      "Epoch 44/1000, Iterations 2240 | Train loss 2.51692\n",
      "Epoch 45/1000, Iterations 2260 | Train loss 2.50536\n",
      "Epoch 45/1000, Iterations 2280 | Train loss 2.50034\n",
      "Epoch 45/1000, Iterations 2300 | Train loss 2.43755\n",
      "Epoch 46/1000, Iterations 2320 | Train loss 2.47104\n",
      "Epoch 46/1000, Iterations 2340 | Train loss 2.47894\n",
      "Epoch 47/1000, Iterations 2360 | Train loss 2.49070\n",
      "Epoch 47/1000, Iterations 2380 | Train loss 2.50526\n",
      "Epoch 47/1000, Iterations 2400 | Train loss 2.38178\n",
      "Epoch 48/1000, Iterations 2420 | Train loss 2.51772\n",
      "Epoch 48/1000, Iterations 2440 | Train loss 2.46678\n",
      "Epoch 49/1000, Iterations 2460 | Train loss 2.47556\n",
      "Epoch 49/1000, Iterations 2480 | Train loss 2.51281\n",
      "Epoch 49/1000, Iterations 2500 | Train loss 2.40684\n",
      "Epoch 50/1000, Iterations 2520 | Train loss 2.47500\n",
      "Epoch 50/1000, Iterations 2540 | Train loss 2.44246\n",
      "Epoch 51/1000, Iterations 2560 | Train loss 2.45653\n",
      "Epoch 51/1000, Iterations 2580 | Train loss 2.48512\n",
      "Epoch 51/1000, Iterations 2600 | Train loss 2.44506\n",
      "Epoch 52/1000, Iterations 2620 | Train loss 2.43834\n",
      "Epoch 52/1000, Iterations 2640 | Train loss 2.47378\n",
      "Epoch 53/1000, Iterations 2660 | Train loss 2.48691\n",
      "Epoch 53/1000, Iterations 2680 | Train loss 2.51742\n",
      "Epoch 53/1000, Iterations 2700 | Train loss 2.40567\n",
      "Epoch 54/1000, Iterations 2720 | Train loss 2.46789\n",
      "Epoch 54/1000, Iterations 2740 | Train loss 2.46575\n",
      "Epoch 55/1000, Iterations 2760 | Train loss 2.44765\n",
      "Epoch 55/1000, Iterations 2780 | Train loss 2.51197\n",
      "Epoch 55/1000, Iterations 2800 | Train loss 2.40195\n",
      "Epoch 56/1000, Iterations 2820 | Train loss 2.44635\n",
      "Epoch 56/1000, Iterations 2840 | Train loss 2.44576\n",
      "Epoch 57/1000, Iterations 2860 | Train loss 2.48189\n",
      "Epoch 57/1000, Iterations 2880 | Train loss 2.46646\n",
      "Epoch 57/1000, Iterations 2900 | Train loss 2.38679\n",
      "Epoch 58/1000, Iterations 2920 | Train loss 2.44605\n",
      "Epoch 58/1000, Iterations 2940 | Train loss 2.45361\n",
      "Epoch 59/1000, Iterations 2960 | Train loss 2.42317\n",
      "Epoch 59/1000, Iterations 2980 | Train loss 2.47149\n",
      "Epoch 59/1000, Iterations 3000 | Train loss 2.40427\n",
      "Epoch 60/1000, Iterations 3020 | Train loss 2.48335\n",
      "Epoch 60/1000, Iterations 3040 | Train loss 2.46316\n",
      "Epoch 61/1000, Iterations 3060 | Train loss 2.42495\n",
      "Epoch 61/1000, Iterations 3080 | Train loss 2.49569\n",
      "Epoch 61/1000, Iterations 3100 | Train loss 2.38758\n",
      "Epoch 62/1000, Iterations 3120 | Train loss 2.44432\n",
      "Epoch 62/1000, Iterations 3140 | Train loss 2.42802\n",
      "Epoch 63/1000, Iterations 3160 | Train loss 2.44496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/1000, Iterations 3180 | Train loss 2.48197\n",
      "Epoch 63/1000, Iterations 3200 | Train loss 2.40939\n",
      "Epoch 64/1000, Iterations 3220 | Train loss 2.44484\n",
      "Epoch 64/1000, Iterations 3240 | Train loss 2.45490\n",
      "Epoch 65/1000, Iterations 3260 | Train loss 2.45881\n",
      "Epoch 65/1000, Iterations 3280 | Train loss 2.44700\n",
      "Epoch 65/1000, Iterations 3300 | Train loss 2.39626\n",
      "Epoch 66/1000, Iterations 3320 | Train loss 2.49094\n",
      "Epoch 66/1000, Iterations 3340 | Train loss 2.43533\n",
      "Epoch 67/1000, Iterations 3360 | Train loss 2.43199\n",
      "Epoch 67/1000, Iterations 3380 | Train loss 2.48754\n",
      "Epoch 67/1000, Iterations 3400 | Train loss 2.36570\n",
      "Epoch 68/1000, Iterations 3420 | Train loss 2.42765\n",
      "Epoch 68/1000, Iterations 3440 | Train loss 2.46714\n",
      "Epoch 69/1000, Iterations 3460 | Train loss 2.45856\n",
      "Epoch 69/1000, Iterations 3480 | Train loss 2.45396\n",
      "Epoch 69/1000, Iterations 3500 | Train loss 2.38819\n",
      "Epoch 70/1000, Iterations 3520 | Train loss 2.47010\n",
      "Epoch 70/1000, Iterations 3540 | Train loss 2.42718\n",
      "Epoch 71/1000, Iterations 3560 | Train loss 2.41548\n",
      "Epoch 71/1000, Iterations 3580 | Train loss 2.45599\n",
      "Epoch 71/1000, Iterations 3600 | Train loss 2.38476\n",
      "Epoch 72/1000, Iterations 3620 | Train loss 2.47089\n",
      "Epoch 72/1000, Iterations 3640 | Train loss 2.44515\n",
      "Epoch 73/1000, Iterations 3660 | Train loss 2.41361\n",
      "Epoch 73/1000, Iterations 3680 | Train loss 2.45230\n",
      "Epoch 73/1000, Iterations 3700 | Train loss 2.37824\n",
      "Epoch 74/1000, Iterations 3720 | Train loss 2.44809\n",
      "Epoch 74/1000, Iterations 3740 | Train loss 2.44887\n",
      "Epoch 75/1000, Iterations 3760 | Train loss 2.42752\n",
      "Epoch 75/1000, Iterations 3780 | Train loss 2.45070\n",
      "Epoch 75/1000, Iterations 3800 | Train loss 2.36017\n",
      "Epoch 76/1000, Iterations 3820 | Train loss 2.45246\n",
      "Epoch 76/1000, Iterations 3840 | Train loss 2.40397\n",
      "Epoch 77/1000, Iterations 3860 | Train loss 2.41979\n",
      "Epoch 77/1000, Iterations 3880 | Train loss 2.41443\n",
      "Epoch 77/1000, Iterations 3900 | Train loss 2.34865\n",
      "Epoch 78/1000, Iterations 3920 | Train loss 2.44871\n",
      "Epoch 78/1000, Iterations 3940 | Train loss 2.44805\n",
      "Epoch 79/1000, Iterations 3960 | Train loss 2.43019\n",
      "Epoch 79/1000, Iterations 3980 | Train loss 2.43883\n",
      "Epoch 79/1000, Iterations 4000 | Train loss 2.34507\n",
      "Epoch 80/1000, Iterations 4020 | Train loss 2.41618\n",
      "Epoch 80/1000, Iterations 4040 | Train loss 2.44761\n",
      "Epoch 81/1000, Iterations 4060 | Train loss 2.43027\n",
      "Epoch 81/1000, Iterations 4080 | Train loss 2.41885\n",
      "Epoch 81/1000, Iterations 4100 | Train loss 2.35395\n",
      "Epoch 82/1000, Iterations 4120 | Train loss 2.40041\n",
      "Epoch 82/1000, Iterations 4140 | Train loss 2.44703\n",
      "Epoch 83/1000, Iterations 4160 | Train loss 2.45719\n",
      "Epoch 83/1000, Iterations 4180 | Train loss 2.47993\n",
      "Epoch 83/1000, Iterations 4200 | Train loss 2.35766\n",
      "Epoch 84/1000, Iterations 4220 | Train loss 2.39800\n",
      "Epoch 84/1000, Iterations 4240 | Train loss 2.45454\n",
      "Epoch 85/1000, Iterations 4260 | Train loss 2.42739\n",
      "Epoch 85/1000, Iterations 4280 | Train loss 2.41410\n",
      "Epoch 85/1000, Iterations 4300 | Train loss 2.35512\n",
      "Epoch 86/1000, Iterations 4320 | Train loss 2.41758\n",
      "Epoch 86/1000, Iterations 4340 | Train loss 2.43470\n",
      "Epoch 87/1000, Iterations 4360 | Train loss 2.40643\n",
      "Epoch 87/1000, Iterations 4380 | Train loss 2.44109\n",
      "Epoch 87/1000, Iterations 4400 | Train loss 2.34409\n",
      "Epoch 88/1000, Iterations 4420 | Train loss 2.42251\n",
      "Epoch 88/1000, Iterations 4440 | Train loss 2.41660\n",
      "Epoch 89/1000, Iterations 4460 | Train loss 2.43666\n",
      "Epoch 89/1000, Iterations 4480 | Train loss 2.43001\n",
      "Epoch 89/1000, Iterations 4500 | Train loss 2.36102\n",
      "Epoch 90/1000, Iterations 4520 | Train loss 2.40944\n",
      "Epoch 90/1000, Iterations 4540 | Train loss 2.38981\n",
      "Epoch 91/1000, Iterations 4560 | Train loss 2.39740\n",
      "Epoch 91/1000, Iterations 4580 | Train loss 2.41918\n",
      "Epoch 91/1000, Iterations 4600 | Train loss 2.33649\n",
      "Epoch 92/1000, Iterations 4620 | Train loss 2.39425\n",
      "Epoch 92/1000, Iterations 4640 | Train loss 2.42000\n",
      "Epoch 93/1000, Iterations 4660 | Train loss 2.41659\n",
      "Epoch 93/1000, Iterations 4680 | Train loss 2.42533\n",
      "Epoch 93/1000, Iterations 4700 | Train loss 2.34090\n",
      "Epoch 94/1000, Iterations 4720 | Train loss 2.42316\n",
      "Epoch 94/1000, Iterations 4740 | Train loss 2.41135\n",
      "Epoch 95/1000, Iterations 4760 | Train loss 2.41900\n",
      "Epoch 95/1000, Iterations 4780 | Train loss 2.40349\n",
      "Epoch 95/1000, Iterations 4800 | Train loss 2.35859\n",
      "Epoch 96/1000, Iterations 4820 | Train loss 2.38385\n",
      "Epoch 96/1000, Iterations 4840 | Train loss 2.39784\n",
      "Epoch 97/1000, Iterations 4860 | Train loss 2.43845\n",
      "Epoch 97/1000, Iterations 4880 | Train loss 2.42977\n",
      "Epoch 97/1000, Iterations 4900 | Train loss 2.32641\n",
      "Epoch 98/1000, Iterations 4920 | Train loss 2.37291\n",
      "Epoch 98/1000, Iterations 4940 | Train loss 2.38641\n",
      "Epoch 99/1000, Iterations 4960 | Train loss 2.39593\n",
      "Epoch 99/1000, Iterations 4980 | Train loss 2.40745\n",
      "Epoch 99/1000, Iterations 5000 | Train loss 2.32636\n",
      "Epoch 100/1000, Iterations 5020 | Train loss 2.41319\n",
      "Epoch 100/1000, Iterations 5040 | Train loss 2.40062\n",
      "Epoch 101/1000, Iterations 5060 | Train loss 2.36693\n",
      "Epoch 101/1000, Iterations 5080 | Train loss 2.41312\n",
      "Epoch 101/1000, Iterations 5100 | Train loss 2.31527\n",
      "Epoch 102/1000, Iterations 5120 | Train loss 2.42861\n",
      "Epoch 102/1000, Iterations 5140 | Train loss 2.39780\n",
      "Epoch 103/1000, Iterations 5160 | Train loss 2.37517\n",
      "Epoch 103/1000, Iterations 5180 | Train loss 2.43679\n",
      "Epoch 103/1000, Iterations 5200 | Train loss 2.31854\n",
      "Epoch 104/1000, Iterations 5220 | Train loss 2.37038\n",
      "Epoch 104/1000, Iterations 5240 | Train loss 2.42326\n",
      "Epoch 105/1000, Iterations 5260 | Train loss 2.41142\n",
      "Epoch 105/1000, Iterations 5280 | Train loss 2.38321\n",
      "Epoch 105/1000, Iterations 5300 | Train loss 2.33482\n",
      "Epoch 106/1000, Iterations 5320 | Train loss 2.40420\n",
      "Epoch 106/1000, Iterations 5340 | Train loss 2.39129\n",
      "Epoch 107/1000, Iterations 5360 | Train loss 2.39630\n",
      "Epoch 107/1000, Iterations 5380 | Train loss 2.40335\n",
      "Epoch 107/1000, Iterations 5400 | Train loss 2.30784\n",
      "Epoch 108/1000, Iterations 5420 | Train loss 2.42504\n",
      "Epoch 108/1000, Iterations 5440 | Train loss 2.42230\n",
      "Epoch 109/1000, Iterations 5460 | Train loss 2.43076\n",
      "Epoch 109/1000, Iterations 5480 | Train loss 2.41787\n",
      "Epoch 109/1000, Iterations 5500 | Train loss 2.30416\n",
      "Epoch 110/1000, Iterations 5520 | Train loss 2.41424\n",
      "Epoch 110/1000, Iterations 5540 | Train loss 2.41206\n",
      "Epoch 111/1000, Iterations 5560 | Train loss 2.33909\n",
      "Epoch 111/1000, Iterations 5580 | Train loss 2.42326\n",
      "Epoch 111/1000, Iterations 5600 | Train loss 2.29307\n",
      "Epoch 112/1000, Iterations 5620 | Train loss 2.37847\n",
      "Epoch 112/1000, Iterations 5640 | Train loss 2.42057\n",
      "Epoch 113/1000, Iterations 5660 | Train loss 2.35376\n",
      "Epoch 113/1000, Iterations 5680 | Train loss 2.40094\n",
      "Epoch 113/1000, Iterations 5700 | Train loss 2.32218\n",
      "Epoch 114/1000, Iterations 5720 | Train loss 2.39158\n",
      "Epoch 114/1000, Iterations 5740 | Train loss 2.38212\n",
      "Epoch 115/1000, Iterations 5760 | Train loss 2.37015\n",
      "Epoch 115/1000, Iterations 5780 | Train loss 2.42663\n",
      "Epoch 115/1000, Iterations 5800 | Train loss 2.28870\n",
      "Epoch 116/1000, Iterations 5820 | Train loss 2.37449\n",
      "Epoch 116/1000, Iterations 5840 | Train loss 2.38621\n",
      "Epoch 117/1000, Iterations 5860 | Train loss 2.39185\n",
      "Epoch 117/1000, Iterations 5880 | Train loss 2.39051\n",
      "Epoch 117/1000, Iterations 5900 | Train loss 2.30122\n",
      "Epoch 118/1000, Iterations 5920 | Train loss 2.40937\n",
      "Epoch 118/1000, Iterations 5940 | Train loss 2.40447\n",
      "Epoch 119/1000, Iterations 5960 | Train loss 2.37347\n",
      "Epoch 119/1000, Iterations 5980 | Train loss 2.38013\n",
      "Epoch 119/1000, Iterations 6000 | Train loss 2.27203\n",
      "Epoch 120/1000, Iterations 6020 | Train loss 2.37466\n",
      "Epoch 120/1000, Iterations 6040 | Train loss 2.37857\n",
      "Epoch 121/1000, Iterations 6060 | Train loss 2.35860\n",
      "Epoch 121/1000, Iterations 6080 | Train loss 2.38270\n",
      "Epoch 121/1000, Iterations 6100 | Train loss 2.27246\n",
      "Epoch 122/1000, Iterations 6120 | Train loss 2.37294\n",
      "Epoch 122/1000, Iterations 6140 | Train loss 2.35357\n",
      "Epoch 123/1000, Iterations 6160 | Train loss 2.34763\n",
      "Epoch 123/1000, Iterations 6180 | Train loss 2.39004\n",
      "Epoch 123/1000, Iterations 6200 | Train loss 2.30841\n",
      "Epoch 124/1000, Iterations 6220 | Train loss 2.39040\n",
      "Epoch 124/1000, Iterations 6240 | Train loss 2.38498\n",
      "Epoch 125/1000, Iterations 6260 | Train loss 2.37517\n",
      "Epoch 125/1000, Iterations 6280 | Train loss 2.38026\n",
      "Epoch 125/1000, Iterations 6300 | Train loss 2.28309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/1000, Iterations 6320 | Train loss 2.36969\n",
      "Epoch 126/1000, Iterations 6340 | Train loss 2.36614\n",
      "Epoch 127/1000, Iterations 6360 | Train loss 2.39578\n",
      "Epoch 127/1000, Iterations 6380 | Train loss 2.40138\n",
      "Epoch 127/1000, Iterations 6400 | Train loss 2.32037\n",
      "Epoch 128/1000, Iterations 6420 | Train loss 2.37142\n",
      "Epoch 128/1000, Iterations 6440 | Train loss 2.36900\n",
      "Epoch 129/1000, Iterations 6460 | Train loss 2.33817\n",
      "Epoch 129/1000, Iterations 6480 | Train loss 2.36329\n",
      "Epoch 129/1000, Iterations 6500 | Train loss 2.28036\n",
      "Epoch 130/1000, Iterations 6520 | Train loss 2.37745\n",
      "Epoch 130/1000, Iterations 6540 | Train loss 2.37558\n",
      "Epoch 131/1000, Iterations 6560 | Train loss 2.38333\n",
      "Epoch 131/1000, Iterations 6580 | Train loss 2.37784\n",
      "Epoch 131/1000, Iterations 6600 | Train loss 2.28242\n",
      "Epoch 132/1000, Iterations 6620 | Train loss 2.38174\n",
      "Epoch 132/1000, Iterations 6640 | Train loss 2.38804\n",
      "Epoch 133/1000, Iterations 6660 | Train loss 2.36865\n",
      "Epoch 133/1000, Iterations 6680 | Train loss 2.38348\n",
      "Epoch 133/1000, Iterations 6700 | Train loss 2.24813\n",
      "Epoch 134/1000, Iterations 6720 | Train loss 2.32022\n",
      "Epoch 134/1000, Iterations 6740 | Train loss 2.37471\n",
      "Epoch 135/1000, Iterations 6760 | Train loss 2.34347\n",
      "Epoch 135/1000, Iterations 6780 | Train loss 2.40029\n",
      "Epoch 135/1000, Iterations 6800 | Train loss 2.29266\n",
      "Epoch 136/1000, Iterations 6820 | Train loss 2.38946\n",
      "Epoch 136/1000, Iterations 6840 | Train loss 2.38103\n",
      "Epoch 137/1000, Iterations 6860 | Train loss 2.38275\n",
      "Epoch 137/1000, Iterations 6880 | Train loss 2.38935\n",
      "Epoch 137/1000, Iterations 6900 | Train loss 2.29056\n",
      "Epoch 138/1000, Iterations 6920 | Train loss 2.32664\n",
      "Epoch 138/1000, Iterations 6940 | Train loss 2.35193\n",
      "Epoch 139/1000, Iterations 6960 | Train loss 2.34288\n",
      "Epoch 139/1000, Iterations 6980 | Train loss 2.33964\n",
      "Epoch 139/1000, Iterations 7000 | Train loss 2.29596\n",
      "Epoch 140/1000, Iterations 7020 | Train loss 2.33592\n",
      "Epoch 140/1000, Iterations 7040 | Train loss 2.34065\n",
      "Epoch 141/1000, Iterations 7060 | Train loss 2.36630\n",
      "Epoch 141/1000, Iterations 7080 | Train loss 2.36706\n",
      "Epoch 141/1000, Iterations 7100 | Train loss 2.25979\n",
      "Epoch 142/1000, Iterations 7120 | Train loss 2.37506\n",
      "Epoch 142/1000, Iterations 7140 | Train loss 2.37240\n",
      "Epoch 143/1000, Iterations 7160 | Train loss 2.34321\n",
      "Epoch 143/1000, Iterations 7180 | Train loss 2.33717\n",
      "Epoch 143/1000, Iterations 7200 | Train loss 2.28416\n",
      "Epoch 144/1000, Iterations 7220 | Train loss 2.33320\n",
      "Epoch 144/1000, Iterations 7240 | Train loss 2.36570\n",
      "Epoch 145/1000, Iterations 7260 | Train loss 2.35231\n",
      "Epoch 145/1000, Iterations 7280 | Train loss 2.39608\n",
      "Epoch 145/1000, Iterations 7300 | Train loss 2.27806\n",
      "Epoch 146/1000, Iterations 7320 | Train loss 2.35191\n",
      "Epoch 146/1000, Iterations 7340 | Train loss 2.37375\n",
      "Epoch 147/1000, Iterations 7360 | Train loss 2.35233\n",
      "Epoch 147/1000, Iterations 7380 | Train loss 2.35250\n",
      "Epoch 147/1000, Iterations 7400 | Train loss 2.26809\n",
      "Epoch 148/1000, Iterations 7420 | Train loss 2.30698\n",
      "Epoch 148/1000, Iterations 7440 | Train loss 2.36629\n",
      "Epoch 149/1000, Iterations 7460 | Train loss 2.34591\n",
      "Epoch 149/1000, Iterations 7480 | Train loss 2.39608\n",
      "Epoch 149/1000, Iterations 7500 | Train loss 2.26454\n",
      "Epoch 150/1000, Iterations 7520 | Train loss 2.33695\n",
      "Epoch 150/1000, Iterations 7540 | Train loss 2.35081\n",
      "Epoch 151/1000, Iterations 7560 | Train loss 2.34900\n",
      "Epoch 151/1000, Iterations 7580 | Train loss 2.34599\n",
      "Epoch 151/1000, Iterations 7600 | Train loss 2.26134\n",
      "Epoch 152/1000, Iterations 7620 | Train loss 2.27500\n",
      "Epoch 152/1000, Iterations 7640 | Train loss 2.38030\n",
      "Epoch 153/1000, Iterations 7660 | Train loss 2.35652\n",
      "Epoch 153/1000, Iterations 7680 | Train loss 2.33296\n",
      "Epoch 153/1000, Iterations 7700 | Train loss 2.27517\n",
      "Epoch 154/1000, Iterations 7720 | Train loss 2.36517\n",
      "Epoch 154/1000, Iterations 7740 | Train loss 2.33260\n",
      "Epoch 155/1000, Iterations 7760 | Train loss 2.37156\n",
      "Epoch 155/1000, Iterations 7780 | Train loss 2.36428\n",
      "Epoch 155/1000, Iterations 7800 | Train loss 2.27818\n",
      "Epoch 156/1000, Iterations 7820 | Train loss 2.28846\n",
      "Epoch 156/1000, Iterations 7840 | Train loss 2.33105\n",
      "Epoch 157/1000, Iterations 7860 | Train loss 2.36072\n",
      "Epoch 157/1000, Iterations 7880 | Train loss 2.31607\n",
      "Epoch 157/1000, Iterations 7900 | Train loss 2.27002\n",
      "Epoch 158/1000, Iterations 7920 | Train loss 2.31206\n",
      "Epoch 158/1000, Iterations 7940 | Train loss 2.36439\n",
      "Epoch 159/1000, Iterations 7960 | Train loss 2.34085\n",
      "Epoch 159/1000, Iterations 7980 | Train loss 2.33899\n",
      "Epoch 159/1000, Iterations 8000 | Train loss 2.25231\n",
      "Epoch 160/1000, Iterations 8020 | Train loss 2.31923\n",
      "Epoch 160/1000, Iterations 8040 | Train loss 2.32038\n",
      "Epoch 161/1000, Iterations 8060 | Train loss 2.37228\n",
      "Epoch 161/1000, Iterations 8080 | Train loss 2.33495\n",
      "Epoch 161/1000, Iterations 8100 | Train loss 2.26630\n",
      "Epoch 162/1000, Iterations 8120 | Train loss 2.33536\n",
      "Epoch 162/1000, Iterations 8140 | Train loss 2.33928\n",
      "Epoch 163/1000, Iterations 8160 | Train loss 2.32683\n",
      "Epoch 163/1000, Iterations 8180 | Train loss 2.33733\n",
      "Epoch 163/1000, Iterations 8200 | Train loss 2.23116\n",
      "Epoch 164/1000, Iterations 8220 | Train loss 2.34459\n",
      "Epoch 164/1000, Iterations 8240 | Train loss 2.37805\n",
      "Epoch 165/1000, Iterations 8260 | Train loss 2.31856\n",
      "Epoch 165/1000, Iterations 8280 | Train loss 2.35144\n",
      "Epoch 165/1000, Iterations 8300 | Train loss 2.28413\n",
      "Epoch 166/1000, Iterations 8320 | Train loss 2.29825\n",
      "Epoch 166/1000, Iterations 8340 | Train loss 2.33440\n",
      "Epoch 167/1000, Iterations 8360 | Train loss 2.31447\n",
      "Epoch 167/1000, Iterations 8380 | Train loss 2.34797\n",
      "Epoch 167/1000, Iterations 8400 | Train loss 2.26593\n",
      "Epoch 168/1000, Iterations 8420 | Train loss 2.32466\n",
      "Epoch 168/1000, Iterations 8440 | Train loss 2.35893\n",
      "Epoch 169/1000, Iterations 8460 | Train loss 2.31695\n",
      "Epoch 169/1000, Iterations 8480 | Train loss 2.34106\n",
      "Epoch 169/1000, Iterations 8500 | Train loss 2.25810\n",
      "Epoch 170/1000, Iterations 8520 | Train loss 2.36798\n",
      "Epoch 170/1000, Iterations 8540 | Train loss 2.32676\n",
      "Epoch 171/1000, Iterations 8560 | Train loss 2.31709\n",
      "Epoch 171/1000, Iterations 8580 | Train loss 2.34656\n",
      "Epoch 171/1000, Iterations 8600 | Train loss 2.25911\n",
      "Epoch 172/1000, Iterations 8620 | Train loss 2.33139\n",
      "Epoch 172/1000, Iterations 8640 | Train loss 2.36710\n",
      "Epoch 173/1000, Iterations 8660 | Train loss 2.35344\n",
      "Epoch 173/1000, Iterations 8680 | Train loss 2.34404\n",
      "Epoch 173/1000, Iterations 8700 | Train loss 2.29176\n",
      "Epoch 174/1000, Iterations 8720 | Train loss 2.33949\n",
      "Epoch 174/1000, Iterations 8740 | Train loss 2.33960\n",
      "Epoch 175/1000, Iterations 8760 | Train loss 2.34585\n",
      "Epoch 175/1000, Iterations 8780 | Train loss 2.37903\n",
      "Epoch 175/1000, Iterations 8800 | Train loss 2.25261\n",
      "Epoch 176/1000, Iterations 8820 | Train loss 2.30992\n",
      "Epoch 176/1000, Iterations 8840 | Train loss 2.33289\n",
      "Epoch 177/1000, Iterations 8860 | Train loss 2.34538\n",
      "Epoch 177/1000, Iterations 8880 | Train loss 2.35165\n",
      "Epoch 177/1000, Iterations 8900 | Train loss 2.24131\n",
      "Epoch 178/1000, Iterations 8920 | Train loss 2.32259\n",
      "Epoch 178/1000, Iterations 8940 | Train loss 2.34349\n",
      "Epoch 179/1000, Iterations 8960 | Train loss 2.31016\n",
      "Epoch 179/1000, Iterations 8980 | Train loss 2.34852\n",
      "Epoch 179/1000, Iterations 9000 | Train loss 2.22601\n",
      "Epoch 180/1000, Iterations 9020 | Train loss 2.34617\n",
      "Epoch 180/1000, Iterations 9040 | Train loss 2.32268\n",
      "Epoch 181/1000, Iterations 9060 | Train loss 2.33304\n",
      "Epoch 181/1000, Iterations 9080 | Train loss 2.33985\n",
      "Epoch 181/1000, Iterations 9100 | Train loss 2.28310\n",
      "Epoch 182/1000, Iterations 9120 | Train loss 2.29446\n",
      "Epoch 182/1000, Iterations 9140 | Train loss 2.35466\n",
      "Epoch 183/1000, Iterations 9160 | Train loss 2.32159\n",
      "Epoch 183/1000, Iterations 9180 | Train loss 2.33619\n",
      "Epoch 183/1000, Iterations 9200 | Train loss 2.27466\n",
      "Epoch 184/1000, Iterations 9220 | Train loss 2.30906\n",
      "Epoch 184/1000, Iterations 9240 | Train loss 2.33505\n",
      "Epoch 185/1000, Iterations 9260 | Train loss 2.34089\n",
      "Epoch 185/1000, Iterations 9280 | Train loss 2.39324\n",
      "Epoch 185/1000, Iterations 9300 | Train loss 2.23972\n",
      "Epoch 186/1000, Iterations 9320 | Train loss 2.30803\n",
      "Epoch 186/1000, Iterations 9340 | Train loss 2.33398\n",
      "Epoch 187/1000, Iterations 9360 | Train loss 2.33228\n",
      "Epoch 187/1000, Iterations 9380 | Train loss 2.34355\n",
      "Epoch 187/1000, Iterations 9400 | Train loss 2.23659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/1000, Iterations 9420 | Train loss 2.33031\n",
      "Epoch 188/1000, Iterations 9440 | Train loss 2.29204\n",
      "Epoch 189/1000, Iterations 9460 | Train loss 2.35961\n",
      "Epoch 189/1000, Iterations 9480 | Train loss 2.31846\n",
      "Epoch 189/1000, Iterations 9500 | Train loss 2.28272\n",
      "Epoch 190/1000, Iterations 9520 | Train loss 2.31879\n",
      "Epoch 190/1000, Iterations 9540 | Train loss 2.30917\n",
      "Epoch 191/1000, Iterations 9560 | Train loss 2.32545\n",
      "Epoch 191/1000, Iterations 9580 | Train loss 2.34503\n",
      "Epoch 191/1000, Iterations 9600 | Train loss 2.22636\n",
      "Epoch 192/1000, Iterations 9620 | Train loss 2.32304\n",
      "Epoch 192/1000, Iterations 9640 | Train loss 2.36196\n",
      "Epoch 193/1000, Iterations 9660 | Train loss 2.30706\n",
      "Epoch 193/1000, Iterations 9680 | Train loss 2.34526\n",
      "Epoch 193/1000, Iterations 9700 | Train loss 2.20997\n",
      "Epoch 194/1000, Iterations 9720 | Train loss 2.35365\n",
      "Epoch 194/1000, Iterations 9740 | Train loss 2.30980\n",
      "Epoch 195/1000, Iterations 9760 | Train loss 2.36541\n",
      "Epoch 195/1000, Iterations 9780 | Train loss 2.33220\n",
      "Epoch 195/1000, Iterations 9800 | Train loss 2.19752\n",
      "Epoch 196/1000, Iterations 9820 | Train loss 2.30696\n",
      "Epoch 196/1000, Iterations 9840 | Train loss 2.33461\n",
      "Epoch 197/1000, Iterations 9860 | Train loss 2.31858\n",
      "Epoch 197/1000, Iterations 9880 | Train loss 2.34677\n",
      "Epoch 197/1000, Iterations 9900 | Train loss 2.24130\n",
      "Epoch 198/1000, Iterations 9920 | Train loss 2.29785\n",
      "Epoch 198/1000, Iterations 9940 | Train loss 2.30888\n",
      "Epoch 199/1000, Iterations 9960 | Train loss 2.33181\n",
      "Epoch 199/1000, Iterations 9980 | Train loss 2.31837\n",
      "Epoch 199/1000, Iterations 10000 | Train loss 2.20161\n",
      "Epoch 200/1000, Iterations 10020 | Train loss 2.32191\n",
      "Epoch 200/1000, Iterations 10040 | Train loss 2.32378\n",
      "Epoch 201/1000, Iterations 10060 | Train loss 2.28052\n",
      "Epoch 201/1000, Iterations 10080 | Train loss 2.32073\n",
      "Epoch 201/1000, Iterations 10100 | Train loss 2.20677\n",
      "Epoch 202/1000, Iterations 10120 | Train loss 2.30788\n",
      "Epoch 202/1000, Iterations 10140 | Train loss 2.34370\n",
      "Epoch 203/1000, Iterations 10160 | Train loss 2.32425\n",
      "Epoch 203/1000, Iterations 10180 | Train loss 2.29538\n",
      "Epoch 203/1000, Iterations 10200 | Train loss 2.22264\n",
      "Epoch 204/1000, Iterations 10220 | Train loss 2.31731\n",
      "Epoch 204/1000, Iterations 10240 | Train loss 2.31741\n",
      "Epoch 205/1000, Iterations 10260 | Train loss 2.29524\n",
      "Epoch 205/1000, Iterations 10280 | Train loss 2.35773\n",
      "Epoch 205/1000, Iterations 10300 | Train loss 2.18499\n",
      "Epoch 206/1000, Iterations 10320 | Train loss 2.28705\n",
      "Epoch 206/1000, Iterations 10340 | Train loss 2.32912\n",
      "Epoch 207/1000, Iterations 10360 | Train loss 2.33434\n",
      "Epoch 207/1000, Iterations 10380 | Train loss 2.34250\n",
      "Epoch 207/1000, Iterations 10400 | Train loss 2.24529\n",
      "Epoch 208/1000, Iterations 10420 | Train loss 2.27788\n",
      "Epoch 208/1000, Iterations 10440 | Train loss 2.30065\n",
      "Epoch 209/1000, Iterations 10460 | Train loss 2.26206\n",
      "Epoch 209/1000, Iterations 10480 | Train loss 2.37612\n",
      "Epoch 209/1000, Iterations 10500 | Train loss 2.21361\n",
      "Epoch 210/1000, Iterations 10520 | Train loss 2.30874\n",
      "Epoch 210/1000, Iterations 10540 | Train loss 2.34461\n",
      "Epoch 211/1000, Iterations 10560 | Train loss 2.31949\n",
      "Epoch 211/1000, Iterations 10580 | Train loss 2.33376\n",
      "Epoch 211/1000, Iterations 10600 | Train loss 2.25205\n",
      "Epoch 212/1000, Iterations 10620 | Train loss 2.28109\n",
      "Epoch 212/1000, Iterations 10640 | Train loss 2.30418\n",
      "Epoch 213/1000, Iterations 10660 | Train loss 2.30785\n",
      "Epoch 213/1000, Iterations 10680 | Train loss 2.30513\n",
      "Epoch 213/1000, Iterations 10700 | Train loss 2.26447\n",
      "Epoch 214/1000, Iterations 10720 | Train loss 2.30535\n",
      "Epoch 214/1000, Iterations 10740 | Train loss 2.33401\n",
      "Epoch 215/1000, Iterations 10760 | Train loss 2.31933\n",
      "Epoch 215/1000, Iterations 10780 | Train loss 2.29378\n",
      "Epoch 215/1000, Iterations 10800 | Train loss 2.22726\n",
      "Epoch 216/1000, Iterations 10820 | Train loss 2.32622\n",
      "Epoch 216/1000, Iterations 10840 | Train loss 2.33718\n",
      "Epoch 217/1000, Iterations 10860 | Train loss 2.31721\n",
      "Epoch 217/1000, Iterations 10880 | Train loss 2.31506\n",
      "Epoch 217/1000, Iterations 10900 | Train loss 2.20044\n",
      "Epoch 218/1000, Iterations 10920 | Train loss 2.30320\n",
      "Epoch 218/1000, Iterations 10940 | Train loss 2.28762\n",
      "Epoch 219/1000, Iterations 10960 | Train loss 2.30408\n",
      "Epoch 219/1000, Iterations 10980 | Train loss 2.32729\n",
      "Epoch 219/1000, Iterations 11000 | Train loss 2.22927\n",
      "Epoch 220/1000, Iterations 11020 | Train loss 2.29809\n",
      "Epoch 220/1000, Iterations 11040 | Train loss 2.37627\n",
      "Epoch 221/1000, Iterations 11060 | Train loss 2.37192\n",
      "Epoch 221/1000, Iterations 11080 | Train loss 2.33156\n",
      "Epoch 221/1000, Iterations 11100 | Train loss 2.21200\n",
      "Epoch 222/1000, Iterations 11120 | Train loss 2.22935\n",
      "Epoch 222/1000, Iterations 11140 | Train loss 2.31215\n",
      "Epoch 223/1000, Iterations 11160 | Train loss 2.33810\n",
      "Epoch 223/1000, Iterations 11180 | Train loss 2.27489\n",
      "Epoch 223/1000, Iterations 11200 | Train loss 2.19826\n",
      "Epoch 224/1000, Iterations 11220 | Train loss 2.30803\n",
      "Epoch 224/1000, Iterations 11240 | Train loss 2.26650\n",
      "Epoch 225/1000, Iterations 11260 | Train loss 2.27352\n",
      "Epoch 225/1000, Iterations 11280 | Train loss 2.31421\n",
      "Epoch 225/1000, Iterations 11300 | Train loss 2.19164\n",
      "Epoch 226/1000, Iterations 11320 | Train loss 2.26594\n",
      "Epoch 226/1000, Iterations 11340 | Train loss 2.31233\n",
      "Epoch 227/1000, Iterations 11360 | Train loss 2.29459\n",
      "Epoch 227/1000, Iterations 11380 | Train loss 2.31274\n",
      "Epoch 227/1000, Iterations 11400 | Train loss 2.23122\n",
      "Epoch 228/1000, Iterations 11420 | Train loss 2.27968\n",
      "Epoch 228/1000, Iterations 11440 | Train loss 2.29362\n",
      "Epoch 229/1000, Iterations 11460 | Train loss 2.28053\n",
      "Epoch 229/1000, Iterations 11480 | Train loss 2.29424\n",
      "Epoch 229/1000, Iterations 11500 | Train loss 2.24146\n",
      "Epoch 230/1000, Iterations 11520 | Train loss 2.31919\n",
      "Epoch 230/1000, Iterations 11540 | Train loss 2.32722\n",
      "Epoch 231/1000, Iterations 11560 | Train loss 2.30848\n",
      "Epoch 231/1000, Iterations 11580 | Train loss 2.27398\n",
      "Epoch 231/1000, Iterations 11600 | Train loss 2.22297\n",
      "Epoch 232/1000, Iterations 11620 | Train loss 2.27485\n",
      "Epoch 232/1000, Iterations 11640 | Train loss 2.29784\n",
      "Epoch 233/1000, Iterations 11660 | Train loss 2.30756\n",
      "Epoch 233/1000, Iterations 11680 | Train loss 2.31303\n",
      "Epoch 233/1000, Iterations 11700 | Train loss 2.19858\n",
      "Epoch 234/1000, Iterations 11720 | Train loss 2.26097\n",
      "Epoch 234/1000, Iterations 11740 | Train loss 2.29977\n",
      "Epoch 235/1000, Iterations 11760 | Train loss 2.28086\n",
      "Epoch 235/1000, Iterations 11780 | Train loss 2.32592\n",
      "Epoch 235/1000, Iterations 11800 | Train loss 2.20457\n",
      "Epoch 236/1000, Iterations 11820 | Train loss 2.29551\n",
      "Epoch 236/1000, Iterations 11840 | Train loss 2.30588\n",
      "Epoch 237/1000, Iterations 11860 | Train loss 2.30979\n",
      "Epoch 237/1000, Iterations 11880 | Train loss 2.33748\n",
      "Epoch 237/1000, Iterations 11900 | Train loss 2.19175\n",
      "Epoch 238/1000, Iterations 11920 | Train loss 2.30101\n",
      "Epoch 238/1000, Iterations 11940 | Train loss 2.30917\n",
      "Epoch 239/1000, Iterations 11960 | Train loss 2.30125\n",
      "Epoch 239/1000, Iterations 11980 | Train loss 2.30896\n",
      "Epoch 239/1000, Iterations 12000 | Train loss 2.20065\n",
      "Epoch 240/1000, Iterations 12020 | Train loss 2.25700\n",
      "Epoch 240/1000, Iterations 12040 | Train loss 2.32258\n",
      "Epoch 241/1000, Iterations 12060 | Train loss 2.29418\n",
      "Epoch 241/1000, Iterations 12080 | Train loss 2.30033\n",
      "Epoch 241/1000, Iterations 12100 | Train loss 2.19937\n",
      "Epoch 242/1000, Iterations 12120 | Train loss 2.28183\n",
      "Epoch 242/1000, Iterations 12140 | Train loss 2.31061\n",
      "Epoch 243/1000, Iterations 12160 | Train loss 2.32333\n",
      "Epoch 243/1000, Iterations 12180 | Train loss 2.30499\n",
      "Epoch 243/1000, Iterations 12200 | Train loss 2.22568\n",
      "Epoch 244/1000, Iterations 12220 | Train loss 2.30645\n",
      "Epoch 244/1000, Iterations 12240 | Train loss 2.28263\n",
      "Epoch 245/1000, Iterations 12260 | Train loss 2.31102\n",
      "Epoch 245/1000, Iterations 12280 | Train loss 2.32208\n",
      "Epoch 245/1000, Iterations 12300 | Train loss 2.20239\n",
      "Epoch 246/1000, Iterations 12320 | Train loss 2.28244\n",
      "Epoch 246/1000, Iterations 12340 | Train loss 2.28860\n",
      "Epoch 247/1000, Iterations 12360 | Train loss 2.28073\n",
      "Epoch 247/1000, Iterations 12380 | Train loss 2.33095\n",
      "Epoch 247/1000, Iterations 12400 | Train loss 2.18811\n",
      "Epoch 248/1000, Iterations 12420 | Train loss 2.32749\n",
      "Epoch 248/1000, Iterations 12440 | Train loss 2.29242\n",
      "Epoch 249/1000, Iterations 12460 | Train loss 2.27329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249/1000, Iterations 12480 | Train loss 2.33768\n",
      "Epoch 249/1000, Iterations 12500 | Train loss 2.22374\n",
      "Epoch 250/1000, Iterations 12520 | Train loss 2.28210\n",
      "Epoch 250/1000, Iterations 12540 | Train loss 2.30507\n",
      "Epoch 251/1000, Iterations 12560 | Train loss 2.28249\n",
      "Epoch 251/1000, Iterations 12580 | Train loss 2.34767\n",
      "Epoch 251/1000, Iterations 12600 | Train loss 2.20333\n",
      "Epoch 252/1000, Iterations 12620 | Train loss 2.27592\n",
      "Epoch 252/1000, Iterations 12640 | Train loss 2.32547\n",
      "Epoch 253/1000, Iterations 12660 | Train loss 2.30033\n",
      "Epoch 253/1000, Iterations 12680 | Train loss 2.28483\n",
      "Epoch 253/1000, Iterations 12700 | Train loss 2.23877\n",
      "Epoch 254/1000, Iterations 12720 | Train loss 2.29362\n",
      "Epoch 254/1000, Iterations 12740 | Train loss 2.27772\n",
      "Epoch 255/1000, Iterations 12760 | Train loss 2.31334\n",
      "Epoch 255/1000, Iterations 12780 | Train loss 2.32882\n",
      "Epoch 255/1000, Iterations 12800 | Train loss 2.17849\n",
      "Epoch 256/1000, Iterations 12820 | Train loss 2.27429\n",
      "Epoch 256/1000, Iterations 12840 | Train loss 2.28960\n",
      "Epoch 257/1000, Iterations 12860 | Train loss 2.28198\n",
      "Epoch 257/1000, Iterations 12880 | Train loss 2.30592\n",
      "Epoch 257/1000, Iterations 12900 | Train loss 2.21770\n",
      "Epoch 258/1000, Iterations 12920 | Train loss 2.27298\n",
      "Epoch 258/1000, Iterations 12940 | Train loss 2.31841\n",
      "Epoch 259/1000, Iterations 12960 | Train loss 2.27433\n",
      "Epoch 259/1000, Iterations 12980 | Train loss 2.29051\n",
      "Epoch 259/1000, Iterations 13000 | Train loss 2.23801\n",
      "Epoch 260/1000, Iterations 13020 | Train loss 2.26372\n",
      "Epoch 260/1000, Iterations 13040 | Train loss 2.30134\n",
      "Epoch 261/1000, Iterations 13060 | Train loss 2.29765\n",
      "Epoch 261/1000, Iterations 13080 | Train loss 2.31984\n",
      "Epoch 261/1000, Iterations 13100 | Train loss 2.20281\n",
      "Epoch 262/1000, Iterations 13120 | Train loss 2.28833\n",
      "Epoch 262/1000, Iterations 13140 | Train loss 2.29874\n",
      "Epoch 263/1000, Iterations 13160 | Train loss 2.29088\n",
      "Epoch 263/1000, Iterations 13180 | Train loss 2.26502\n",
      "Epoch 263/1000, Iterations 13200 | Train loss 2.22118\n",
      "Epoch 264/1000, Iterations 13220 | Train loss 2.28634\n",
      "Epoch 264/1000, Iterations 13240 | Train loss 2.33563\n",
      "Epoch 265/1000, Iterations 13260 | Train loss 2.28144\n",
      "Epoch 265/1000, Iterations 13280 | Train loss 2.33053\n",
      "Epoch 265/1000, Iterations 13300 | Train loss 2.19736\n",
      "Epoch 266/1000, Iterations 13320 | Train loss 2.27618\n",
      "Epoch 266/1000, Iterations 13340 | Train loss 2.28887\n",
      "Epoch 267/1000, Iterations 13360 | Train loss 2.28121\n",
      "Epoch 267/1000, Iterations 13380 | Train loss 2.33563\n",
      "Epoch 267/1000, Iterations 13400 | Train loss 2.20873\n",
      "Epoch 268/1000, Iterations 13420 | Train loss 2.25476\n",
      "Epoch 268/1000, Iterations 13440 | Train loss 2.30870\n",
      "Epoch 269/1000, Iterations 13460 | Train loss 2.26962\n",
      "Epoch 269/1000, Iterations 13480 | Train loss 2.34005\n",
      "Epoch 269/1000, Iterations 13500 | Train loss 2.21366\n",
      "Epoch 270/1000, Iterations 13520 | Train loss 2.25507\n",
      "Epoch 270/1000, Iterations 13540 | Train loss 2.31003\n",
      "Epoch 271/1000, Iterations 13560 | Train loss 2.28981\n",
      "Epoch 271/1000, Iterations 13580 | Train loss 2.31226\n",
      "Epoch 271/1000, Iterations 13600 | Train loss 2.15910\n",
      "Epoch 272/1000, Iterations 13620 | Train loss 2.27445\n",
      "Epoch 272/1000, Iterations 13640 | Train loss 2.31963\n",
      "Epoch 273/1000, Iterations 13660 | Train loss 2.30896\n",
      "Epoch 273/1000, Iterations 13680 | Train loss 2.29570\n",
      "Epoch 273/1000, Iterations 13700 | Train loss 2.22220\n",
      "Epoch 274/1000, Iterations 13720 | Train loss 2.26270\n",
      "Epoch 274/1000, Iterations 13740 | Train loss 2.26678\n",
      "Epoch 275/1000, Iterations 13760 | Train loss 2.29787\n",
      "Epoch 275/1000, Iterations 13780 | Train loss 2.27623\n",
      "Epoch 275/1000, Iterations 13800 | Train loss 2.16328\n",
      "Epoch 276/1000, Iterations 13820 | Train loss 2.27947\n",
      "Epoch 276/1000, Iterations 13840 | Train loss 2.30031\n",
      "Epoch 277/1000, Iterations 13860 | Train loss 2.23567\n",
      "Epoch 277/1000, Iterations 13880 | Train loss 2.29588\n",
      "Epoch 277/1000, Iterations 13900 | Train loss 2.22503\n",
      "Epoch 278/1000, Iterations 13920 | Train loss 2.27004\n",
      "Epoch 278/1000, Iterations 13940 | Train loss 2.26068\n",
      "Epoch 279/1000, Iterations 13960 | Train loss 2.26999\n",
      "Epoch 279/1000, Iterations 13980 | Train loss 2.29743\n",
      "Epoch 279/1000, Iterations 14000 | Train loss 2.20554\n",
      "Epoch 280/1000, Iterations 14020 | Train loss 2.26496\n",
      "Epoch 280/1000, Iterations 14040 | Train loss 2.33008\n",
      "Epoch 281/1000, Iterations 14060 | Train loss 2.33593\n",
      "Epoch 281/1000, Iterations 14080 | Train loss 2.26076\n",
      "Epoch 281/1000, Iterations 14100 | Train loss 2.19667\n",
      "Epoch 282/1000, Iterations 14120 | Train loss 2.27677\n",
      "Epoch 282/1000, Iterations 14140 | Train loss 2.29142\n",
      "Epoch 283/1000, Iterations 14160 | Train loss 2.27941\n",
      "Epoch 283/1000, Iterations 14180 | Train loss 2.27697\n",
      "Epoch 283/1000, Iterations 14200 | Train loss 2.23393\n",
      "Epoch 284/1000, Iterations 14220 | Train loss 2.29262\n",
      "Epoch 284/1000, Iterations 14240 | Train loss 2.28537\n",
      "Epoch 285/1000, Iterations 14260 | Train loss 2.29965\n",
      "Epoch 285/1000, Iterations 14280 | Train loss 2.30037\n",
      "Epoch 285/1000, Iterations 14300 | Train loss 2.22977\n",
      "Epoch 286/1000, Iterations 14320 | Train loss 2.27382\n",
      "Epoch 286/1000, Iterations 14340 | Train loss 2.26214\n",
      "Epoch 287/1000, Iterations 14360 | Train loss 2.32094\n",
      "Epoch 287/1000, Iterations 14380 | Train loss 2.24734\n",
      "Epoch 287/1000, Iterations 14400 | Train loss 2.21396\n",
      "Epoch 288/1000, Iterations 14420 | Train loss 2.28576\n",
      "Epoch 288/1000, Iterations 14440 | Train loss 2.29730\n",
      "Epoch 289/1000, Iterations 14460 | Train loss 2.29654\n",
      "Epoch 289/1000, Iterations 14480 | Train loss 2.28531\n",
      "Epoch 289/1000, Iterations 14500 | Train loss 2.15786\n",
      "Epoch 290/1000, Iterations 14520 | Train loss 2.28703\n",
      "Epoch 290/1000, Iterations 14540 | Train loss 2.30943\n",
      "Epoch 291/1000, Iterations 14560 | Train loss 2.23806\n",
      "Epoch 291/1000, Iterations 14580 | Train loss 2.30656\n",
      "Epoch 291/1000, Iterations 14600 | Train loss 2.18479\n",
      "Epoch 292/1000, Iterations 14620 | Train loss 2.28495\n",
      "Epoch 292/1000, Iterations 14640 | Train loss 2.27570\n",
      "Epoch 293/1000, Iterations 14660 | Train loss 2.26805\n",
      "Epoch 293/1000, Iterations 14680 | Train loss 2.29634\n",
      "Epoch 293/1000, Iterations 14700 | Train loss 2.17609\n",
      "Epoch 294/1000, Iterations 14720 | Train loss 2.24556\n",
      "Epoch 294/1000, Iterations 14740 | Train loss 2.31414\n",
      "Epoch 295/1000, Iterations 14760 | Train loss 2.27831\n",
      "Epoch 295/1000, Iterations 14780 | Train loss 2.28204\n",
      "Epoch 295/1000, Iterations 14800 | Train loss 2.21040\n",
      "Epoch 296/1000, Iterations 14820 | Train loss 2.23010\n",
      "Epoch 296/1000, Iterations 14840 | Train loss 2.30908\n",
      "Epoch 297/1000, Iterations 14860 | Train loss 2.27173\n",
      "Epoch 297/1000, Iterations 14880 | Train loss 2.26875\n",
      "Epoch 297/1000, Iterations 14900 | Train loss 2.17240\n",
      "Epoch 298/1000, Iterations 14920 | Train loss 2.28465\n",
      "Epoch 298/1000, Iterations 14940 | Train loss 2.31623\n",
      "Epoch 299/1000, Iterations 14960 | Train loss 2.29996\n",
      "Epoch 299/1000, Iterations 14980 | Train loss 2.29849\n",
      "Epoch 299/1000, Iterations 15000 | Train loss 2.20629\n",
      "Epoch 300/1000, Iterations 15020 | Train loss 2.26599\n",
      "Epoch 300/1000, Iterations 15040 | Train loss 2.26714\n",
      "Epoch 301/1000, Iterations 15060 | Train loss 2.23605\n",
      "Epoch 301/1000, Iterations 15080 | Train loss 2.30442\n",
      "Epoch 301/1000, Iterations 15100 | Train loss 2.19001\n",
      "Epoch 302/1000, Iterations 15120 | Train loss 2.26552\n",
      "Epoch 302/1000, Iterations 15140 | Train loss 2.29839\n",
      "Epoch 303/1000, Iterations 15160 | Train loss 2.32050\n",
      "Epoch 303/1000, Iterations 15180 | Train loss 2.26980\n",
      "Epoch 303/1000, Iterations 15200 | Train loss 2.17482\n",
      "Epoch 304/1000, Iterations 15220 | Train loss 2.24564\n",
      "Epoch 304/1000, Iterations 15240 | Train loss 2.27716\n",
      "Epoch 305/1000, Iterations 15260 | Train loss 2.26729\n",
      "Epoch 305/1000, Iterations 15280 | Train loss 2.25865\n",
      "Epoch 305/1000, Iterations 15300 | Train loss 2.17090\n",
      "Epoch 306/1000, Iterations 15320 | Train loss 2.29857\n",
      "Epoch 306/1000, Iterations 15340 | Train loss 2.25443\n",
      "Epoch 307/1000, Iterations 15360 | Train loss 2.25337\n",
      "Epoch 307/1000, Iterations 15380 | Train loss 2.30438\n",
      "Epoch 307/1000, Iterations 15400 | Train loss 2.20592\n",
      "Epoch 308/1000, Iterations 15420 | Train loss 2.28041\n",
      "Epoch 308/1000, Iterations 15440 | Train loss 2.34932\n",
      "Epoch 309/1000, Iterations 15460 | Train loss 2.25600\n",
      "Epoch 309/1000, Iterations 15480 | Train loss 2.26104\n",
      "Epoch 309/1000, Iterations 15500 | Train loss 2.16115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310/1000, Iterations 15520 | Train loss 2.22281\n",
      "Epoch 310/1000, Iterations 15540 | Train loss 2.33023\n",
      "Epoch 311/1000, Iterations 15560 | Train loss 2.26463\n",
      "Epoch 311/1000, Iterations 15580 | Train loss 2.30499\n",
      "Epoch 311/1000, Iterations 15600 | Train loss 2.20687\n",
      "Epoch 312/1000, Iterations 15620 | Train loss 2.22154\n",
      "Epoch 312/1000, Iterations 15640 | Train loss 2.25753\n",
      "Epoch 313/1000, Iterations 15660 | Train loss 2.28118\n",
      "Epoch 313/1000, Iterations 15680 | Train loss 2.28809\n",
      "Epoch 313/1000, Iterations 15700 | Train loss 2.21500\n",
      "Epoch 314/1000, Iterations 15720 | Train loss 2.24355\n",
      "Epoch 314/1000, Iterations 15740 | Train loss 2.25655\n",
      "Epoch 315/1000, Iterations 15760 | Train loss 2.28316\n",
      "Epoch 315/1000, Iterations 15780 | Train loss 2.22742\n",
      "Epoch 315/1000, Iterations 15800 | Train loss 2.21459\n",
      "Epoch 316/1000, Iterations 15820 | Train loss 2.20686\n",
      "Epoch 316/1000, Iterations 15840 | Train loss 2.30404\n",
      "Epoch 317/1000, Iterations 15860 | Train loss 2.26370\n",
      "Epoch 317/1000, Iterations 15880 | Train loss 2.26863\n",
      "Epoch 317/1000, Iterations 15900 | Train loss 2.17563\n",
      "Epoch 318/1000, Iterations 15920 | Train loss 2.27720\n",
      "Epoch 318/1000, Iterations 15940 | Train loss 2.27742\n",
      "Epoch 319/1000, Iterations 15960 | Train loss 2.23660\n",
      "Epoch 319/1000, Iterations 15980 | Train loss 2.26296\n",
      "Epoch 319/1000, Iterations 16000 | Train loss 2.21487\n",
      "Epoch 320/1000, Iterations 16020 | Train loss 2.29224\n",
      "Epoch 320/1000, Iterations 16040 | Train loss 2.26193\n",
      "Epoch 321/1000, Iterations 16060 | Train loss 2.29012\n",
      "Epoch 321/1000, Iterations 16080 | Train loss 2.24000\n",
      "Epoch 321/1000, Iterations 16100 | Train loss 2.15785\n",
      "Epoch 322/1000, Iterations 16120 | Train loss 2.24167\n",
      "Epoch 322/1000, Iterations 16140 | Train loss 2.28484\n",
      "Epoch 323/1000, Iterations 16160 | Train loss 2.27953\n",
      "Epoch 323/1000, Iterations 16180 | Train loss 2.24126\n",
      "Epoch 323/1000, Iterations 16200 | Train loss 2.18010\n",
      "Epoch 324/1000, Iterations 16220 | Train loss 2.24066\n",
      "Epoch 324/1000, Iterations 16240 | Train loss 2.28021\n",
      "Epoch 325/1000, Iterations 16260 | Train loss 2.26572\n",
      "Epoch 325/1000, Iterations 16280 | Train loss 2.29448\n",
      "Epoch 325/1000, Iterations 16300 | Train loss 2.18538\n",
      "Epoch 326/1000, Iterations 16320 | Train loss 2.23793\n",
      "Epoch 326/1000, Iterations 16340 | Train loss 2.28566\n",
      "Epoch 327/1000, Iterations 16360 | Train loss 2.30480\n",
      "Epoch 327/1000, Iterations 16380 | Train loss 2.24514\n",
      "Epoch 327/1000, Iterations 16400 | Train loss 2.17754\n",
      "Epoch 328/1000, Iterations 16420 | Train loss 2.26216\n",
      "Epoch 328/1000, Iterations 16440 | Train loss 2.26346\n",
      "Epoch 329/1000, Iterations 16460 | Train loss 2.27370\n",
      "Epoch 329/1000, Iterations 16480 | Train loss 2.23872\n",
      "Epoch 329/1000, Iterations 16500 | Train loss 2.16174\n",
      "Epoch 330/1000, Iterations 16520 | Train loss 2.24104\n",
      "Epoch 330/1000, Iterations 16540 | Train loss 2.29854\n",
      "Epoch 331/1000, Iterations 16560 | Train loss 2.27581\n",
      "Epoch 331/1000, Iterations 16580 | Train loss 2.27570\n",
      "Epoch 331/1000, Iterations 16600 | Train loss 2.11587\n",
      "Epoch 332/1000, Iterations 16620 | Train loss 2.21318\n",
      "Epoch 332/1000, Iterations 16640 | Train loss 2.27107\n",
      "Epoch 333/1000, Iterations 16660 | Train loss 2.29476\n",
      "Epoch 333/1000, Iterations 16680 | Train loss 2.24644\n",
      "Epoch 333/1000, Iterations 16700 | Train loss 2.16634\n",
      "Epoch 334/1000, Iterations 16720 | Train loss 2.26809\n",
      "Epoch 334/1000, Iterations 16740 | Train loss 2.26379\n",
      "Epoch 335/1000, Iterations 16760 | Train loss 2.29502\n",
      "Epoch 335/1000, Iterations 16780 | Train loss 2.26768\n",
      "Epoch 335/1000, Iterations 16800 | Train loss 2.21758\n",
      "Epoch 336/1000, Iterations 16820 | Train loss 2.24993\n",
      "Epoch 336/1000, Iterations 16840 | Train loss 2.25666\n",
      "Epoch 337/1000, Iterations 16860 | Train loss 2.25243\n",
      "Epoch 337/1000, Iterations 16880 | Train loss 2.29860\n",
      "Epoch 337/1000, Iterations 16900 | Train loss 2.21299\n",
      "Epoch 338/1000, Iterations 16920 | Train loss 2.26783\n",
      "Epoch 338/1000, Iterations 16940 | Train loss 2.28386\n",
      "Epoch 339/1000, Iterations 16960 | Train loss 2.28117\n",
      "Epoch 339/1000, Iterations 16980 | Train loss 2.27554\n",
      "Epoch 339/1000, Iterations 17000 | Train loss 2.17372\n",
      "Epoch 340/1000, Iterations 17020 | Train loss 2.22687\n",
      "Epoch 340/1000, Iterations 17040 | Train loss 2.28061\n",
      "Epoch 341/1000, Iterations 17060 | Train loss 2.28339\n",
      "Epoch 341/1000, Iterations 17080 | Train loss 2.27394\n",
      "Epoch 341/1000, Iterations 17100 | Train loss 2.15325\n",
      "Epoch 342/1000, Iterations 17120 | Train loss 2.26670\n",
      "Epoch 342/1000, Iterations 17140 | Train loss 2.25794\n",
      "Epoch 343/1000, Iterations 17160 | Train loss 2.30163\n",
      "Epoch 343/1000, Iterations 17180 | Train loss 2.27579\n",
      "Epoch 343/1000, Iterations 17200 | Train loss 2.17846\n",
      "Epoch 344/1000, Iterations 17220 | Train loss 2.25552\n",
      "Epoch 344/1000, Iterations 17240 | Train loss 2.30959\n",
      "Epoch 345/1000, Iterations 17260 | Train loss 2.25441\n",
      "Epoch 345/1000, Iterations 17280 | Train loss 2.26490\n",
      "Epoch 345/1000, Iterations 17300 | Train loss 2.19199\n",
      "Epoch 346/1000, Iterations 17320 | Train loss 2.27407\n",
      "Epoch 346/1000, Iterations 17340 | Train loss 2.27476\n",
      "Epoch 347/1000, Iterations 17360 | Train loss 2.23589\n",
      "Epoch 347/1000, Iterations 17380 | Train loss 2.24879\n",
      "Epoch 347/1000, Iterations 17400 | Train loss 2.19626\n",
      "Epoch 348/1000, Iterations 17420 | Train loss 2.24458\n",
      "Epoch 348/1000, Iterations 17440 | Train loss 2.26995\n",
      "Epoch 349/1000, Iterations 17460 | Train loss 2.24084\n",
      "Epoch 349/1000, Iterations 17480 | Train loss 2.30060\n",
      "Epoch 349/1000, Iterations 17500 | Train loss 2.17549\n",
      "Epoch 350/1000, Iterations 17520 | Train loss 2.27170\n",
      "Epoch 350/1000, Iterations 17540 | Train loss 2.28722\n",
      "Epoch 351/1000, Iterations 17560 | Train loss 2.24795\n",
      "Epoch 351/1000, Iterations 17580 | Train loss 2.27071\n",
      "Epoch 351/1000, Iterations 17600 | Train loss 2.16652\n",
      "Epoch 352/1000, Iterations 17620 | Train loss 2.20471\n",
      "Epoch 352/1000, Iterations 17640 | Train loss 2.28320\n",
      "Epoch 353/1000, Iterations 17660 | Train loss 2.22668\n",
      "Epoch 353/1000, Iterations 17680 | Train loss 2.27053\n",
      "Epoch 353/1000, Iterations 17700 | Train loss 2.12454\n",
      "Epoch 354/1000, Iterations 17720 | Train loss 2.20900\n",
      "Epoch 354/1000, Iterations 17740 | Train loss 2.26230\n",
      "Epoch 355/1000, Iterations 17760 | Train loss 2.27542\n",
      "Epoch 355/1000, Iterations 17780 | Train loss 2.28314\n",
      "Epoch 355/1000, Iterations 17800 | Train loss 2.19206\n",
      "Epoch 356/1000, Iterations 17820 | Train loss 2.20633\n",
      "Epoch 356/1000, Iterations 17840 | Train loss 2.29406\n",
      "Epoch 357/1000, Iterations 17860 | Train loss 2.25269\n",
      "Epoch 357/1000, Iterations 17880 | Train loss 2.25193\n",
      "Epoch 357/1000, Iterations 17900 | Train loss 2.19110\n",
      "Epoch 358/1000, Iterations 17920 | Train loss 2.23778\n",
      "Epoch 358/1000, Iterations 17940 | Train loss 2.25047\n",
      "Epoch 359/1000, Iterations 17960 | Train loss 2.23105\n",
      "Epoch 359/1000, Iterations 17980 | Train loss 2.25204\n",
      "Epoch 359/1000, Iterations 18000 | Train loss 2.16308\n",
      "Epoch 360/1000, Iterations 18020 | Train loss 2.22769\n",
      "Epoch 360/1000, Iterations 18040 | Train loss 2.28118\n",
      "Epoch 361/1000, Iterations 18060 | Train loss 2.24741\n",
      "Epoch 361/1000, Iterations 18080 | Train loss 2.24148\n",
      "Epoch 361/1000, Iterations 18100 | Train loss 2.21735\n",
      "Epoch 362/1000, Iterations 18120 | Train loss 2.24612\n",
      "Epoch 362/1000, Iterations 18140 | Train loss 2.27519\n",
      "Epoch 363/1000, Iterations 18160 | Train loss 2.26851\n",
      "Epoch 363/1000, Iterations 18180 | Train loss 2.24682\n",
      "Epoch 363/1000, Iterations 18200 | Train loss 2.17653\n",
      "Epoch 364/1000, Iterations 18220 | Train loss 2.21533\n",
      "Epoch 364/1000, Iterations 18240 | Train loss 2.28082\n",
      "Epoch 365/1000, Iterations 18260 | Train loss 2.21111\n",
      "Epoch 365/1000, Iterations 18280 | Train loss 2.27858\n",
      "Epoch 365/1000, Iterations 18300 | Train loss 2.14619\n",
      "Epoch 366/1000, Iterations 18320 | Train loss 2.17920\n",
      "Epoch 366/1000, Iterations 18340 | Train loss 2.28706\n",
      "Epoch 367/1000, Iterations 18360 | Train loss 2.22184\n",
      "Epoch 367/1000, Iterations 18380 | Train loss 2.23342\n",
      "Epoch 367/1000, Iterations 18400 | Train loss 2.18859\n",
      "Epoch 368/1000, Iterations 18420 | Train loss 2.24307\n",
      "Epoch 368/1000, Iterations 18440 | Train loss 2.27171\n",
      "Epoch 369/1000, Iterations 18460 | Train loss 2.24583\n",
      "Epoch 369/1000, Iterations 18480 | Train loss 2.29263\n",
      "Epoch 369/1000, Iterations 18500 | Train loss 2.18618\n",
      "Epoch 370/1000, Iterations 18520 | Train loss 2.24431\n",
      "Epoch 370/1000, Iterations 18540 | Train loss 2.27041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/1000, Iterations 18560 | Train loss 2.23616\n",
      "Epoch 371/1000, Iterations 18580 | Train loss 2.26575\n",
      "Epoch 371/1000, Iterations 18600 | Train loss 2.18642\n",
      "Epoch 372/1000, Iterations 18620 | Train loss 2.21992\n",
      "Epoch 372/1000, Iterations 18640 | Train loss 2.26663\n",
      "Epoch 373/1000, Iterations 18660 | Train loss 2.24132\n",
      "Epoch 373/1000, Iterations 18680 | Train loss 2.28779\n",
      "Epoch 373/1000, Iterations 18700 | Train loss 2.21448\n",
      "Epoch 374/1000, Iterations 18720 | Train loss 2.22564\n",
      "Epoch 374/1000, Iterations 18740 | Train loss 2.28482\n",
      "Epoch 375/1000, Iterations 18760 | Train loss 2.23359\n",
      "Epoch 375/1000, Iterations 18780 | Train loss 2.25028\n",
      "Epoch 375/1000, Iterations 18800 | Train loss 2.15959\n",
      "Epoch 376/1000, Iterations 18820 | Train loss 2.26458\n",
      "Epoch 376/1000, Iterations 18840 | Train loss 2.26779\n",
      "Epoch 377/1000, Iterations 18860 | Train loss 2.20214\n",
      "Epoch 377/1000, Iterations 18880 | Train loss 2.21905\n",
      "Epoch 377/1000, Iterations 18900 | Train loss 2.12716\n",
      "Epoch 378/1000, Iterations 18920 | Train loss 2.21145\n",
      "Epoch 378/1000, Iterations 18940 | Train loss 2.26335\n",
      "Epoch 379/1000, Iterations 18960 | Train loss 2.28799\n",
      "Epoch 379/1000, Iterations 18980 | Train loss 2.27172\n",
      "Epoch 379/1000, Iterations 19000 | Train loss 2.14260\n",
      "Epoch 380/1000, Iterations 19020 | Train loss 2.20422\n",
      "Epoch 380/1000, Iterations 19040 | Train loss 2.27910\n",
      "Epoch 381/1000, Iterations 19060 | Train loss 2.20374\n",
      "Epoch 381/1000, Iterations 19080 | Train loss 2.28901\n",
      "Epoch 381/1000, Iterations 19100 | Train loss 2.15502\n",
      "Epoch 382/1000, Iterations 19120 | Train loss 2.20918\n",
      "Epoch 382/1000, Iterations 19140 | Train loss 2.27881\n",
      "Epoch 383/1000, Iterations 19160 | Train loss 2.24714\n",
      "Epoch 383/1000, Iterations 19180 | Train loss 2.26532\n",
      "Epoch 383/1000, Iterations 19200 | Train loss 2.15012\n",
      "Epoch 384/1000, Iterations 19220 | Train loss 2.26803\n",
      "Epoch 384/1000, Iterations 19240 | Train loss 2.28876\n",
      "Epoch 385/1000, Iterations 19260 | Train loss 2.21992\n",
      "Epoch 385/1000, Iterations 19280 | Train loss 2.22987\n",
      "Epoch 385/1000, Iterations 19300 | Train loss 2.17176\n",
      "Epoch 386/1000, Iterations 19320 | Train loss 2.24446\n",
      "Epoch 386/1000, Iterations 19340 | Train loss 2.27437\n",
      "Epoch 387/1000, Iterations 19360 | Train loss 2.26171\n",
      "Epoch 387/1000, Iterations 19380 | Train loss 2.27306\n",
      "Epoch 387/1000, Iterations 19400 | Train loss 2.19651\n",
      "Epoch 388/1000, Iterations 19420 | Train loss 2.20434\n",
      "Epoch 388/1000, Iterations 19440 | Train loss 2.28946\n",
      "Epoch 389/1000, Iterations 19460 | Train loss 2.25848\n",
      "Epoch 389/1000, Iterations 19480 | Train loss 2.26332\n",
      "Epoch 389/1000, Iterations 19500 | Train loss 2.17868\n",
      "Epoch 390/1000, Iterations 19520 | Train loss 2.22655\n",
      "Epoch 390/1000, Iterations 19540 | Train loss 2.24961\n",
      "Epoch 391/1000, Iterations 19560 | Train loss 2.24130\n",
      "Epoch 391/1000, Iterations 19580 | Train loss 2.27952\n",
      "Epoch 391/1000, Iterations 19600 | Train loss 2.15137\n",
      "Epoch 392/1000, Iterations 19620 | Train loss 2.22756\n",
      "Epoch 392/1000, Iterations 19640 | Train loss 2.26964\n",
      "Epoch 393/1000, Iterations 19660 | Train loss 2.25367\n",
      "Epoch 393/1000, Iterations 19680 | Train loss 2.22626\n",
      "Epoch 393/1000, Iterations 19700 | Train loss 2.18844\n",
      "Epoch 394/1000, Iterations 19720 | Train loss 2.23263\n",
      "Epoch 394/1000, Iterations 19740 | Train loss 2.27138\n",
      "Epoch 395/1000, Iterations 19760 | Train loss 2.27116\n",
      "Epoch 395/1000, Iterations 19780 | Train loss 2.23498\n",
      "Epoch 395/1000, Iterations 19800 | Train loss 2.11663\n",
      "Epoch 396/1000, Iterations 19820 | Train loss 2.27651\n",
      "Epoch 396/1000, Iterations 19840 | Train loss 2.23032\n",
      "Epoch 397/1000, Iterations 19860 | Train loss 2.22975\n",
      "Epoch 397/1000, Iterations 19880 | Train loss 2.27022\n",
      "Epoch 397/1000, Iterations 19900 | Train loss 2.15799\n",
      "Epoch 398/1000, Iterations 19920 | Train loss 2.23983\n",
      "Epoch 398/1000, Iterations 19940 | Train loss 2.24257\n",
      "Epoch 399/1000, Iterations 19960 | Train loss 2.28274\n",
      "Epoch 399/1000, Iterations 19980 | Train loss 2.25257\n",
      "Epoch 399/1000, Iterations 20000 | Train loss 2.18459\n",
      "Epoch 400/1000, Iterations 20020 | Train loss 2.20382\n",
      "Epoch 400/1000, Iterations 20040 | Train loss 2.25022\n",
      "Epoch 401/1000, Iterations 20060 | Train loss 2.23687\n",
      "Epoch 401/1000, Iterations 20080 | Train loss 2.25549\n",
      "Epoch 401/1000, Iterations 20100 | Train loss 2.11647\n",
      "Epoch 402/1000, Iterations 20120 | Train loss 2.19714\n",
      "Epoch 402/1000, Iterations 20140 | Train loss 2.28636\n",
      "Epoch 403/1000, Iterations 20160 | Train loss 2.28008\n",
      "Epoch 403/1000, Iterations 20180 | Train loss 2.24177\n",
      "Epoch 403/1000, Iterations 20200 | Train loss 2.15927\n",
      "Epoch 404/1000, Iterations 20220 | Train loss 2.22891\n",
      "Epoch 404/1000, Iterations 20240 | Train loss 2.25897\n",
      "Epoch 405/1000, Iterations 20260 | Train loss 2.20299\n",
      "Epoch 405/1000, Iterations 20280 | Train loss 2.25560\n",
      "Epoch 405/1000, Iterations 20300 | Train loss 2.15115\n",
      "Epoch 406/1000, Iterations 20320 | Train loss 2.27643\n",
      "Epoch 406/1000, Iterations 20340 | Train loss 2.26489\n",
      "Epoch 407/1000, Iterations 20360 | Train loss 2.23024\n",
      "Epoch 407/1000, Iterations 20380 | Train loss 2.25392\n",
      "Epoch 407/1000, Iterations 20400 | Train loss 2.16379\n",
      "Epoch 408/1000, Iterations 20420 | Train loss 2.23930\n",
      "Epoch 408/1000, Iterations 20440 | Train loss 2.26650\n",
      "Epoch 409/1000, Iterations 20460 | Train loss 2.24390\n",
      "Epoch 409/1000, Iterations 20480 | Train loss 2.21647\n",
      "Epoch 409/1000, Iterations 20500 | Train loss 2.16373\n",
      "Epoch 410/1000, Iterations 20520 | Train loss 2.20957\n",
      "Epoch 410/1000, Iterations 20540 | Train loss 2.26741\n",
      "Epoch 411/1000, Iterations 20560 | Train loss 2.24343\n",
      "Epoch 411/1000, Iterations 20580 | Train loss 2.26231\n",
      "Epoch 411/1000, Iterations 20600 | Train loss 2.20455\n",
      "Epoch 412/1000, Iterations 20620 | Train loss 2.25472\n",
      "Epoch 412/1000, Iterations 20640 | Train loss 2.25784\n",
      "Epoch 413/1000, Iterations 20660 | Train loss 2.21150\n",
      "Epoch 413/1000, Iterations 20680 | Train loss 2.23117\n",
      "Epoch 413/1000, Iterations 20700 | Train loss 2.15752\n",
      "Epoch 414/1000, Iterations 20720 | Train loss 2.24685\n",
      "Epoch 414/1000, Iterations 20740 | Train loss 2.24656\n",
      "Epoch 415/1000, Iterations 20760 | Train loss 2.24290\n",
      "Epoch 415/1000, Iterations 20780 | Train loss 2.26232\n",
      "Epoch 415/1000, Iterations 20800 | Train loss 2.15191\n",
      "Epoch 416/1000, Iterations 20820 | Train loss 2.20412\n",
      "Epoch 416/1000, Iterations 20840 | Train loss 2.29253\n",
      "Epoch 417/1000, Iterations 20860 | Train loss 2.23564\n",
      "Epoch 417/1000, Iterations 20880 | Train loss 2.23695\n",
      "Epoch 417/1000, Iterations 20900 | Train loss 2.13576\n",
      "Epoch 418/1000, Iterations 20920 | Train loss 2.24118\n",
      "Epoch 418/1000, Iterations 20940 | Train loss 2.29477\n",
      "Epoch 419/1000, Iterations 20960 | Train loss 2.26170\n",
      "Epoch 419/1000, Iterations 20980 | Train loss 2.28280\n",
      "Epoch 419/1000, Iterations 21000 | Train loss 2.13536\n",
      "Epoch 420/1000, Iterations 21020 | Train loss 2.23233\n",
      "Epoch 420/1000, Iterations 21040 | Train loss 2.24648\n",
      "Epoch 421/1000, Iterations 21060 | Train loss 2.22367\n",
      "Epoch 421/1000, Iterations 21080 | Train loss 2.29062\n",
      "Epoch 421/1000, Iterations 21100 | Train loss 2.14797\n",
      "Epoch 422/1000, Iterations 21120 | Train loss 2.24806\n",
      "Epoch 422/1000, Iterations 21140 | Train loss 2.25217\n",
      "Epoch 423/1000, Iterations 21160 | Train loss 2.22792\n",
      "Epoch 423/1000, Iterations 21180 | Train loss 2.24864\n",
      "Epoch 423/1000, Iterations 21200 | Train loss 2.11118\n",
      "Epoch 424/1000, Iterations 21220 | Train loss 2.22627\n",
      "Epoch 424/1000, Iterations 21240 | Train loss 2.25409\n",
      "Epoch 425/1000, Iterations 21260 | Train loss 2.27195\n",
      "Epoch 425/1000, Iterations 21280 | Train loss 2.30823\n",
      "Epoch 425/1000, Iterations 21300 | Train loss 2.16675\n",
      "Epoch 426/1000, Iterations 21320 | Train loss 2.22597\n",
      "Epoch 426/1000, Iterations 21340 | Train loss 2.25088\n",
      "Epoch 427/1000, Iterations 21360 | Train loss 2.25555\n",
      "Epoch 427/1000, Iterations 21380 | Train loss 2.26747\n",
      "Epoch 427/1000, Iterations 21400 | Train loss 2.09411\n",
      "Epoch 428/1000, Iterations 21420 | Train loss 2.22246\n",
      "Epoch 428/1000, Iterations 21440 | Train loss 2.29235\n",
      "Epoch 429/1000, Iterations 21460 | Train loss 2.20892\n",
      "Epoch 429/1000, Iterations 21480 | Train loss 2.26113\n",
      "Epoch 429/1000, Iterations 21500 | Train loss 2.19979\n",
      "Epoch 430/1000, Iterations 21520 | Train loss 2.22214\n",
      "Epoch 430/1000, Iterations 21540 | Train loss 2.21523\n",
      "Epoch 431/1000, Iterations 21560 | Train loss 2.25975\n",
      "Epoch 431/1000, Iterations 21580 | Train loss 2.26290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 431/1000, Iterations 21600 | Train loss 2.15566\n",
      "Epoch 432/1000, Iterations 21620 | Train loss 2.22230\n",
      "Epoch 432/1000, Iterations 21640 | Train loss 2.22653\n",
      "Epoch 433/1000, Iterations 21660 | Train loss 2.22910\n",
      "Epoch 433/1000, Iterations 21680 | Train loss 2.21801\n",
      "Epoch 433/1000, Iterations 21700 | Train loss 2.13366\n",
      "Epoch 434/1000, Iterations 21720 | Train loss 2.23812\n",
      "Epoch 434/1000, Iterations 21740 | Train loss 2.24032\n",
      "Epoch 435/1000, Iterations 21760 | Train loss 2.19920\n",
      "Epoch 435/1000, Iterations 21780 | Train loss 2.23714\n",
      "Epoch 435/1000, Iterations 21800 | Train loss 2.16252\n",
      "Epoch 436/1000, Iterations 21820 | Train loss 2.22217\n",
      "Epoch 436/1000, Iterations 21840 | Train loss 2.26198\n",
      "Epoch 437/1000, Iterations 21860 | Train loss 2.26630\n",
      "Epoch 437/1000, Iterations 21880 | Train loss 2.25272\n",
      "Epoch 437/1000, Iterations 21900 | Train loss 2.11409\n",
      "Epoch 438/1000, Iterations 21920 | Train loss 2.19993\n",
      "Epoch 438/1000, Iterations 21940 | Train loss 2.29615\n",
      "Epoch 439/1000, Iterations 21960 | Train loss 2.26592\n",
      "Epoch 439/1000, Iterations 21980 | Train loss 2.25018\n",
      "Epoch 439/1000, Iterations 22000 | Train loss 2.12912\n",
      "Epoch 440/1000, Iterations 22020 | Train loss 2.20694\n",
      "Epoch 440/1000, Iterations 22040 | Train loss 2.20030\n",
      "Epoch 441/1000, Iterations 22060 | Train loss 2.25625\n",
      "Epoch 441/1000, Iterations 22080 | Train loss 2.23728\n",
      "Epoch 441/1000, Iterations 22100 | Train loss 2.14934\n",
      "Epoch 442/1000, Iterations 22120 | Train loss 2.19566\n",
      "Epoch 442/1000, Iterations 22140 | Train loss 2.22687\n",
      "Epoch 443/1000, Iterations 22160 | Train loss 2.23915\n",
      "Epoch 443/1000, Iterations 22180 | Train loss 2.23221\n",
      "Epoch 443/1000, Iterations 22200 | Train loss 2.14510\n",
      "Epoch 444/1000, Iterations 22220 | Train loss 2.21831\n",
      "Epoch 444/1000, Iterations 22240 | Train loss 2.26287\n",
      "Epoch 445/1000, Iterations 22260 | Train loss 2.24381\n",
      "Epoch 445/1000, Iterations 22280 | Train loss 2.26011\n",
      "Epoch 445/1000, Iterations 22300 | Train loss 2.18004\n",
      "Epoch 446/1000, Iterations 22320 | Train loss 2.19053\n",
      "Epoch 446/1000, Iterations 22340 | Train loss 2.25965\n",
      "Epoch 447/1000, Iterations 22360 | Train loss 2.21871\n",
      "Epoch 447/1000, Iterations 22380 | Train loss 2.27669\n",
      "Epoch 447/1000, Iterations 22400 | Train loss 2.10227\n",
      "Epoch 448/1000, Iterations 22420 | Train loss 2.14013\n",
      "Epoch 448/1000, Iterations 22440 | Train loss 2.27190\n",
      "Epoch 449/1000, Iterations 22460 | Train loss 2.21824\n",
      "Epoch 449/1000, Iterations 22480 | Train loss 2.35252\n",
      "Epoch 449/1000, Iterations 22500 | Train loss 2.12914\n",
      "Epoch 450/1000, Iterations 22520 | Train loss 2.20991\n",
      "Epoch 450/1000, Iterations 22540 | Train loss 2.26915\n",
      "Epoch 451/1000, Iterations 22560 | Train loss 2.25033\n",
      "Epoch 451/1000, Iterations 22580 | Train loss 2.20955\n",
      "Epoch 451/1000, Iterations 22600 | Train loss 2.16978\n",
      "Epoch 452/1000, Iterations 22620 | Train loss 2.21805\n",
      "Epoch 452/1000, Iterations 22640 | Train loss 2.25636\n",
      "Epoch 453/1000, Iterations 22660 | Train loss 2.22487\n",
      "Epoch 453/1000, Iterations 22680 | Train loss 2.25609\n",
      "Epoch 453/1000, Iterations 22700 | Train loss 2.16182\n",
      "Epoch 454/1000, Iterations 22720 | Train loss 2.20721\n",
      "Epoch 454/1000, Iterations 22740 | Train loss 2.26275\n",
      "Epoch 455/1000, Iterations 22760 | Train loss 2.17902\n",
      "Epoch 455/1000, Iterations 22780 | Train loss 2.23224\n",
      "Epoch 455/1000, Iterations 22800 | Train loss 2.12356\n",
      "Epoch 456/1000, Iterations 22820 | Train loss 2.22409\n",
      "Epoch 456/1000, Iterations 22840 | Train loss 2.28331\n",
      "Epoch 457/1000, Iterations 22860 | Train loss 2.17509\n",
      "Epoch 457/1000, Iterations 22880 | Train loss 2.26916\n",
      "Epoch 457/1000, Iterations 22900 | Train loss 2.11010\n",
      "Epoch 458/1000, Iterations 22920 | Train loss 2.19096\n",
      "Epoch 458/1000, Iterations 22940 | Train loss 2.26237\n",
      "Epoch 459/1000, Iterations 22960 | Train loss 2.21748\n",
      "Epoch 459/1000, Iterations 22980 | Train loss 2.24530\n",
      "Epoch 459/1000, Iterations 23000 | Train loss 2.11060\n",
      "Epoch 460/1000, Iterations 23020 | Train loss 2.21747\n",
      "Epoch 460/1000, Iterations 23040 | Train loss 2.21427\n",
      "Epoch 461/1000, Iterations 23060 | Train loss 2.18501\n",
      "Epoch 461/1000, Iterations 23080 | Train loss 2.21521\n",
      "Epoch 461/1000, Iterations 23100 | Train loss 2.13403\n",
      "Epoch 462/1000, Iterations 23120 | Train loss 2.16359\n",
      "Epoch 462/1000, Iterations 23140 | Train loss 2.24399\n",
      "Epoch 463/1000, Iterations 23160 | Train loss 2.18228\n",
      "Epoch 463/1000, Iterations 23180 | Train loss 2.22423\n",
      "Epoch 463/1000, Iterations 23200 | Train loss 2.07945\n",
      "Epoch 464/1000, Iterations 23220 | Train loss 2.19486\n",
      "Epoch 464/1000, Iterations 23240 | Train loss 2.26650\n",
      "Epoch 465/1000, Iterations 23260 | Train loss 2.24774\n",
      "Epoch 465/1000, Iterations 23280 | Train loss 2.28003\n",
      "Epoch 465/1000, Iterations 23300 | Train loss 2.10676\n",
      "Epoch 466/1000, Iterations 23320 | Train loss 2.23467\n",
      "Epoch 466/1000, Iterations 23340 | Train loss 2.23806\n",
      "Epoch 467/1000, Iterations 23360 | Train loss 2.24418\n",
      "Epoch 467/1000, Iterations 23380 | Train loss 2.25593\n",
      "Epoch 467/1000, Iterations 23400 | Train loss 2.15700\n",
      "Epoch 468/1000, Iterations 23420 | Train loss 2.22194\n",
      "Epoch 468/1000, Iterations 23440 | Train loss 2.23188\n",
      "Epoch 469/1000, Iterations 23460 | Train loss 2.22446\n",
      "Epoch 469/1000, Iterations 23480 | Train loss 2.24997\n",
      "Epoch 469/1000, Iterations 23500 | Train loss 2.13089\n",
      "Epoch 470/1000, Iterations 23520 | Train loss 2.20842\n",
      "Epoch 470/1000, Iterations 23540 | Train loss 2.25620\n",
      "Epoch 471/1000, Iterations 23560 | Train loss 2.20808\n",
      "Epoch 471/1000, Iterations 23580 | Train loss 2.24120\n",
      "Epoch 471/1000, Iterations 23600 | Train loss 2.18077\n",
      "Epoch 472/1000, Iterations 23620 | Train loss 2.24473\n",
      "Epoch 472/1000, Iterations 23640 | Train loss 2.27255\n",
      "Epoch 473/1000, Iterations 23660 | Train loss 2.25337\n",
      "Epoch 473/1000, Iterations 23680 | Train loss 2.21896\n",
      "Epoch 473/1000, Iterations 23700 | Train loss 2.14402\n",
      "Epoch 474/1000, Iterations 23720 | Train loss 2.20638\n",
      "Epoch 474/1000, Iterations 23740 | Train loss 2.26240\n",
      "Epoch 475/1000, Iterations 23760 | Train loss 2.25199\n",
      "Epoch 475/1000, Iterations 23780 | Train loss 2.27930\n",
      "Epoch 475/1000, Iterations 23800 | Train loss 2.13198\n",
      "Epoch 476/1000, Iterations 23820 | Train loss 2.17731\n",
      "Epoch 476/1000, Iterations 23840 | Train loss 2.19975\n",
      "Epoch 477/1000, Iterations 23860 | Train loss 2.23852\n",
      "Epoch 477/1000, Iterations 23880 | Train loss 2.23885\n",
      "Epoch 477/1000, Iterations 23900 | Train loss 2.15023\n",
      "Epoch 478/1000, Iterations 23920 | Train loss 2.22766\n",
      "Epoch 478/1000, Iterations 23940 | Train loss 2.28568\n",
      "Epoch 479/1000, Iterations 23960 | Train loss 2.20614\n",
      "Epoch 479/1000, Iterations 23980 | Train loss 2.22682\n",
      "Epoch 479/1000, Iterations 24000 | Train loss 2.17236\n",
      "Epoch 480/1000, Iterations 24020 | Train loss 2.17376\n",
      "Epoch 480/1000, Iterations 24040 | Train loss 2.21947\n",
      "Epoch 481/1000, Iterations 24060 | Train loss 2.20391\n",
      "Epoch 481/1000, Iterations 24080 | Train loss 2.22654\n",
      "Epoch 481/1000, Iterations 24100 | Train loss 2.15092\n",
      "Epoch 482/1000, Iterations 24120 | Train loss 2.22470\n",
      "Epoch 482/1000, Iterations 24140 | Train loss 2.23029\n",
      "Epoch 483/1000, Iterations 24160 | Train loss 2.18113\n",
      "Epoch 483/1000, Iterations 24180 | Train loss 2.21110\n",
      "Epoch 483/1000, Iterations 24200 | Train loss 2.17505\n",
      "Epoch 484/1000, Iterations 24220 | Train loss 2.19494\n",
      "Epoch 484/1000, Iterations 24240 | Train loss 2.21661\n",
      "Epoch 485/1000, Iterations 24260 | Train loss 2.19723\n",
      "Epoch 485/1000, Iterations 24280 | Train loss 2.24989\n",
      "Epoch 485/1000, Iterations 24300 | Train loss 2.13414\n",
      "Epoch 486/1000, Iterations 24320 | Train loss 2.22331\n",
      "Epoch 486/1000, Iterations 24340 | Train loss 2.20350\n",
      "Epoch 487/1000, Iterations 24360 | Train loss 2.20894\n",
      "Epoch 487/1000, Iterations 24380 | Train loss 2.23211\n",
      "Epoch 487/1000, Iterations 24400 | Train loss 2.13318\n",
      "Epoch 488/1000, Iterations 24420 | Train loss 2.18743\n",
      "Epoch 488/1000, Iterations 24440 | Train loss 2.23189\n",
      "Epoch 489/1000, Iterations 24460 | Train loss 2.23980\n",
      "Epoch 489/1000, Iterations 24480 | Train loss 2.23396\n",
      "Epoch 489/1000, Iterations 24500 | Train loss 2.10099\n",
      "Epoch 490/1000, Iterations 24520 | Train loss 2.21937\n",
      "Epoch 490/1000, Iterations 24540 | Train loss 2.22161\n",
      "Epoch 491/1000, Iterations 24560 | Train loss 2.19512\n",
      "Epoch 491/1000, Iterations 24580 | Train loss 2.22987\n",
      "Epoch 491/1000, Iterations 24600 | Train loss 2.12228\n",
      "Epoch 492/1000, Iterations 24620 | Train loss 2.17788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 492/1000, Iterations 24640 | Train loss 2.24561\n",
      "Epoch 493/1000, Iterations 24660 | Train loss 2.21780\n",
      "Epoch 493/1000, Iterations 24680 | Train loss 2.21458\n",
      "Epoch 493/1000, Iterations 24700 | Train loss 2.12147\n",
      "Epoch 494/1000, Iterations 24720 | Train loss 2.19730\n",
      "Epoch 494/1000, Iterations 24740 | Train loss 2.23847\n",
      "Epoch 495/1000, Iterations 24760 | Train loss 2.24646\n",
      "Epoch 495/1000, Iterations 24780 | Train loss 2.20380\n",
      "Epoch 495/1000, Iterations 24800 | Train loss 2.15014\n",
      "Epoch 496/1000, Iterations 24820 | Train loss 2.23604\n",
      "Epoch 496/1000, Iterations 24840 | Train loss 2.19748\n",
      "Epoch 497/1000, Iterations 24860 | Train loss 2.20303\n",
      "Epoch 497/1000, Iterations 24880 | Train loss 2.25139\n",
      "Epoch 497/1000, Iterations 24900 | Train loss 2.17434\n",
      "Epoch 498/1000, Iterations 24920 | Train loss 2.19522\n",
      "Epoch 498/1000, Iterations 24940 | Train loss 2.21410\n",
      "Epoch 499/1000, Iterations 24960 | Train loss 2.20636\n",
      "Epoch 499/1000, Iterations 24980 | Train loss 2.26783\n",
      "Epoch 499/1000, Iterations 25000 | Train loss 2.16306\n",
      "Epoch 500/1000, Iterations 25020 | Train loss 2.21475\n",
      "Epoch 500/1000, Iterations 25040 | Train loss 2.23805\n",
      "Epoch 501/1000, Iterations 25060 | Train loss 2.28807\n",
      "Epoch 501/1000, Iterations 25080 | Train loss 2.27782\n",
      "Epoch 501/1000, Iterations 25100 | Train loss 2.17494\n",
      "Epoch 502/1000, Iterations 25120 | Train loss 2.19169\n",
      "Epoch 502/1000, Iterations 25140 | Train loss 2.23174\n",
      "Epoch 503/1000, Iterations 25160 | Train loss 2.22034\n",
      "Epoch 503/1000, Iterations 25180 | Train loss 2.24428\n",
      "Epoch 503/1000, Iterations 25200 | Train loss 2.14064\n",
      "Epoch 504/1000, Iterations 25220 | Train loss 2.21610\n",
      "Epoch 504/1000, Iterations 25240 | Train loss 2.22629\n",
      "Epoch 505/1000, Iterations 25260 | Train loss 2.17399\n",
      "Epoch 505/1000, Iterations 25280 | Train loss 2.20537\n",
      "Epoch 505/1000, Iterations 25300 | Train loss 2.13411\n",
      "Epoch 506/1000, Iterations 25320 | Train loss 2.19562\n",
      "Epoch 506/1000, Iterations 25340 | Train loss 2.23267\n",
      "Epoch 507/1000, Iterations 25360 | Train loss 2.22165\n",
      "Epoch 507/1000, Iterations 25380 | Train loss 2.23804\n",
      "Epoch 507/1000, Iterations 25400 | Train loss 2.13642\n",
      "Epoch 508/1000, Iterations 25420 | Train loss 2.20557\n",
      "Epoch 508/1000, Iterations 25440 | Train loss 2.25750\n",
      "Epoch 509/1000, Iterations 25460 | Train loss 2.19963\n",
      "Epoch 509/1000, Iterations 25480 | Train loss 2.24179\n",
      "Epoch 509/1000, Iterations 25500 | Train loss 2.12254\n",
      "Epoch 510/1000, Iterations 25520 | Train loss 2.21719\n",
      "Epoch 510/1000, Iterations 25540 | Train loss 2.24145\n",
      "Epoch 511/1000, Iterations 25560 | Train loss 2.23384\n",
      "Epoch 511/1000, Iterations 25580 | Train loss 2.25489\n",
      "Epoch 511/1000, Iterations 25600 | Train loss 2.15519\n",
      "Epoch 512/1000, Iterations 25620 | Train loss 2.19499\n",
      "Epoch 512/1000, Iterations 25640 | Train loss 2.27168\n",
      "Epoch 513/1000, Iterations 25660 | Train loss 2.24794\n",
      "Epoch 513/1000, Iterations 25680 | Train loss 2.22084\n",
      "Epoch 513/1000, Iterations 25700 | Train loss 2.14667\n",
      "Epoch 514/1000, Iterations 25720 | Train loss 2.22424\n",
      "Epoch 514/1000, Iterations 25740 | Train loss 2.21716\n",
      "Epoch 515/1000, Iterations 25760 | Train loss 2.22609\n",
      "Epoch 515/1000, Iterations 25780 | Train loss 2.22746\n",
      "Epoch 515/1000, Iterations 25800 | Train loss 2.13044\n",
      "Epoch 516/1000, Iterations 25820 | Train loss 2.18047\n",
      "Epoch 516/1000, Iterations 25840 | Train loss 2.21917\n",
      "Epoch 517/1000, Iterations 25860 | Train loss 2.18953\n",
      "Epoch 517/1000, Iterations 25880 | Train loss 2.25787\n",
      "Epoch 517/1000, Iterations 25900 | Train loss 2.12279\n",
      "Epoch 518/1000, Iterations 25920 | Train loss 2.21714\n",
      "Epoch 518/1000, Iterations 25940 | Train loss 2.23519\n",
      "Epoch 519/1000, Iterations 25960 | Train loss 2.24177\n",
      "Epoch 519/1000, Iterations 25980 | Train loss 2.21601\n",
      "Epoch 519/1000, Iterations 26000 | Train loss 2.11226\n",
      "Epoch 520/1000, Iterations 26020 | Train loss 2.16075\n",
      "Epoch 520/1000, Iterations 26040 | Train loss 2.22161\n",
      "Epoch 521/1000, Iterations 26060 | Train loss 2.15869\n",
      "Epoch 521/1000, Iterations 26080 | Train loss 2.20421\n",
      "Epoch 521/1000, Iterations 26100 | Train loss 2.13762\n",
      "Epoch 522/1000, Iterations 26120 | Train loss 2.18236\n",
      "Epoch 522/1000, Iterations 26140 | Train loss 2.24427\n",
      "Epoch 523/1000, Iterations 26160 | Train loss 2.18152\n",
      "Epoch 523/1000, Iterations 26180 | Train loss 2.25237\n",
      "Epoch 523/1000, Iterations 26200 | Train loss 2.12996\n",
      "Epoch 524/1000, Iterations 26220 | Train loss 2.24098\n",
      "Epoch 524/1000, Iterations 26240 | Train loss 2.26357\n",
      "Epoch 525/1000, Iterations 26260 | Train loss 2.24966\n",
      "Epoch 525/1000, Iterations 26280 | Train loss 2.21313\n",
      "Epoch 525/1000, Iterations 26300 | Train loss 2.15175\n",
      "Epoch 526/1000, Iterations 26320 | Train loss 2.20517\n",
      "Epoch 526/1000, Iterations 26340 | Train loss 2.21716\n",
      "Epoch 527/1000, Iterations 26360 | Train loss 2.23998\n",
      "Epoch 527/1000, Iterations 26380 | Train loss 2.18135\n",
      "Epoch 527/1000, Iterations 26400 | Train loss 2.14624\n",
      "Epoch 528/1000, Iterations 26420 | Train loss 2.19414\n",
      "Epoch 528/1000, Iterations 26440 | Train loss 2.21314\n",
      "Epoch 529/1000, Iterations 26460 | Train loss 2.15938\n",
      "Epoch 529/1000, Iterations 26480 | Train loss 2.19241\n",
      "Epoch 529/1000, Iterations 26500 | Train loss 2.14128\n",
      "Epoch 530/1000, Iterations 26520 | Train loss 2.16897\n",
      "Epoch 530/1000, Iterations 26540 | Train loss 2.24537\n",
      "Epoch 531/1000, Iterations 26560 | Train loss 2.26471\n",
      "Epoch 531/1000, Iterations 26580 | Train loss 2.21146\n",
      "Epoch 531/1000, Iterations 26600 | Train loss 2.09120\n",
      "Epoch 532/1000, Iterations 26620 | Train loss 2.14833\n",
      "Epoch 532/1000, Iterations 26640 | Train loss 2.24461\n",
      "Epoch 533/1000, Iterations 26660 | Train loss 2.18373\n",
      "Epoch 533/1000, Iterations 26680 | Train loss 2.21891\n",
      "Epoch 533/1000, Iterations 26700 | Train loss 2.14265\n",
      "Epoch 534/1000, Iterations 26720 | Train loss 2.19345\n",
      "Epoch 534/1000, Iterations 26740 | Train loss 2.22415\n",
      "Epoch 535/1000, Iterations 26760 | Train loss 2.28738\n",
      "Epoch 535/1000, Iterations 26780 | Train loss 2.25072\n",
      "Epoch 535/1000, Iterations 26800 | Train loss 2.11772\n",
      "Epoch 536/1000, Iterations 26820 | Train loss 2.21222\n",
      "Epoch 536/1000, Iterations 26840 | Train loss 2.26230\n",
      "Epoch 537/1000, Iterations 26860 | Train loss 2.24733\n",
      "Epoch 537/1000, Iterations 26880 | Train loss 2.23940\n",
      "Epoch 537/1000, Iterations 26900 | Train loss 2.11387\n",
      "Epoch 538/1000, Iterations 26920 | Train loss 2.17132\n",
      "Epoch 538/1000, Iterations 26940 | Train loss 2.22346\n",
      "Epoch 539/1000, Iterations 26960 | Train loss 2.18998\n",
      "Epoch 539/1000, Iterations 26980 | Train loss 2.18633\n",
      "Epoch 539/1000, Iterations 27000 | Train loss 2.13543\n",
      "Epoch 540/1000, Iterations 27020 | Train loss 2.18681\n",
      "Epoch 540/1000, Iterations 27040 | Train loss 2.24060\n",
      "Epoch 541/1000, Iterations 27060 | Train loss 2.21357\n",
      "Epoch 541/1000, Iterations 27080 | Train loss 2.20897\n",
      "Epoch 541/1000, Iterations 27100 | Train loss 2.12159\n",
      "Epoch 542/1000, Iterations 27120 | Train loss 2.21397\n",
      "Epoch 542/1000, Iterations 27140 | Train loss 2.20318\n",
      "Epoch 543/1000, Iterations 27160 | Train loss 2.22116\n",
      "Epoch 543/1000, Iterations 27180 | Train loss 2.18916\n",
      "Epoch 543/1000, Iterations 27200 | Train loss 2.09690\n",
      "Epoch 544/1000, Iterations 27220 | Train loss 2.19929\n",
      "Epoch 544/1000, Iterations 27240 | Train loss 2.26298\n",
      "Epoch 545/1000, Iterations 27260 | Train loss 2.21530\n",
      "Epoch 545/1000, Iterations 27280 | Train loss 2.23307\n",
      "Epoch 545/1000, Iterations 27300 | Train loss 2.10978\n",
      "Epoch 546/1000, Iterations 27320 | Train loss 2.17322\n",
      "Epoch 546/1000, Iterations 27340 | Train loss 2.20670\n",
      "Epoch 547/1000, Iterations 27360 | Train loss 2.21135\n",
      "Epoch 547/1000, Iterations 27380 | Train loss 2.24052\n",
      "Epoch 547/1000, Iterations 27400 | Train loss 2.10781\n",
      "Epoch 548/1000, Iterations 27420 | Train loss 2.19635\n",
      "Epoch 548/1000, Iterations 27440 | Train loss 2.22705\n",
      "Epoch 549/1000, Iterations 27460 | Train loss 2.19048\n",
      "Epoch 549/1000, Iterations 27480 | Train loss 2.21574\n",
      "Epoch 549/1000, Iterations 27500 | Train loss 2.09765\n",
      "Epoch 550/1000, Iterations 27520 | Train loss 2.20289\n",
      "Epoch 550/1000, Iterations 27540 | Train loss 2.21706\n",
      "Epoch 551/1000, Iterations 27560 | Train loss 2.14613\n",
      "Epoch 551/1000, Iterations 27580 | Train loss 2.23579\n",
      "Epoch 551/1000, Iterations 27600 | Train loss 2.15749\n",
      "Epoch 552/1000, Iterations 27620 | Train loss 2.15131\n",
      "Epoch 552/1000, Iterations 27640 | Train loss 2.19477\n",
      "Epoch 553/1000, Iterations 27660 | Train loss 2.20047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 553/1000, Iterations 27680 | Train loss 2.21994\n",
      "Epoch 553/1000, Iterations 27700 | Train loss 2.12995\n",
      "Epoch 554/1000, Iterations 27720 | Train loss 2.18193\n",
      "Epoch 554/1000, Iterations 27740 | Train loss 2.23530\n",
      "Epoch 555/1000, Iterations 27760 | Train loss 2.20088\n",
      "Epoch 555/1000, Iterations 27780 | Train loss 2.19887\n",
      "Epoch 555/1000, Iterations 27800 | Train loss 2.08946\n",
      "Epoch 556/1000, Iterations 27820 | Train loss 2.19746\n",
      "Epoch 556/1000, Iterations 27840 | Train loss 2.19035\n",
      "Epoch 557/1000, Iterations 27860 | Train loss 2.22466\n",
      "Epoch 557/1000, Iterations 27880 | Train loss 2.22987\n",
      "Epoch 557/1000, Iterations 27900 | Train loss 2.10997\n",
      "Epoch 558/1000, Iterations 27920 | Train loss 2.21528\n",
      "Epoch 558/1000, Iterations 27940 | Train loss 2.27441\n",
      "Epoch 559/1000, Iterations 27960 | Train loss 2.27688\n",
      "Epoch 559/1000, Iterations 27980 | Train loss 2.18435\n",
      "Epoch 559/1000, Iterations 28000 | Train loss 2.13166\n",
      "Epoch 560/1000, Iterations 28020 | Train loss 2.26878\n",
      "Epoch 560/1000, Iterations 28040 | Train loss 2.22703\n",
      "Epoch 561/1000, Iterations 28060 | Train loss 2.19858\n",
      "Epoch 561/1000, Iterations 28080 | Train loss 2.22820\n",
      "Epoch 561/1000, Iterations 28100 | Train loss 2.13170\n",
      "Epoch 562/1000, Iterations 28120 | Train loss 2.18749\n",
      "Epoch 562/1000, Iterations 28140 | Train loss 2.23848\n",
      "Epoch 563/1000, Iterations 28160 | Train loss 2.23764\n",
      "Epoch 563/1000, Iterations 28180 | Train loss 2.28005\n",
      "Epoch 563/1000, Iterations 28200 | Train loss 2.11678\n",
      "Epoch 564/1000, Iterations 28220 | Train loss 2.16975\n",
      "Epoch 564/1000, Iterations 28240 | Train loss 2.20602\n",
      "Epoch 565/1000, Iterations 28260 | Train loss 2.14991\n",
      "Epoch 565/1000, Iterations 28280 | Train loss 2.21497\n",
      "Epoch 565/1000, Iterations 28300 | Train loss 2.10191\n",
      "Epoch 566/1000, Iterations 28320 | Train loss 2.17440\n",
      "Epoch 566/1000, Iterations 28340 | Train loss 2.24225\n",
      "Epoch 567/1000, Iterations 28360 | Train loss 2.23895\n",
      "Epoch 567/1000, Iterations 28380 | Train loss 2.22727\n",
      "Epoch 567/1000, Iterations 28400 | Train loss 2.11663\n",
      "Epoch 568/1000, Iterations 28420 | Train loss 2.18552\n",
      "Epoch 568/1000, Iterations 28440 | Train loss 2.22740\n",
      "Epoch 569/1000, Iterations 28460 | Train loss 2.20908\n",
      "Epoch 569/1000, Iterations 28480 | Train loss 2.19861\n",
      "Epoch 569/1000, Iterations 28500 | Train loss 2.10250\n",
      "Epoch 570/1000, Iterations 28520 | Train loss 2.17533\n",
      "Epoch 570/1000, Iterations 28540 | Train loss 2.25892\n",
      "Epoch 571/1000, Iterations 28560 | Train loss 2.26940\n",
      "Epoch 571/1000, Iterations 28580 | Train loss 2.23082\n",
      "Epoch 571/1000, Iterations 28600 | Train loss 2.08969\n",
      "Epoch 572/1000, Iterations 28620 | Train loss 2.15782\n",
      "Epoch 572/1000, Iterations 28640 | Train loss 2.24543\n",
      "Epoch 573/1000, Iterations 28660 | Train loss 2.22022\n",
      "Epoch 573/1000, Iterations 28680 | Train loss 2.21507\n",
      "Epoch 573/1000, Iterations 28700 | Train loss 2.11639\n",
      "Epoch 574/1000, Iterations 28720 | Train loss 2.19186\n",
      "Epoch 574/1000, Iterations 28740 | Train loss 2.22763\n",
      "Epoch 575/1000, Iterations 28760 | Train loss 2.22017\n",
      "Epoch 575/1000, Iterations 28780 | Train loss 2.21552\n",
      "Epoch 575/1000, Iterations 28800 | Train loss 2.09885\n",
      "Epoch 576/1000, Iterations 28820 | Train loss 2.22937\n",
      "Epoch 576/1000, Iterations 28840 | Train loss 2.20263\n",
      "Epoch 577/1000, Iterations 28860 | Train loss 2.22377\n",
      "Epoch 577/1000, Iterations 28880 | Train loss 2.27997\n",
      "Epoch 577/1000, Iterations 28900 | Train loss 2.10024\n",
      "Epoch 578/1000, Iterations 28920 | Train loss 2.17412\n",
      "Epoch 578/1000, Iterations 28940 | Train loss 2.23299\n",
      "Epoch 579/1000, Iterations 28960 | Train loss 2.16695\n",
      "Epoch 579/1000, Iterations 28980 | Train loss 2.20474\n",
      "Epoch 579/1000, Iterations 29000 | Train loss 2.12361\n",
      "Epoch 580/1000, Iterations 29020 | Train loss 2.20000\n",
      "Epoch 580/1000, Iterations 29040 | Train loss 2.26266\n",
      "Epoch 581/1000, Iterations 29060 | Train loss 2.22461\n",
      "Epoch 581/1000, Iterations 29080 | Train loss 2.22291\n",
      "Epoch 581/1000, Iterations 29100 | Train loss 2.07997\n",
      "Epoch 582/1000, Iterations 29120 | Train loss 2.16113\n",
      "Epoch 582/1000, Iterations 29140 | Train loss 2.24715\n",
      "Epoch 583/1000, Iterations 29160 | Train loss 2.26073\n",
      "Epoch 583/1000, Iterations 29180 | Train loss 2.22436\n",
      "Epoch 583/1000, Iterations 29200 | Train loss 2.12514\n",
      "Epoch 584/1000, Iterations 29220 | Train loss 2.18700\n",
      "Epoch 584/1000, Iterations 29240 | Train loss 2.17418\n",
      "Epoch 585/1000, Iterations 29260 | Train loss 2.22020\n",
      "Epoch 585/1000, Iterations 29280 | Train loss 2.21973\n",
      "Epoch 585/1000, Iterations 29300 | Train loss 2.15405\n",
      "Epoch 586/1000, Iterations 29320 | Train loss 2.18083\n",
      "Epoch 586/1000, Iterations 29340 | Train loss 2.24526\n",
      "Epoch 587/1000, Iterations 29360 | Train loss 2.21811\n",
      "Epoch 587/1000, Iterations 29380 | Train loss 2.20165\n",
      "Epoch 587/1000, Iterations 29400 | Train loss 2.13176\n",
      "Epoch 588/1000, Iterations 29420 | Train loss 2.18734\n",
      "Epoch 588/1000, Iterations 29440 | Train loss 2.26732\n",
      "Epoch 589/1000, Iterations 29460 | Train loss 2.17993\n",
      "Epoch 589/1000, Iterations 29480 | Train loss 2.21001\n",
      "Epoch 589/1000, Iterations 29500 | Train loss 2.11255\n",
      "Epoch 590/1000, Iterations 29520 | Train loss 2.17982\n",
      "Epoch 590/1000, Iterations 29540 | Train loss 2.23040\n",
      "Epoch 591/1000, Iterations 29560 | Train loss 2.16280\n",
      "Epoch 591/1000, Iterations 29580 | Train loss 2.21864\n",
      "Epoch 591/1000, Iterations 29600 | Train loss 2.11745\n",
      "Epoch 592/1000, Iterations 29620 | Train loss 2.20471\n",
      "Epoch 592/1000, Iterations 29640 | Train loss 2.23595\n",
      "Epoch 593/1000, Iterations 29660 | Train loss 2.16679\n",
      "Epoch 593/1000, Iterations 29680 | Train loss 2.21512\n",
      "Epoch 593/1000, Iterations 29700 | Train loss 2.08793\n",
      "Epoch 594/1000, Iterations 29720 | Train loss 2.19964\n",
      "Epoch 594/1000, Iterations 29740 | Train loss 2.24018\n",
      "Epoch 595/1000, Iterations 29760 | Train loss 2.24893\n",
      "Epoch 595/1000, Iterations 29780 | Train loss 2.23676\n",
      "Epoch 595/1000, Iterations 29800 | Train loss 2.09539\n",
      "Epoch 596/1000, Iterations 29820 | Train loss 2.15976\n",
      "Epoch 596/1000, Iterations 29840 | Train loss 2.19873\n",
      "Epoch 597/1000, Iterations 29860 | Train loss 2.24915\n",
      "Epoch 597/1000, Iterations 29880 | Train loss 2.19161\n",
      "Epoch 597/1000, Iterations 29900 | Train loss 2.11247\n",
      "Epoch 598/1000, Iterations 29920 | Train loss 2.18648\n",
      "Epoch 598/1000, Iterations 29940 | Train loss 2.22980\n",
      "Epoch 599/1000, Iterations 29960 | Train loss 2.21311\n",
      "Epoch 599/1000, Iterations 29980 | Train loss 2.23555\n",
      "Epoch 599/1000, Iterations 30000 | Train loss 2.16780\n",
      "Epoch 600/1000, Iterations 30020 | Train loss 2.22308\n",
      "Epoch 600/1000, Iterations 30040 | Train loss 2.25471\n",
      "Epoch 601/1000, Iterations 30060 | Train loss 2.19929\n",
      "Epoch 601/1000, Iterations 30080 | Train loss 2.17950\n",
      "Epoch 601/1000, Iterations 30100 | Train loss 2.09083\n",
      "Epoch 602/1000, Iterations 30120 | Train loss 2.20872\n",
      "Epoch 602/1000, Iterations 30140 | Train loss 2.24518\n",
      "Epoch 603/1000, Iterations 30160 | Train loss 2.17336\n",
      "Epoch 603/1000, Iterations 30180 | Train loss 2.21044\n",
      "Epoch 603/1000, Iterations 30200 | Train loss 2.10932\n",
      "Epoch 604/1000, Iterations 30220 | Train loss 2.21068\n",
      "Epoch 604/1000, Iterations 30240 | Train loss 2.20628\n",
      "Epoch 605/1000, Iterations 30260 | Train loss 2.18396\n",
      "Epoch 605/1000, Iterations 30280 | Train loss 2.25224\n",
      "Epoch 605/1000, Iterations 30300 | Train loss 2.15768\n",
      "Epoch 606/1000, Iterations 30320 | Train loss 2.19028\n",
      "Epoch 606/1000, Iterations 30340 | Train loss 2.22794\n",
      "Epoch 607/1000, Iterations 30360 | Train loss 2.20115\n",
      "Epoch 607/1000, Iterations 30380 | Train loss 2.22106\n",
      "Epoch 607/1000, Iterations 30400 | Train loss 2.11776\n",
      "Epoch 608/1000, Iterations 30420 | Train loss 2.14571\n",
      "Epoch 608/1000, Iterations 30440 | Train loss 2.21600\n",
      "Epoch 609/1000, Iterations 30460 | Train loss 2.17695\n",
      "Epoch 609/1000, Iterations 30480 | Train loss 2.24853\n",
      "Epoch 609/1000, Iterations 30500 | Train loss 2.13388\n",
      "Epoch 610/1000, Iterations 30520 | Train loss 2.13675\n",
      "Epoch 610/1000, Iterations 30540 | Train loss 2.22117\n",
      "Epoch 611/1000, Iterations 30560 | Train loss 2.22022\n",
      "Epoch 611/1000, Iterations 30580 | Train loss 2.22875\n",
      "Epoch 611/1000, Iterations 30600 | Train loss 2.12547\n",
      "Epoch 612/1000, Iterations 30620 | Train loss 2.17328\n",
      "Epoch 612/1000, Iterations 30640 | Train loss 2.23735\n",
      "Epoch 613/1000, Iterations 30660 | Train loss 2.19958\n",
      "Epoch 613/1000, Iterations 30680 | Train loss 2.23567\n",
      "Epoch 613/1000, Iterations 30700 | Train loss 2.07140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 614/1000, Iterations 30720 | Train loss 2.20145\n",
      "Epoch 614/1000, Iterations 30740 | Train loss 2.22361\n",
      "Epoch 615/1000, Iterations 30760 | Train loss 2.20289\n",
      "Epoch 615/1000, Iterations 30780 | Train loss 2.22515\n",
      "Epoch 615/1000, Iterations 30800 | Train loss 2.10981\n",
      "Epoch 616/1000, Iterations 30820 | Train loss 2.22920\n",
      "Epoch 616/1000, Iterations 30840 | Train loss 2.24657\n",
      "Epoch 617/1000, Iterations 30860 | Train loss 2.23326\n",
      "Epoch 617/1000, Iterations 30880 | Train loss 2.17995\n",
      "Epoch 617/1000, Iterations 30900 | Train loss 2.12265\n",
      "Epoch 618/1000, Iterations 30920 | Train loss 2.16260\n",
      "Epoch 618/1000, Iterations 30940 | Train loss 2.22484\n",
      "Epoch 619/1000, Iterations 30960 | Train loss 2.21111\n",
      "Epoch 619/1000, Iterations 30980 | Train loss 2.18601\n",
      "Epoch 619/1000, Iterations 31000 | Train loss 2.13082\n",
      "Epoch 620/1000, Iterations 31020 | Train loss 2.17617\n",
      "Epoch 620/1000, Iterations 31040 | Train loss 2.22292\n",
      "Epoch 621/1000, Iterations 31060 | Train loss 2.19798\n",
      "Epoch 621/1000, Iterations 31080 | Train loss 2.23916\n",
      "Epoch 621/1000, Iterations 31100 | Train loss 2.13656\n",
      "Epoch 622/1000, Iterations 31120 | Train loss 2.19783\n",
      "Epoch 622/1000, Iterations 31140 | Train loss 2.20611\n",
      "Epoch 623/1000, Iterations 31160 | Train loss 2.21525\n",
      "Epoch 623/1000, Iterations 31180 | Train loss 2.24305\n",
      "Epoch 623/1000, Iterations 31200 | Train loss 2.14942\n",
      "Epoch 624/1000, Iterations 31220 | Train loss 2.22842\n",
      "Epoch 624/1000, Iterations 31240 | Train loss 2.21922\n",
      "Epoch 625/1000, Iterations 31260 | Train loss 2.17995\n",
      "Epoch 625/1000, Iterations 31280 | Train loss 2.20527\n",
      "Epoch 625/1000, Iterations 31300 | Train loss 2.12459\n",
      "Epoch 626/1000, Iterations 31320 | Train loss 2.22265\n",
      "Epoch 626/1000, Iterations 31340 | Train loss 2.25415\n",
      "Epoch 627/1000, Iterations 31360 | Train loss 2.21343\n",
      "Epoch 627/1000, Iterations 31380 | Train loss 2.21261\n",
      "Epoch 627/1000, Iterations 31400 | Train loss 2.12059\n",
      "Epoch 628/1000, Iterations 31420 | Train loss 2.15868\n",
      "Epoch 628/1000, Iterations 31440 | Train loss 2.25550\n",
      "Epoch 629/1000, Iterations 31460 | Train loss 2.20899\n",
      "Epoch 629/1000, Iterations 31480 | Train loss 2.18389\n",
      "Epoch 629/1000, Iterations 31500 | Train loss 2.07084\n",
      "Epoch 630/1000, Iterations 31520 | Train loss 2.18243\n",
      "Epoch 630/1000, Iterations 31540 | Train loss 2.17024\n",
      "Epoch 631/1000, Iterations 31560 | Train loss 2.19993\n",
      "Epoch 631/1000, Iterations 31580 | Train loss 2.20984\n",
      "Epoch 631/1000, Iterations 31600 | Train loss 2.12672\n",
      "Epoch 632/1000, Iterations 31620 | Train loss 2.18168\n",
      "Epoch 632/1000, Iterations 31640 | Train loss 2.22567\n",
      "Epoch 633/1000, Iterations 31660 | Train loss 2.21605\n",
      "Epoch 633/1000, Iterations 31680 | Train loss 2.18526\n",
      "Epoch 633/1000, Iterations 31700 | Train loss 2.11525\n",
      "Epoch 634/1000, Iterations 31720 | Train loss 2.18850\n",
      "Epoch 634/1000, Iterations 31740 | Train loss 2.20424\n",
      "Epoch 635/1000, Iterations 31760 | Train loss 2.18012\n",
      "Epoch 635/1000, Iterations 31780 | Train loss 2.21609\n",
      "Epoch 635/1000, Iterations 31800 | Train loss 2.05476\n",
      "Epoch 636/1000, Iterations 31820 | Train loss 2.17529\n",
      "Epoch 636/1000, Iterations 31840 | Train loss 2.21348\n",
      "Epoch 637/1000, Iterations 31860 | Train loss 2.16876\n",
      "Epoch 637/1000, Iterations 31880 | Train loss 2.14351\n",
      "Epoch 637/1000, Iterations 31900 | Train loss 2.11427\n",
      "Epoch 638/1000, Iterations 31920 | Train loss 2.17668\n",
      "Epoch 638/1000, Iterations 31940 | Train loss 2.21979\n",
      "Epoch 639/1000, Iterations 31960 | Train loss 2.23271\n",
      "Epoch 639/1000, Iterations 31980 | Train loss 2.20863\n",
      "Epoch 639/1000, Iterations 32000 | Train loss 2.08398\n",
      "Epoch 640/1000, Iterations 32020 | Train loss 2.14353\n",
      "Epoch 640/1000, Iterations 32040 | Train loss 2.24488\n",
      "Epoch 641/1000, Iterations 32060 | Train loss 2.25115\n",
      "Epoch 641/1000, Iterations 32080 | Train loss 2.18952\n",
      "Epoch 641/1000, Iterations 32100 | Train loss 2.09862\n",
      "Epoch 642/1000, Iterations 32120 | Train loss 2.20247\n",
      "Epoch 642/1000, Iterations 32140 | Train loss 2.23125\n",
      "Epoch 643/1000, Iterations 32160 | Train loss 2.22156\n",
      "Epoch 643/1000, Iterations 32180 | Train loss 2.19367\n",
      "Epoch 643/1000, Iterations 32200 | Train loss 2.15439\n",
      "Epoch 644/1000, Iterations 32220 | Train loss 2.18202\n",
      "Epoch 644/1000, Iterations 32240 | Train loss 2.22740\n",
      "Epoch 645/1000, Iterations 32260 | Train loss 2.20975\n",
      "Epoch 645/1000, Iterations 32280 | Train loss 2.22148\n",
      "Epoch 645/1000, Iterations 32300 | Train loss 2.11348\n",
      "Epoch 646/1000, Iterations 32320 | Train loss 2.21076\n",
      "Epoch 646/1000, Iterations 32340 | Train loss 2.22646\n",
      "Epoch 647/1000, Iterations 32360 | Train loss 2.19600\n",
      "Epoch 647/1000, Iterations 32380 | Train loss 2.20555\n",
      "Epoch 647/1000, Iterations 32400 | Train loss 2.10235\n",
      "Epoch 648/1000, Iterations 32420 | Train loss 2.18029\n",
      "Epoch 648/1000, Iterations 32440 | Train loss 2.20167\n",
      "Epoch 649/1000, Iterations 32460 | Train loss 2.19527\n",
      "Epoch 649/1000, Iterations 32480 | Train loss 2.18704\n",
      "Epoch 649/1000, Iterations 32500 | Train loss 2.10221\n",
      "Epoch 650/1000, Iterations 32520 | Train loss 2.20185\n",
      "Epoch 650/1000, Iterations 32540 | Train loss 2.21754\n",
      "Epoch 651/1000, Iterations 32560 | Train loss 2.22979\n",
      "Epoch 651/1000, Iterations 32580 | Train loss 2.23092\n",
      "Epoch 651/1000, Iterations 32600 | Train loss 2.10941\n",
      "Epoch 652/1000, Iterations 32620 | Train loss 2.21632\n",
      "Epoch 652/1000, Iterations 32640 | Train loss 2.22432\n",
      "Epoch 653/1000, Iterations 32660 | Train loss 2.20271\n",
      "Epoch 653/1000, Iterations 32680 | Train loss 2.17584\n",
      "Epoch 653/1000, Iterations 32700 | Train loss 2.07711\n",
      "Epoch 654/1000, Iterations 32720 | Train loss 2.16368\n",
      "Epoch 654/1000, Iterations 32740 | Train loss 2.18510\n",
      "Epoch 655/1000, Iterations 32760 | Train loss 2.19925\n",
      "Epoch 655/1000, Iterations 32780 | Train loss 2.20466\n",
      "Epoch 655/1000, Iterations 32800 | Train loss 2.12087\n",
      "Epoch 656/1000, Iterations 32820 | Train loss 2.17459\n",
      "Epoch 656/1000, Iterations 32840 | Train loss 2.20629\n",
      "Epoch 657/1000, Iterations 32860 | Train loss 2.20249\n",
      "Epoch 657/1000, Iterations 32880 | Train loss 2.18532\n",
      "Epoch 657/1000, Iterations 32900 | Train loss 2.10107\n",
      "Epoch 658/1000, Iterations 32920 | Train loss 2.15474\n",
      "Epoch 658/1000, Iterations 32940 | Train loss 2.23518\n",
      "Epoch 659/1000, Iterations 32960 | Train loss 2.19269\n",
      "Epoch 659/1000, Iterations 32980 | Train loss 2.22603\n",
      "Epoch 659/1000, Iterations 33000 | Train loss 2.08551\n",
      "Epoch 660/1000, Iterations 33020 | Train loss 2.16808\n",
      "Epoch 660/1000, Iterations 33040 | Train loss 2.20569\n",
      "Epoch 661/1000, Iterations 33060 | Train loss 2.21615\n",
      "Epoch 661/1000, Iterations 33080 | Train loss 2.21798\n",
      "Epoch 661/1000, Iterations 33100 | Train loss 2.15564\n",
      "Epoch 662/1000, Iterations 33120 | Train loss 2.15752\n",
      "Epoch 662/1000, Iterations 33140 | Train loss 2.21154\n",
      "Epoch 663/1000, Iterations 33160 | Train loss 2.20158\n",
      "Epoch 663/1000, Iterations 33180 | Train loss 2.21883\n",
      "Epoch 663/1000, Iterations 33200 | Train loss 2.07558\n",
      "Epoch 664/1000, Iterations 33220 | Train loss 2.19720\n",
      "Epoch 664/1000, Iterations 33240 | Train loss 2.24521\n",
      "Epoch 665/1000, Iterations 33260 | Train loss 2.20091\n",
      "Epoch 665/1000, Iterations 33280 | Train loss 2.21354\n",
      "Epoch 665/1000, Iterations 33300 | Train loss 2.11515\n",
      "Epoch 666/1000, Iterations 33320 | Train loss 2.18921\n",
      "Epoch 666/1000, Iterations 33340 | Train loss 2.23385\n",
      "Epoch 667/1000, Iterations 33360 | Train loss 2.19903\n",
      "Epoch 667/1000, Iterations 33380 | Train loss 2.23416\n",
      "Epoch 667/1000, Iterations 33400 | Train loss 2.07584\n",
      "Epoch 668/1000, Iterations 33420 | Train loss 2.20974\n",
      "Epoch 668/1000, Iterations 33440 | Train loss 2.20822\n",
      "Epoch 669/1000, Iterations 33460 | Train loss 2.12562\n",
      "Epoch 669/1000, Iterations 33480 | Train loss 2.18116\n",
      "Epoch 669/1000, Iterations 33500 | Train loss 2.15389\n",
      "Epoch 670/1000, Iterations 33520 | Train loss 2.19375\n",
      "Epoch 670/1000, Iterations 33540 | Train loss 2.20519\n",
      "Epoch 671/1000, Iterations 33560 | Train loss 2.21421\n",
      "Epoch 671/1000, Iterations 33580 | Train loss 2.21410\n",
      "Epoch 671/1000, Iterations 33600 | Train loss 2.09377\n",
      "Epoch 672/1000, Iterations 33620 | Train loss 2.20420\n",
      "Epoch 672/1000, Iterations 33640 | Train loss 2.18411\n",
      "Epoch 673/1000, Iterations 33660 | Train loss 2.18996\n",
      "Epoch 673/1000, Iterations 33680 | Train loss 2.20372\n",
      "Epoch 673/1000, Iterations 33700 | Train loss 2.11575\n",
      "Epoch 674/1000, Iterations 33720 | Train loss 2.18892\n",
      "Epoch 674/1000, Iterations 33740 | Train loss 2.16850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 675/1000, Iterations 33760 | Train loss 2.19612\n",
      "Epoch 675/1000, Iterations 33780 | Train loss 2.20742\n",
      "Epoch 675/1000, Iterations 33800 | Train loss 2.13090\n",
      "Epoch 676/1000, Iterations 33820 | Train loss 2.21603\n",
      "Epoch 676/1000, Iterations 33840 | Train loss 2.23700\n",
      "Epoch 677/1000, Iterations 33860 | Train loss 2.15958\n",
      "Epoch 677/1000, Iterations 33880 | Train loss 2.21411\n",
      "Epoch 677/1000, Iterations 33900 | Train loss 2.12332\n",
      "Epoch 678/1000, Iterations 33920 | Train loss 2.19322\n",
      "Epoch 678/1000, Iterations 33940 | Train loss 2.23070\n",
      "Epoch 679/1000, Iterations 33960 | Train loss 2.24436\n",
      "Epoch 679/1000, Iterations 33980 | Train loss 2.19174\n",
      "Epoch 679/1000, Iterations 34000 | Train loss 2.12400\n",
      "Epoch 680/1000, Iterations 34020 | Train loss 2.20274\n",
      "Epoch 680/1000, Iterations 34040 | Train loss 2.21921\n",
      "Epoch 681/1000, Iterations 34060 | Train loss 2.17117\n",
      "Epoch 681/1000, Iterations 34080 | Train loss 2.19251\n",
      "Epoch 681/1000, Iterations 34100 | Train loss 2.11289\n",
      "Epoch 682/1000, Iterations 34120 | Train loss 2.15853\n",
      "Epoch 682/1000, Iterations 34140 | Train loss 2.24554\n",
      "Epoch 683/1000, Iterations 34160 | Train loss 2.18066\n",
      "Epoch 683/1000, Iterations 34180 | Train loss 2.23436\n",
      "Epoch 683/1000, Iterations 34200 | Train loss 2.11360\n",
      "Epoch 684/1000, Iterations 34220 | Train loss 2.18028\n",
      "Epoch 684/1000, Iterations 34240 | Train loss 2.22294\n",
      "Epoch 685/1000, Iterations 34260 | Train loss 2.18521\n",
      "Epoch 685/1000, Iterations 34280 | Train loss 2.21055\n",
      "Epoch 685/1000, Iterations 34300 | Train loss 2.09433\n",
      "Epoch 686/1000, Iterations 34320 | Train loss 2.19737\n",
      "Epoch 686/1000, Iterations 34340 | Train loss 2.25028\n",
      "Epoch 687/1000, Iterations 34360 | Train loss 2.20296\n",
      "Epoch 687/1000, Iterations 34380 | Train loss 2.16948\n",
      "Epoch 687/1000, Iterations 34400 | Train loss 2.13266\n",
      "Epoch 688/1000, Iterations 34420 | Train loss 2.12159\n",
      "Epoch 688/1000, Iterations 34440 | Train loss 2.19130\n",
      "Epoch 689/1000, Iterations 34460 | Train loss 2.17744\n",
      "Epoch 689/1000, Iterations 34480 | Train loss 2.23306\n",
      "Epoch 689/1000, Iterations 34500 | Train loss 2.08859\n",
      "Epoch 690/1000, Iterations 34520 | Train loss 2.17764\n",
      "Epoch 690/1000, Iterations 34540 | Train loss 2.22413\n",
      "Epoch 691/1000, Iterations 34560 | Train loss 2.21421\n",
      "Epoch 691/1000, Iterations 34580 | Train loss 2.18671\n",
      "Epoch 691/1000, Iterations 34600 | Train loss 2.04073\n",
      "Epoch 692/1000, Iterations 34620 | Train loss 2.16796\n",
      "Epoch 692/1000, Iterations 34640 | Train loss 2.22714\n",
      "Epoch 693/1000, Iterations 34660 | Train loss 2.22316\n",
      "Epoch 693/1000, Iterations 34680 | Train loss 2.24888\n",
      "Epoch 693/1000, Iterations 34700 | Train loss 2.11318\n",
      "Epoch 694/1000, Iterations 34720 | Train loss 2.20403\n",
      "Epoch 694/1000, Iterations 34740 | Train loss 2.22903\n",
      "Epoch 695/1000, Iterations 34760 | Train loss 2.21479\n",
      "Epoch 695/1000, Iterations 34780 | Train loss 2.19512\n",
      "Epoch 695/1000, Iterations 34800 | Train loss 2.11154\n",
      "Epoch 696/1000, Iterations 34820 | Train loss 2.16491\n",
      "Epoch 696/1000, Iterations 34840 | Train loss 2.21789\n",
      "Epoch 697/1000, Iterations 34860 | Train loss 2.17130\n",
      "Epoch 697/1000, Iterations 34880 | Train loss 2.18418\n",
      "Epoch 697/1000, Iterations 34900 | Train loss 2.07226\n",
      "Epoch 698/1000, Iterations 34920 | Train loss 2.18072\n",
      "Epoch 698/1000, Iterations 34940 | Train loss 2.23710\n",
      "Epoch 699/1000, Iterations 34960 | Train loss 2.21476\n",
      "Epoch 699/1000, Iterations 34980 | Train loss 2.21782\n",
      "Epoch 699/1000, Iterations 35000 | Train loss 2.13908\n",
      "Epoch 700/1000, Iterations 35020 | Train loss 2.16197\n",
      "Epoch 700/1000, Iterations 35040 | Train loss 2.21483\n",
      "Epoch 701/1000, Iterations 35060 | Train loss 2.20098\n",
      "Epoch 701/1000, Iterations 35080 | Train loss 2.22012\n",
      "Epoch 701/1000, Iterations 35100 | Train loss 2.09554\n",
      "Epoch 702/1000, Iterations 35120 | Train loss 2.19181\n",
      "Epoch 702/1000, Iterations 35140 | Train loss 2.21468\n",
      "Epoch 703/1000, Iterations 35160 | Train loss 2.18055\n",
      "Epoch 703/1000, Iterations 35180 | Train loss 2.23017\n",
      "Epoch 703/1000, Iterations 35200 | Train loss 2.08641\n",
      "Epoch 704/1000, Iterations 35220 | Train loss 2.18792\n",
      "Epoch 704/1000, Iterations 35240 | Train loss 2.21270\n",
      "Epoch 705/1000, Iterations 35260 | Train loss 2.19756\n",
      "Epoch 705/1000, Iterations 35280 | Train loss 2.26676\n",
      "Epoch 705/1000, Iterations 35300 | Train loss 2.07732\n",
      "Epoch 706/1000, Iterations 35320 | Train loss 2.16615\n",
      "Epoch 706/1000, Iterations 35340 | Train loss 2.17918\n",
      "Epoch 707/1000, Iterations 35360 | Train loss 2.21688\n",
      "Epoch 707/1000, Iterations 35380 | Train loss 2.18893\n",
      "Epoch 707/1000, Iterations 35400 | Train loss 2.13256\n",
      "Epoch 708/1000, Iterations 35420 | Train loss 2.18671\n",
      "Epoch 708/1000, Iterations 35440 | Train loss 2.18878\n",
      "Epoch 709/1000, Iterations 35460 | Train loss 2.20057\n",
      "Epoch 709/1000, Iterations 35480 | Train loss 2.21948\n",
      "Epoch 709/1000, Iterations 35500 | Train loss 2.12123\n",
      "Epoch 710/1000, Iterations 35520 | Train loss 2.16829\n",
      "Epoch 710/1000, Iterations 35540 | Train loss 2.21051\n",
      "Epoch 711/1000, Iterations 35560 | Train loss 2.17095\n",
      "Epoch 711/1000, Iterations 35580 | Train loss 2.22874\n",
      "Epoch 711/1000, Iterations 35600 | Train loss 2.07710\n",
      "Epoch 712/1000, Iterations 35620 | Train loss 2.14100\n",
      "Epoch 712/1000, Iterations 35640 | Train loss 2.23274\n",
      "Epoch 713/1000, Iterations 35660 | Train loss 2.18339\n",
      "Epoch 713/1000, Iterations 35680 | Train loss 2.23387\n",
      "Epoch 713/1000, Iterations 35700 | Train loss 2.10506\n",
      "Epoch 714/1000, Iterations 35720 | Train loss 2.19179\n",
      "Epoch 714/1000, Iterations 35740 | Train loss 2.24434\n",
      "Epoch 715/1000, Iterations 35760 | Train loss 2.18407\n",
      "Epoch 715/1000, Iterations 35780 | Train loss 2.22940\n",
      "Epoch 715/1000, Iterations 35800 | Train loss 2.10931\n",
      "Epoch 716/1000, Iterations 35820 | Train loss 2.15140\n",
      "Epoch 716/1000, Iterations 35840 | Train loss 2.26156\n",
      "Epoch 717/1000, Iterations 35860 | Train loss 2.20087\n",
      "Epoch 717/1000, Iterations 35880 | Train loss 2.21765\n",
      "Epoch 717/1000, Iterations 35900 | Train loss 2.06925\n",
      "Epoch 718/1000, Iterations 35920 | Train loss 2.20150\n",
      "Epoch 718/1000, Iterations 35940 | Train loss 2.19440\n",
      "Epoch 719/1000, Iterations 35960 | Train loss 2.13227\n",
      "Epoch 719/1000, Iterations 35980 | Train loss 2.19420\n",
      "Epoch 719/1000, Iterations 36000 | Train loss 2.14787\n",
      "Epoch 720/1000, Iterations 36020 | Train loss 2.13089\n",
      "Epoch 720/1000, Iterations 36040 | Train loss 2.18436\n",
      "Epoch 721/1000, Iterations 36060 | Train loss 2.24060\n",
      "Epoch 721/1000, Iterations 36080 | Train loss 2.20392\n",
      "Epoch 721/1000, Iterations 36100 | Train loss 2.10105\n",
      "Epoch 722/1000, Iterations 36120 | Train loss 2.17738\n",
      "Epoch 722/1000, Iterations 36140 | Train loss 2.28598\n",
      "Epoch 723/1000, Iterations 36160 | Train loss 2.16184\n",
      "Epoch 723/1000, Iterations 36180 | Train loss 2.16958\n",
      "Epoch 723/1000, Iterations 36200 | Train loss 2.10304\n",
      "Epoch 724/1000, Iterations 36220 | Train loss 2.16399\n",
      "Epoch 724/1000, Iterations 36240 | Train loss 2.19436\n",
      "Epoch 725/1000, Iterations 36260 | Train loss 2.18255\n",
      "Epoch 725/1000, Iterations 36280 | Train loss 2.19948\n",
      "Epoch 725/1000, Iterations 36300 | Train loss 2.04510\n",
      "Epoch 726/1000, Iterations 36320 | Train loss 2.18314\n",
      "Epoch 726/1000, Iterations 36340 | Train loss 2.21691\n",
      "Epoch 727/1000, Iterations 36360 | Train loss 2.22557\n",
      "Epoch 727/1000, Iterations 36380 | Train loss 2.19286\n",
      "Epoch 727/1000, Iterations 36400 | Train loss 2.08531\n",
      "Epoch 728/1000, Iterations 36420 | Train loss 2.19899\n",
      "Epoch 728/1000, Iterations 36440 | Train loss 2.24765\n",
      "Epoch 729/1000, Iterations 36460 | Train loss 2.22291\n",
      "Epoch 729/1000, Iterations 36480 | Train loss 2.18847\n",
      "Epoch 729/1000, Iterations 36500 | Train loss 2.08559\n",
      "Epoch 730/1000, Iterations 36520 | Train loss 2.19137\n",
      "Epoch 730/1000, Iterations 36540 | Train loss 2.20632\n",
      "Epoch 731/1000, Iterations 36560 | Train loss 2.22483\n",
      "Epoch 731/1000, Iterations 36580 | Train loss 2.18466\n",
      "Epoch 731/1000, Iterations 36600 | Train loss 2.12568\n",
      "Epoch 732/1000, Iterations 36620 | Train loss 2.16723\n",
      "Epoch 732/1000, Iterations 36640 | Train loss 2.19403\n",
      "Epoch 733/1000, Iterations 36660 | Train loss 2.17861\n",
      "Epoch 733/1000, Iterations 36680 | Train loss 2.22667\n",
      "Epoch 733/1000, Iterations 36700 | Train loss 2.07629\n",
      "Epoch 734/1000, Iterations 36720 | Train loss 2.20201\n",
      "Epoch 734/1000, Iterations 36740 | Train loss 2.21435\n",
      "Epoch 735/1000, Iterations 36760 | Train loss 2.16804\n",
      "Epoch 735/1000, Iterations 36780 | Train loss 2.19363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 735/1000, Iterations 36800 | Train loss 2.10190\n",
      "Epoch 736/1000, Iterations 36820 | Train loss 2.22323\n",
      "Epoch 736/1000, Iterations 36840 | Train loss 2.21782\n",
      "Epoch 737/1000, Iterations 36860 | Train loss 2.16909\n",
      "Epoch 737/1000, Iterations 36880 | Train loss 2.18800\n",
      "Epoch 737/1000, Iterations 36900 | Train loss 2.07599\n",
      "Epoch 738/1000, Iterations 36920 | Train loss 2.19153\n",
      "Epoch 738/1000, Iterations 36940 | Train loss 2.23521\n",
      "Epoch 739/1000, Iterations 36960 | Train loss 2.15335\n",
      "Epoch 739/1000, Iterations 36980 | Train loss 2.18158\n",
      "Epoch 739/1000, Iterations 37000 | Train loss 2.08713\n",
      "Epoch 740/1000, Iterations 37020 | Train loss 2.16690\n",
      "Epoch 740/1000, Iterations 37040 | Train loss 2.21799\n",
      "Epoch 741/1000, Iterations 37060 | Train loss 2.18011\n",
      "Epoch 741/1000, Iterations 37080 | Train loss 2.22052\n",
      "Epoch 741/1000, Iterations 37100 | Train loss 2.14444\n",
      "Epoch 742/1000, Iterations 37120 | Train loss 2.19015\n",
      "Epoch 742/1000, Iterations 37140 | Train loss 2.22158\n",
      "Epoch 743/1000, Iterations 37160 | Train loss 2.14268\n",
      "Epoch 743/1000, Iterations 37180 | Train loss 2.19783\n",
      "Epoch 743/1000, Iterations 37200 | Train loss 2.10397\n",
      "Epoch 744/1000, Iterations 37220 | Train loss 2.15178\n",
      "Epoch 744/1000, Iterations 37240 | Train loss 2.22494\n",
      "Epoch 745/1000, Iterations 37260 | Train loss 2.15664\n",
      "Epoch 745/1000, Iterations 37280 | Train loss 2.19731\n",
      "Epoch 745/1000, Iterations 37300 | Train loss 2.10142\n",
      "Epoch 746/1000, Iterations 37320 | Train loss 2.15339\n",
      "Epoch 746/1000, Iterations 37340 | Train loss 2.20102\n",
      "Epoch 747/1000, Iterations 37360 | Train loss 2.16464\n",
      "Epoch 747/1000, Iterations 37380 | Train loss 2.24656\n",
      "Epoch 747/1000, Iterations 37400 | Train loss 2.09992\n",
      "Epoch 748/1000, Iterations 37420 | Train loss 2.16599\n",
      "Epoch 748/1000, Iterations 37440 | Train loss 2.21476\n",
      "Epoch 749/1000, Iterations 37460 | Train loss 2.16742\n",
      "Epoch 749/1000, Iterations 37480 | Train loss 2.18324\n",
      "Epoch 749/1000, Iterations 37500 | Train loss 2.12978\n",
      "Epoch 750/1000, Iterations 37520 | Train loss 2.15601\n",
      "Epoch 750/1000, Iterations 37540 | Train loss 2.21703\n",
      "Epoch 751/1000, Iterations 37560 | Train loss 2.18829\n",
      "Epoch 751/1000, Iterations 37580 | Train loss 2.19161\n",
      "Epoch 751/1000, Iterations 37600 | Train loss 2.08729\n",
      "Epoch 752/1000, Iterations 37620 | Train loss 2.19136\n",
      "Epoch 752/1000, Iterations 37640 | Train loss 2.20115\n",
      "Epoch 753/1000, Iterations 37660 | Train loss 2.22089\n",
      "Epoch 753/1000, Iterations 37680 | Train loss 2.18936\n",
      "Epoch 753/1000, Iterations 37700 | Train loss 2.10023\n",
      "Epoch 754/1000, Iterations 37720 | Train loss 2.14758\n",
      "Epoch 754/1000, Iterations 37740 | Train loss 2.21527\n",
      "Epoch 755/1000, Iterations 37760 | Train loss 2.17321\n",
      "Epoch 755/1000, Iterations 37780 | Train loss 2.23403\n",
      "Epoch 755/1000, Iterations 37800 | Train loss 2.11389\n",
      "Epoch 756/1000, Iterations 37820 | Train loss 2.14673\n",
      "Epoch 756/1000, Iterations 37840 | Train loss 2.22998\n",
      "Epoch 757/1000, Iterations 37860 | Train loss 2.15073\n",
      "Epoch 757/1000, Iterations 37880 | Train loss 2.20928\n",
      "Epoch 757/1000, Iterations 37900 | Train loss 2.12380\n",
      "Epoch 758/1000, Iterations 37920 | Train loss 2.17040\n",
      "Epoch 758/1000, Iterations 37940 | Train loss 2.23029\n",
      "Epoch 759/1000, Iterations 37960 | Train loss 2.15830\n",
      "Epoch 759/1000, Iterations 37980 | Train loss 2.19706\n",
      "Epoch 759/1000, Iterations 38000 | Train loss 2.11000\n",
      "Epoch 760/1000, Iterations 38020 | Train loss 2.15568\n",
      "Epoch 760/1000, Iterations 38040 | Train loss 2.20498\n",
      "Epoch 761/1000, Iterations 38060 | Train loss 2.20007\n",
      "Epoch 761/1000, Iterations 38080 | Train loss 2.21677\n",
      "Epoch 761/1000, Iterations 38100 | Train loss 2.12936\n",
      "Epoch 762/1000, Iterations 38120 | Train loss 2.16957\n",
      "Epoch 762/1000, Iterations 38140 | Train loss 2.18500\n",
      "Epoch 763/1000, Iterations 38160 | Train loss 2.23588\n",
      "Epoch 763/1000, Iterations 38180 | Train loss 2.22705\n",
      "Epoch 763/1000, Iterations 38200 | Train loss 2.12495\n",
      "Epoch 764/1000, Iterations 38220 | Train loss 2.14465\n",
      "Epoch 764/1000, Iterations 38240 | Train loss 2.16657\n",
      "Epoch 765/1000, Iterations 38260 | Train loss 2.20351\n",
      "Epoch 765/1000, Iterations 38280 | Train loss 2.21321\n",
      "Epoch 765/1000, Iterations 38300 | Train loss 2.07565\n",
      "Epoch 766/1000, Iterations 38320 | Train loss 2.20588\n",
      "Epoch 766/1000, Iterations 38340 | Train loss 2.26898\n",
      "Epoch 767/1000, Iterations 38360 | Train loss 2.13257\n",
      "Epoch 767/1000, Iterations 38380 | Train loss 2.20161\n",
      "Epoch 767/1000, Iterations 38400 | Train loss 2.11336\n",
      "Epoch 768/1000, Iterations 38420 | Train loss 2.15061\n",
      "Epoch 768/1000, Iterations 38440 | Train loss 2.25643\n",
      "Epoch 769/1000, Iterations 38460 | Train loss 2.17661\n",
      "Epoch 769/1000, Iterations 38480 | Train loss 2.20071\n",
      "Epoch 769/1000, Iterations 38500 | Train loss 2.12624\n",
      "Epoch 770/1000, Iterations 38520 | Train loss 2.15346\n",
      "Epoch 770/1000, Iterations 38540 | Train loss 2.24838\n",
      "Epoch 771/1000, Iterations 38560 | Train loss 2.17114\n",
      "Epoch 771/1000, Iterations 38580 | Train loss 2.17194\n",
      "Epoch 771/1000, Iterations 38600 | Train loss 2.08733\n",
      "Epoch 772/1000, Iterations 38620 | Train loss 2.25093\n",
      "Epoch 772/1000, Iterations 38640 | Train loss 2.19698\n",
      "Epoch 773/1000, Iterations 38660 | Train loss 2.22495\n",
      "Epoch 773/1000, Iterations 38680 | Train loss 2.24070\n",
      "Epoch 773/1000, Iterations 38700 | Train loss 2.08146\n",
      "Epoch 774/1000, Iterations 38720 | Train loss 2.17465\n",
      "Epoch 774/1000, Iterations 38740 | Train loss 2.19904\n",
      "Epoch 775/1000, Iterations 38760 | Train loss 2.18483\n",
      "Epoch 775/1000, Iterations 38780 | Train loss 2.22638\n",
      "Epoch 775/1000, Iterations 38800 | Train loss 2.04247\n",
      "Epoch 776/1000, Iterations 38820 | Train loss 2.16994\n",
      "Epoch 776/1000, Iterations 38840 | Train loss 2.16380\n",
      "Epoch 777/1000, Iterations 38860 | Train loss 2.19935\n",
      "Epoch 777/1000, Iterations 38880 | Train loss 2.23176\n",
      "Epoch 777/1000, Iterations 38900 | Train loss 2.10279\n",
      "Epoch 778/1000, Iterations 38920 | Train loss 2.15720\n",
      "Epoch 778/1000, Iterations 38940 | Train loss 2.19066\n",
      "Epoch 779/1000, Iterations 38960 | Train loss 2.13184\n",
      "Epoch 779/1000, Iterations 38980 | Train loss 2.18617\n",
      "Epoch 779/1000, Iterations 39000 | Train loss 2.10902\n",
      "Epoch 780/1000, Iterations 39020 | Train loss 2.18664\n",
      "Epoch 780/1000, Iterations 39040 | Train loss 2.17413\n",
      "Epoch 781/1000, Iterations 39060 | Train loss 2.20347\n",
      "Epoch 781/1000, Iterations 39080 | Train loss 2.20516\n",
      "Epoch 781/1000, Iterations 39100 | Train loss 2.12243\n",
      "Epoch 782/1000, Iterations 39120 | Train loss 2.16361\n",
      "Epoch 782/1000, Iterations 39140 | Train loss 2.18362\n",
      "Epoch 783/1000, Iterations 39160 | Train loss 2.19906\n",
      "Epoch 783/1000, Iterations 39180 | Train loss 2.17594\n",
      "Epoch 783/1000, Iterations 39200 | Train loss 2.06033\n",
      "Epoch 784/1000, Iterations 39220 | Train loss 2.16874\n",
      "Epoch 784/1000, Iterations 39240 | Train loss 2.22526\n",
      "Epoch 785/1000, Iterations 39260 | Train loss 2.16245\n",
      "Epoch 785/1000, Iterations 39280 | Train loss 2.20340\n",
      "Epoch 785/1000, Iterations 39300 | Train loss 2.12144\n",
      "Epoch 786/1000, Iterations 39320 | Train loss 2.17560\n",
      "Epoch 786/1000, Iterations 39340 | Train loss 2.22684\n",
      "Epoch 787/1000, Iterations 39360 | Train loss 2.20381\n",
      "Epoch 787/1000, Iterations 39380 | Train loss 2.19388\n",
      "Epoch 787/1000, Iterations 39400 | Train loss 2.12183\n",
      "Epoch 788/1000, Iterations 39420 | Train loss 2.19902\n",
      "Epoch 788/1000, Iterations 39440 | Train loss 2.20474\n",
      "Epoch 789/1000, Iterations 39460 | Train loss 2.14520\n",
      "Epoch 789/1000, Iterations 39480 | Train loss 2.15043\n",
      "Epoch 789/1000, Iterations 39500 | Train loss 2.11884\n",
      "Epoch 790/1000, Iterations 39520 | Train loss 2.13439\n",
      "Epoch 790/1000, Iterations 39540 | Train loss 2.18713\n",
      "Epoch 791/1000, Iterations 39560 | Train loss 2.19122\n",
      "Epoch 791/1000, Iterations 39580 | Train loss 2.17949\n",
      "Epoch 791/1000, Iterations 39600 | Train loss 2.10228\n",
      "Epoch 792/1000, Iterations 39620 | Train loss 2.17666\n",
      "Epoch 792/1000, Iterations 39640 | Train loss 2.20080\n",
      "Epoch 793/1000, Iterations 39660 | Train loss 2.19424\n",
      "Epoch 793/1000, Iterations 39680 | Train loss 2.21639\n",
      "Epoch 793/1000, Iterations 39700 | Train loss 2.13297\n",
      "Epoch 794/1000, Iterations 39720 | Train loss 2.16364\n",
      "Epoch 794/1000, Iterations 39740 | Train loss 2.18974\n",
      "Epoch 795/1000, Iterations 39760 | Train loss 2.20121\n",
      "Epoch 795/1000, Iterations 39780 | Train loss 2.19174\n",
      "Epoch 795/1000, Iterations 39800 | Train loss 2.09997\n",
      "Epoch 796/1000, Iterations 39820 | Train loss 2.16562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 796/1000, Iterations 39840 | Train loss 2.22041\n",
      "Epoch 797/1000, Iterations 39860 | Train loss 2.19672\n",
      "Epoch 797/1000, Iterations 39880 | Train loss 2.19274\n",
      "Epoch 797/1000, Iterations 39900 | Train loss 2.07805\n",
      "Epoch 798/1000, Iterations 39920 | Train loss 2.20697\n",
      "Epoch 798/1000, Iterations 39940 | Train loss 2.22026\n",
      "Epoch 799/1000, Iterations 39960 | Train loss 2.21332\n",
      "Epoch 799/1000, Iterations 39980 | Train loss 2.20356\n",
      "Epoch 799/1000, Iterations 40000 | Train loss 2.09635\n",
      "Epoch 800/1000, Iterations 40020 | Train loss 2.11743\n",
      "Epoch 800/1000, Iterations 40040 | Train loss 2.16176\n",
      "Epoch 801/1000, Iterations 40060 | Train loss 2.16513\n",
      "Epoch 801/1000, Iterations 40080 | Train loss 2.17916\n",
      "Epoch 801/1000, Iterations 40100 | Train loss 2.14910\n",
      "Epoch 802/1000, Iterations 40120 | Train loss 2.18980\n",
      "Epoch 802/1000, Iterations 40140 | Train loss 2.20838\n",
      "Epoch 803/1000, Iterations 40160 | Train loss 2.22350\n",
      "Epoch 803/1000, Iterations 40180 | Train loss 2.17034\n",
      "Epoch 803/1000, Iterations 40200 | Train loss 2.15208\n",
      "Epoch 804/1000, Iterations 40220 | Train loss 2.15058\n",
      "Epoch 804/1000, Iterations 40240 | Train loss 2.14434\n",
      "Epoch 805/1000, Iterations 40260 | Train loss 2.20400\n",
      "Epoch 805/1000, Iterations 40280 | Train loss 2.21807\n",
      "Epoch 805/1000, Iterations 40300 | Train loss 2.09344\n",
      "Epoch 806/1000, Iterations 40320 | Train loss 2.16981\n",
      "Epoch 806/1000, Iterations 40340 | Train loss 2.22629\n",
      "Epoch 807/1000, Iterations 40360 | Train loss 2.17117\n",
      "Epoch 807/1000, Iterations 40380 | Train loss 2.20162\n",
      "Epoch 807/1000, Iterations 40400 | Train loss 2.06023\n",
      "Epoch 808/1000, Iterations 40420 | Train loss 2.22547\n",
      "Epoch 808/1000, Iterations 40440 | Train loss 2.22895\n",
      "Epoch 809/1000, Iterations 40460 | Train loss 2.20189\n",
      "Epoch 809/1000, Iterations 40480 | Train loss 2.20377\n",
      "Epoch 809/1000, Iterations 40500 | Train loss 2.13655\n",
      "Epoch 810/1000, Iterations 40520 | Train loss 2.13417\n",
      "Epoch 810/1000, Iterations 40540 | Train loss 2.20201\n",
      "Epoch 811/1000, Iterations 40560 | Train loss 2.16977\n",
      "Epoch 811/1000, Iterations 40580 | Train loss 2.17288\n",
      "Epoch 811/1000, Iterations 40600 | Train loss 2.06385\n",
      "Epoch 812/1000, Iterations 40620 | Train loss 2.16948\n",
      "Epoch 812/1000, Iterations 40640 | Train loss 2.21498\n",
      "Epoch 813/1000, Iterations 40660 | Train loss 2.23201\n",
      "Epoch 813/1000, Iterations 40680 | Train loss 2.20428\n",
      "Epoch 813/1000, Iterations 40700 | Train loss 2.09495\n",
      "Epoch 814/1000, Iterations 40720 | Train loss 2.12887\n",
      "Epoch 814/1000, Iterations 40740 | Train loss 2.18813\n",
      "Epoch 815/1000, Iterations 40760 | Train loss 2.16688\n",
      "Epoch 815/1000, Iterations 40780 | Train loss 2.16282\n",
      "Epoch 815/1000, Iterations 40800 | Train loss 2.07817\n",
      "Epoch 816/1000, Iterations 40820 | Train loss 2.15942\n",
      "Epoch 816/1000, Iterations 40840 | Train loss 2.17479\n",
      "Epoch 817/1000, Iterations 40860 | Train loss 2.14477\n",
      "Epoch 817/1000, Iterations 40880 | Train loss 2.20407\n",
      "Epoch 817/1000, Iterations 40900 | Train loss 2.11340\n",
      "Epoch 818/1000, Iterations 40920 | Train loss 2.15005\n",
      "Epoch 818/1000, Iterations 40940 | Train loss 2.21864\n",
      "Epoch 819/1000, Iterations 40960 | Train loss 2.19347\n",
      "Epoch 819/1000, Iterations 40980 | Train loss 2.17622\n",
      "Epoch 819/1000, Iterations 41000 | Train loss 2.12806\n",
      "Epoch 820/1000, Iterations 41020 | Train loss 2.16631\n",
      "Epoch 820/1000, Iterations 41040 | Train loss 2.22259\n",
      "Epoch 821/1000, Iterations 41060 | Train loss 2.17250\n",
      "Epoch 821/1000, Iterations 41080 | Train loss 2.13305\n",
      "Epoch 821/1000, Iterations 41100 | Train loss 2.05323\n",
      "Epoch 822/1000, Iterations 41120 | Train loss 2.15620\n",
      "Epoch 822/1000, Iterations 41140 | Train loss 2.19789\n",
      "Epoch 823/1000, Iterations 41160 | Train loss 2.16380\n",
      "Epoch 823/1000, Iterations 41180 | Train loss 2.21504\n",
      "Epoch 823/1000, Iterations 41200 | Train loss 2.10964\n",
      "Epoch 824/1000, Iterations 41220 | Train loss 2.13524\n",
      "Epoch 824/1000, Iterations 41240 | Train loss 2.18255\n",
      "Epoch 825/1000, Iterations 41260 | Train loss 2.20473\n",
      "Epoch 825/1000, Iterations 41280 | Train loss 2.18198\n",
      "Epoch 825/1000, Iterations 41300 | Train loss 2.12983\n",
      "Epoch 826/1000, Iterations 41320 | Train loss 2.18254\n",
      "Epoch 826/1000, Iterations 41340 | Train loss 2.21078\n",
      "Epoch 827/1000, Iterations 41360 | Train loss 2.18060\n",
      "Epoch 827/1000, Iterations 41380 | Train loss 2.21998\n",
      "Epoch 827/1000, Iterations 41400 | Train loss 2.06199\n",
      "Epoch 828/1000, Iterations 41420 | Train loss 2.20231\n",
      "Epoch 828/1000, Iterations 41440 | Train loss 2.20012\n",
      "Epoch 829/1000, Iterations 41460 | Train loss 2.25251\n",
      "Epoch 829/1000, Iterations 41480 | Train loss 2.20407\n",
      "Epoch 829/1000, Iterations 41500 | Train loss 2.09718\n",
      "Epoch 830/1000, Iterations 41520 | Train loss 2.14558\n",
      "Epoch 830/1000, Iterations 41540 | Train loss 2.14920\n",
      "Epoch 831/1000, Iterations 41560 | Train loss 2.18836\n",
      "Epoch 831/1000, Iterations 41580 | Train loss 2.20561\n",
      "Epoch 831/1000, Iterations 41600 | Train loss 2.05659\n",
      "Epoch 832/1000, Iterations 41620 | Train loss 2.11568\n",
      "Epoch 832/1000, Iterations 41640 | Train loss 2.17276\n",
      "Epoch 833/1000, Iterations 41660 | Train loss 2.18388\n",
      "Epoch 833/1000, Iterations 41680 | Train loss 2.16100\n",
      "Epoch 833/1000, Iterations 41700 | Train loss 2.05019\n",
      "Epoch 834/1000, Iterations 41720 | Train loss 2.15191\n",
      "Epoch 834/1000, Iterations 41740 | Train loss 2.19870\n",
      "Epoch 835/1000, Iterations 41760 | Train loss 2.15176\n",
      "Epoch 835/1000, Iterations 41780 | Train loss 2.19947\n",
      "Epoch 835/1000, Iterations 41800 | Train loss 2.09297\n",
      "Epoch 836/1000, Iterations 41820 | Train loss 2.17019\n",
      "Epoch 836/1000, Iterations 41840 | Train loss 2.18425\n",
      "Epoch 837/1000, Iterations 41860 | Train loss 2.14336\n",
      "Epoch 837/1000, Iterations 41880 | Train loss 2.20941\n",
      "Epoch 837/1000, Iterations 41900 | Train loss 2.14587\n",
      "Epoch 838/1000, Iterations 41920 | Train loss 2.16059\n",
      "Epoch 838/1000, Iterations 41940 | Train loss 2.21143\n",
      "Epoch 839/1000, Iterations 41960 | Train loss 2.20035\n",
      "Epoch 839/1000, Iterations 41980 | Train loss 2.22705\n",
      "Epoch 839/1000, Iterations 42000 | Train loss 2.08251\n",
      "Epoch 840/1000, Iterations 42020 | Train loss 2.15962\n",
      "Epoch 840/1000, Iterations 42040 | Train loss 2.19015\n",
      "Epoch 841/1000, Iterations 42060 | Train loss 2.14347\n",
      "Epoch 841/1000, Iterations 42080 | Train loss 2.16463\n",
      "Epoch 841/1000, Iterations 42100 | Train loss 2.08173\n",
      "Epoch 842/1000, Iterations 42120 | Train loss 2.13721\n",
      "Epoch 842/1000, Iterations 42140 | Train loss 2.17822\n",
      "Epoch 843/1000, Iterations 42160 | Train loss 2.22890\n",
      "Epoch 843/1000, Iterations 42180 | Train loss 2.18692\n",
      "Epoch 843/1000, Iterations 42200 | Train loss 2.05008\n",
      "Epoch 844/1000, Iterations 42220 | Train loss 2.18115\n",
      "Epoch 844/1000, Iterations 42240 | Train loss 2.22322\n",
      "Epoch 845/1000, Iterations 42260 | Train loss 2.18123\n",
      "Epoch 845/1000, Iterations 42280 | Train loss 2.19291\n",
      "Epoch 845/1000, Iterations 42300 | Train loss 2.09585\n",
      "Epoch 846/1000, Iterations 42320 | Train loss 2.14795\n",
      "Epoch 846/1000, Iterations 42340 | Train loss 2.18115\n",
      "Epoch 847/1000, Iterations 42360 | Train loss 2.26790\n",
      "Epoch 847/1000, Iterations 42380 | Train loss 2.13927\n",
      "Epoch 847/1000, Iterations 42400 | Train loss 2.04098\n",
      "Epoch 848/1000, Iterations 42420 | Train loss 2.18094\n",
      "Epoch 848/1000, Iterations 42440 | Train loss 2.18588\n",
      "Epoch 849/1000, Iterations 42460 | Train loss 2.17882\n",
      "Epoch 849/1000, Iterations 42480 | Train loss 2.22657\n",
      "Epoch 849/1000, Iterations 42500 | Train loss 2.04692\n",
      "Epoch 850/1000, Iterations 42520 | Train loss 2.17544\n",
      "Epoch 850/1000, Iterations 42540 | Train loss 2.20001\n",
      "Epoch 851/1000, Iterations 42560 | Train loss 2.19915\n",
      "Epoch 851/1000, Iterations 42580 | Train loss 2.17331\n",
      "Epoch 851/1000, Iterations 42600 | Train loss 2.08254\n",
      "Epoch 852/1000, Iterations 42620 | Train loss 2.14670\n",
      "Epoch 852/1000, Iterations 42640 | Train loss 2.21232\n",
      "Epoch 853/1000, Iterations 42660 | Train loss 2.15454\n",
      "Epoch 853/1000, Iterations 42680 | Train loss 2.23875\n",
      "Epoch 853/1000, Iterations 42700 | Train loss 2.07889\n",
      "Epoch 854/1000, Iterations 42720 | Train loss 2.19252\n",
      "Epoch 854/1000, Iterations 42740 | Train loss 2.16854\n",
      "Epoch 855/1000, Iterations 42760 | Train loss 2.20259\n",
      "Epoch 855/1000, Iterations 42780 | Train loss 2.18984\n",
      "Epoch 855/1000, Iterations 42800 | Train loss 2.11472\n",
      "Epoch 856/1000, Iterations 42820 | Train loss 2.16089\n",
      "Epoch 856/1000, Iterations 42840 | Train loss 2.19926\n",
      "Epoch 857/1000, Iterations 42860 | Train loss 2.19419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 857/1000, Iterations 42880 | Train loss 2.18304\n",
      "Epoch 857/1000, Iterations 42900 | Train loss 2.09865\n",
      "Epoch 858/1000, Iterations 42920 | Train loss 2.20073\n",
      "Epoch 858/1000, Iterations 42940 | Train loss 2.17642\n",
      "Epoch 859/1000, Iterations 42960 | Train loss 2.12409\n",
      "Epoch 859/1000, Iterations 42980 | Train loss 2.18583\n",
      "Epoch 859/1000, Iterations 43000 | Train loss 2.08967\n",
      "Epoch 860/1000, Iterations 43020 | Train loss 2.13610\n",
      "Epoch 860/1000, Iterations 43040 | Train loss 2.19738\n",
      "Epoch 861/1000, Iterations 43060 | Train loss 2.14939\n",
      "Epoch 861/1000, Iterations 43080 | Train loss 2.21732\n",
      "Epoch 861/1000, Iterations 43100 | Train loss 2.07107\n",
      "Epoch 862/1000, Iterations 43120 | Train loss 2.14134\n",
      "Epoch 862/1000, Iterations 43140 | Train loss 2.19443\n",
      "Epoch 863/1000, Iterations 43160 | Train loss 2.18781\n",
      "Epoch 863/1000, Iterations 43180 | Train loss 2.14950\n",
      "Epoch 863/1000, Iterations 43200 | Train loss 2.09575\n",
      "Epoch 864/1000, Iterations 43220 | Train loss 2.17383\n",
      "Epoch 864/1000, Iterations 43240 | Train loss 2.14830\n",
      "Epoch 865/1000, Iterations 43260 | Train loss 2.22287\n",
      "Epoch 865/1000, Iterations 43280 | Train loss 2.21583\n",
      "Epoch 865/1000, Iterations 43300 | Train loss 2.10277\n",
      "Epoch 866/1000, Iterations 43320 | Train loss 2.12215\n",
      "Epoch 866/1000, Iterations 43340 | Train loss 2.17789\n",
      "Epoch 867/1000, Iterations 43360 | Train loss 2.17162\n",
      "Epoch 867/1000, Iterations 43380 | Train loss 2.22342\n",
      "Epoch 867/1000, Iterations 43400 | Train loss 2.14073\n",
      "Epoch 868/1000, Iterations 43420 | Train loss 2.20360\n",
      "Epoch 868/1000, Iterations 43440 | Train loss 2.20891\n",
      "Epoch 869/1000, Iterations 43460 | Train loss 2.17723\n",
      "Epoch 869/1000, Iterations 43480 | Train loss 2.19562\n",
      "Epoch 869/1000, Iterations 43500 | Train loss 2.09861\n",
      "Epoch 870/1000, Iterations 43520 | Train loss 2.18050\n",
      "Epoch 870/1000, Iterations 43540 | Train loss 2.21523\n",
      "Epoch 871/1000, Iterations 43560 | Train loss 2.19115\n",
      "Epoch 871/1000, Iterations 43580 | Train loss 2.22825\n",
      "Epoch 871/1000, Iterations 43600 | Train loss 2.13127\n",
      "Epoch 872/1000, Iterations 43620 | Train loss 2.12325\n",
      "Epoch 872/1000, Iterations 43640 | Train loss 2.21556\n",
      "Epoch 873/1000, Iterations 43660 | Train loss 2.18000\n",
      "Epoch 873/1000, Iterations 43680 | Train loss 2.21869\n",
      "Epoch 873/1000, Iterations 43700 | Train loss 2.13995\n",
      "Epoch 874/1000, Iterations 43720 | Train loss 2.14685\n",
      "Epoch 874/1000, Iterations 43740 | Train loss 2.20484\n",
      "Epoch 875/1000, Iterations 43760 | Train loss 2.18163\n",
      "Epoch 875/1000, Iterations 43780 | Train loss 2.15760\n",
      "Epoch 875/1000, Iterations 43800 | Train loss 2.09300\n",
      "Epoch 876/1000, Iterations 43820 | Train loss 2.20085\n",
      "Epoch 876/1000, Iterations 43840 | Train loss 2.24679\n",
      "Epoch 877/1000, Iterations 43860 | Train loss 2.19388\n",
      "Epoch 877/1000, Iterations 43880 | Train loss 2.23805\n",
      "Epoch 877/1000, Iterations 43900 | Train loss 2.04320\n",
      "Epoch 878/1000, Iterations 43920 | Train loss 2.20530\n",
      "Epoch 878/1000, Iterations 43940 | Train loss 2.20153\n",
      "Epoch 879/1000, Iterations 43960 | Train loss 2.17949\n",
      "Epoch 879/1000, Iterations 43980 | Train loss 2.21116\n",
      "Epoch 879/1000, Iterations 44000 | Train loss 2.08456\n",
      "Epoch 880/1000, Iterations 44020 | Train loss 2.14256\n",
      "Epoch 880/1000, Iterations 44040 | Train loss 2.15843\n",
      "Epoch 881/1000, Iterations 44060 | Train loss 2.20377\n",
      "Epoch 881/1000, Iterations 44080 | Train loss 2.22863\n",
      "Epoch 881/1000, Iterations 44100 | Train loss 2.12157\n",
      "Epoch 882/1000, Iterations 44120 | Train loss 2.17705\n",
      "Epoch 882/1000, Iterations 44140 | Train loss 2.16083\n",
      "Epoch 883/1000, Iterations 44160 | Train loss 2.13746\n",
      "Epoch 883/1000, Iterations 44180 | Train loss 2.22632\n",
      "Epoch 883/1000, Iterations 44200 | Train loss 2.09095\n",
      "Epoch 884/1000, Iterations 44220 | Train loss 2.18032\n",
      "Epoch 884/1000, Iterations 44240 | Train loss 2.22334\n",
      "Epoch 885/1000, Iterations 44260 | Train loss 2.12878\n",
      "Epoch 885/1000, Iterations 44280 | Train loss 2.17418\n",
      "Epoch 885/1000, Iterations 44300 | Train loss 2.11408\n",
      "Epoch 886/1000, Iterations 44320 | Train loss 2.17541\n",
      "Epoch 886/1000, Iterations 44340 | Train loss 2.17934\n",
      "Epoch 887/1000, Iterations 44360 | Train loss 2.16263\n",
      "Epoch 887/1000, Iterations 44380 | Train loss 2.19086\n",
      "Epoch 887/1000, Iterations 44400 | Train loss 2.03625\n",
      "Epoch 888/1000, Iterations 44420 | Train loss 2.19472\n",
      "Epoch 888/1000, Iterations 44440 | Train loss 2.20959\n",
      "Epoch 889/1000, Iterations 44460 | Train loss 2.22317\n",
      "Epoch 889/1000, Iterations 44480 | Train loss 2.18719\n",
      "Epoch 889/1000, Iterations 44500 | Train loss 2.11111\n",
      "Epoch 890/1000, Iterations 44520 | Train loss 2.21081\n",
      "Epoch 890/1000, Iterations 44540 | Train loss 2.17348\n",
      "Epoch 891/1000, Iterations 44560 | Train loss 2.18730\n",
      "Epoch 891/1000, Iterations 44580 | Train loss 2.20982\n",
      "Epoch 891/1000, Iterations 44600 | Train loss 2.03828\n",
      "Epoch 892/1000, Iterations 44620 | Train loss 2.16146\n",
      "Epoch 892/1000, Iterations 44640 | Train loss 2.14289\n",
      "Epoch 893/1000, Iterations 44660 | Train loss 2.20523\n",
      "Epoch 893/1000, Iterations 44680 | Train loss 2.20199\n",
      "Epoch 893/1000, Iterations 44700 | Train loss 2.09350\n",
      "Epoch 894/1000, Iterations 44720 | Train loss 2.14905\n",
      "Epoch 894/1000, Iterations 44740 | Train loss 2.23552\n",
      "Epoch 895/1000, Iterations 44760 | Train loss 2.16098\n",
      "Epoch 895/1000, Iterations 44780 | Train loss 2.20595\n",
      "Epoch 895/1000, Iterations 44800 | Train loss 2.14757\n",
      "Epoch 896/1000, Iterations 44820 | Train loss 2.19455\n",
      "Epoch 896/1000, Iterations 44840 | Train loss 2.19670\n",
      "Epoch 897/1000, Iterations 44860 | Train loss 2.17035\n",
      "Epoch 897/1000, Iterations 44880 | Train loss 2.17593\n",
      "Epoch 897/1000, Iterations 44900 | Train loss 2.07014\n",
      "Epoch 898/1000, Iterations 44920 | Train loss 2.12943\n",
      "Epoch 898/1000, Iterations 44940 | Train loss 2.13707\n",
      "Epoch 899/1000, Iterations 44960 | Train loss 2.20235\n",
      "Epoch 899/1000, Iterations 44980 | Train loss 2.17585\n",
      "Epoch 899/1000, Iterations 45000 | Train loss 2.08116\n",
      "Epoch 900/1000, Iterations 45020 | Train loss 2.20242\n",
      "Epoch 900/1000, Iterations 45040 | Train loss 2.20481\n",
      "Epoch 901/1000, Iterations 45060 | Train loss 2.15875\n",
      "Epoch 901/1000, Iterations 45080 | Train loss 2.23443\n",
      "Epoch 901/1000, Iterations 45100 | Train loss 2.08001\n",
      "Epoch 902/1000, Iterations 45120 | Train loss 2.15095\n",
      "Epoch 902/1000, Iterations 45140 | Train loss 2.17053\n",
      "Epoch 903/1000, Iterations 45160 | Train loss 2.13047\n",
      "Epoch 903/1000, Iterations 45180 | Train loss 2.16837\n",
      "Epoch 903/1000, Iterations 45200 | Train loss 2.08242\n",
      "Epoch 904/1000, Iterations 45220 | Train loss 2.14636\n",
      "Epoch 904/1000, Iterations 45240 | Train loss 2.14714\n",
      "Epoch 905/1000, Iterations 45260 | Train loss 2.14894\n",
      "Epoch 905/1000, Iterations 45280 | Train loss 2.19700\n",
      "Epoch 905/1000, Iterations 45300 | Train loss 2.11871\n",
      "Epoch 906/1000, Iterations 45320 | Train loss 2.18539\n",
      "Epoch 906/1000, Iterations 45340 | Train loss 2.17383\n",
      "Epoch 907/1000, Iterations 45360 | Train loss 2.16519\n",
      "Epoch 907/1000, Iterations 45380 | Train loss 2.18588\n",
      "Epoch 907/1000, Iterations 45400 | Train loss 2.13060\n",
      "Epoch 908/1000, Iterations 45420 | Train loss 2.13981\n",
      "Epoch 908/1000, Iterations 45440 | Train loss 2.20308\n",
      "Epoch 909/1000, Iterations 45460 | Train loss 2.17593\n",
      "Epoch 909/1000, Iterations 45480 | Train loss 2.20363\n",
      "Epoch 909/1000, Iterations 45500 | Train loss 2.13743\n",
      "Epoch 910/1000, Iterations 45520 | Train loss 2.12409\n",
      "Epoch 910/1000, Iterations 45540 | Train loss 2.21890\n",
      "Epoch 911/1000, Iterations 45560 | Train loss 2.19505\n",
      "Epoch 911/1000, Iterations 45580 | Train loss 2.17632\n",
      "Epoch 911/1000, Iterations 45600 | Train loss 2.07119\n",
      "Epoch 912/1000, Iterations 45620 | Train loss 2.11825\n",
      "Epoch 912/1000, Iterations 45640 | Train loss 2.21270\n",
      "Epoch 913/1000, Iterations 45660 | Train loss 2.21082\n",
      "Epoch 913/1000, Iterations 45680 | Train loss 2.22309\n",
      "Epoch 913/1000, Iterations 45700 | Train loss 2.10998\n",
      "Epoch 914/1000, Iterations 45720 | Train loss 2.14088\n",
      "Epoch 914/1000, Iterations 45740 | Train loss 2.20416\n",
      "Epoch 915/1000, Iterations 45760 | Train loss 2.15255\n",
      "Epoch 915/1000, Iterations 45780 | Train loss 2.20994\n",
      "Epoch 915/1000, Iterations 45800 | Train loss 2.09968\n",
      "Epoch 916/1000, Iterations 45820 | Train loss 2.14884\n",
      "Epoch 916/1000, Iterations 45840 | Train loss 2.20852\n",
      "Epoch 917/1000, Iterations 45860 | Train loss 2.18497\n",
      "Epoch 917/1000, Iterations 45880 | Train loss 2.19886\n",
      "Epoch 917/1000, Iterations 45900 | Train loss 2.08368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 918/1000, Iterations 45920 | Train loss 2.16219\n",
      "Epoch 918/1000, Iterations 45940 | Train loss 2.22379\n",
      "Epoch 919/1000, Iterations 45960 | Train loss 2.21494\n",
      "Epoch 919/1000, Iterations 45980 | Train loss 2.17346\n",
      "Epoch 919/1000, Iterations 46000 | Train loss 2.09457\n",
      "Epoch 920/1000, Iterations 46020 | Train loss 2.10048\n",
      "Epoch 920/1000, Iterations 46040 | Train loss 2.13729\n",
      "Epoch 921/1000, Iterations 46060 | Train loss 2.14604\n",
      "Epoch 921/1000, Iterations 46080 | Train loss 2.17873\n",
      "Epoch 921/1000, Iterations 46100 | Train loss 2.05427\n",
      "Epoch 922/1000, Iterations 46120 | Train loss 2.11770\n",
      "Epoch 922/1000, Iterations 46140 | Train loss 2.19604\n",
      "Epoch 923/1000, Iterations 46160 | Train loss 2.13917\n",
      "Epoch 923/1000, Iterations 46180 | Train loss 2.20017\n",
      "Epoch 923/1000, Iterations 46200 | Train loss 2.04211\n",
      "Epoch 924/1000, Iterations 46220 | Train loss 2.17413\n",
      "Epoch 924/1000, Iterations 46240 | Train loss 2.23399\n",
      "Epoch 925/1000, Iterations 46260 | Train loss 2.16899\n",
      "Epoch 925/1000, Iterations 46280 | Train loss 2.20299\n",
      "Epoch 925/1000, Iterations 46300 | Train loss 2.09082\n",
      "Epoch 926/1000, Iterations 46320 | Train loss 2.16186\n",
      "Epoch 926/1000, Iterations 46340 | Train loss 2.15032\n",
      "Epoch 927/1000, Iterations 46360 | Train loss 2.20707\n",
      "Epoch 927/1000, Iterations 46380 | Train loss 2.17060\n",
      "Epoch 927/1000, Iterations 46400 | Train loss 2.06086\n",
      "Epoch 928/1000, Iterations 46420 | Train loss 2.16952\n",
      "Epoch 928/1000, Iterations 46440 | Train loss 2.22128\n",
      "Epoch 929/1000, Iterations 46460 | Train loss 2.16606\n",
      "Epoch 929/1000, Iterations 46480 | Train loss 2.16413\n",
      "Epoch 929/1000, Iterations 46500 | Train loss 2.10810\n",
      "Epoch 930/1000, Iterations 46520 | Train loss 2.14479\n",
      "Epoch 930/1000, Iterations 46540 | Train loss 2.15761\n",
      "Epoch 931/1000, Iterations 46560 | Train loss 2.16217\n",
      "Epoch 931/1000, Iterations 46580 | Train loss 2.18583\n",
      "Epoch 931/1000, Iterations 46600 | Train loss 2.06380\n",
      "Epoch 932/1000, Iterations 46620 | Train loss 2.15639\n",
      "Epoch 932/1000, Iterations 46640 | Train loss 2.17720\n",
      "Epoch 933/1000, Iterations 46660 | Train loss 2.19372\n",
      "Epoch 933/1000, Iterations 46680 | Train loss 2.18552\n",
      "Epoch 933/1000, Iterations 46700 | Train loss 2.11985\n",
      "Epoch 934/1000, Iterations 46720 | Train loss 2.14446\n",
      "Epoch 934/1000, Iterations 46740 | Train loss 2.22074\n",
      "Epoch 935/1000, Iterations 46760 | Train loss 2.17961\n",
      "Epoch 935/1000, Iterations 46780 | Train loss 2.21036\n",
      "Epoch 935/1000, Iterations 46800 | Train loss 2.10378\n",
      "Epoch 936/1000, Iterations 46820 | Train loss 2.14817\n",
      "Epoch 936/1000, Iterations 46840 | Train loss 2.21012\n",
      "Epoch 937/1000, Iterations 46860 | Train loss 2.16018\n",
      "Epoch 937/1000, Iterations 46880 | Train loss 2.18923\n",
      "Epoch 937/1000, Iterations 46900 | Train loss 2.10348\n",
      "Epoch 938/1000, Iterations 46920 | Train loss 2.15356\n",
      "Epoch 938/1000, Iterations 46940 | Train loss 2.21001\n",
      "Epoch 939/1000, Iterations 46960 | Train loss 2.14232\n",
      "Epoch 939/1000, Iterations 46980 | Train loss 2.15574\n",
      "Epoch 939/1000, Iterations 47000 | Train loss 2.05928\n",
      "Epoch 940/1000, Iterations 47020 | Train loss 2.15647\n",
      "Epoch 940/1000, Iterations 47040 | Train loss 2.17970\n",
      "Epoch 941/1000, Iterations 47060 | Train loss 2.14746\n",
      "Epoch 941/1000, Iterations 47080 | Train loss 2.18408\n",
      "Epoch 941/1000, Iterations 47100 | Train loss 2.09391\n",
      "Epoch 942/1000, Iterations 47120 | Train loss 2.15457\n",
      "Epoch 942/1000, Iterations 47140 | Train loss 2.21630\n",
      "Epoch 943/1000, Iterations 47160 | Train loss 2.18269\n",
      "Epoch 943/1000, Iterations 47180 | Train loss 2.19419\n",
      "Epoch 943/1000, Iterations 47200 | Train loss 2.11233\n",
      "Epoch 944/1000, Iterations 47220 | Train loss 2.22225\n",
      "Epoch 944/1000, Iterations 47240 | Train loss 2.14010\n",
      "Epoch 945/1000, Iterations 47260 | Train loss 2.16930\n",
      "Epoch 945/1000, Iterations 47280 | Train loss 2.22117\n",
      "Epoch 945/1000, Iterations 47300 | Train loss 2.09602\n",
      "Epoch 946/1000, Iterations 47320 | Train loss 2.11977\n",
      "Epoch 946/1000, Iterations 47340 | Train loss 2.20493\n",
      "Epoch 947/1000, Iterations 47360 | Train loss 2.22706\n",
      "Epoch 947/1000, Iterations 47380 | Train loss 2.19867\n",
      "Epoch 947/1000, Iterations 47400 | Train loss 2.08129\n",
      "Epoch 948/1000, Iterations 47420 | Train loss 2.17469\n",
      "Epoch 948/1000, Iterations 47440 | Train loss 2.19345\n",
      "Epoch 949/1000, Iterations 47460 | Train loss 2.20924\n",
      "Epoch 949/1000, Iterations 47480 | Train loss 2.19730\n",
      "Epoch 949/1000, Iterations 47500 | Train loss 2.05043\n",
      "Epoch 950/1000, Iterations 47520 | Train loss 2.19113\n",
      "Epoch 950/1000, Iterations 47540 | Train loss 2.16933\n",
      "Epoch 951/1000, Iterations 47560 | Train loss 2.16974\n",
      "Epoch 951/1000, Iterations 47580 | Train loss 2.19283\n",
      "Epoch 951/1000, Iterations 47600 | Train loss 2.07439\n",
      "Epoch 952/1000, Iterations 47620 | Train loss 2.15807\n",
      "Epoch 952/1000, Iterations 47640 | Train loss 2.16762\n",
      "Epoch 953/1000, Iterations 47660 | Train loss 2.14351\n",
      "Epoch 953/1000, Iterations 47680 | Train loss 2.16102\n",
      "Epoch 953/1000, Iterations 47700 | Train loss 2.02953\n",
      "Epoch 954/1000, Iterations 47720 | Train loss 2.11500\n",
      "Epoch 954/1000, Iterations 47740 | Train loss 2.20773\n",
      "Epoch 955/1000, Iterations 47760 | Train loss 2.19592\n",
      "Epoch 955/1000, Iterations 47780 | Train loss 2.23495\n",
      "Epoch 955/1000, Iterations 47800 | Train loss 2.10617\n",
      "Epoch 956/1000, Iterations 47820 | Train loss 2.16890\n",
      "Epoch 956/1000, Iterations 47840 | Train loss 2.14879\n",
      "Epoch 957/1000, Iterations 47860 | Train loss 2.17571\n",
      "Epoch 957/1000, Iterations 47880 | Train loss 2.14465\n",
      "Epoch 957/1000, Iterations 47900 | Train loss 2.11341\n",
      "Epoch 958/1000, Iterations 47920 | Train loss 2.13743\n",
      "Epoch 958/1000, Iterations 47940 | Train loss 2.18896\n",
      "Epoch 959/1000, Iterations 47960 | Train loss 2.20549\n",
      "Epoch 959/1000, Iterations 47980 | Train loss 2.26833\n",
      "Epoch 959/1000, Iterations 48000 | Train loss 2.07008\n",
      "Epoch 960/1000, Iterations 48020 | Train loss 2.11060\n",
      "Epoch 960/1000, Iterations 48040 | Train loss 2.23075\n",
      "Epoch 961/1000, Iterations 48060 | Train loss 2.15016\n",
      "Epoch 961/1000, Iterations 48080 | Train loss 2.20900\n",
      "Epoch 961/1000, Iterations 48100 | Train loss 2.15382\n",
      "Epoch 962/1000, Iterations 48120 | Train loss 2.14279\n",
      "Epoch 962/1000, Iterations 48140 | Train loss 2.22491\n",
      "Epoch 963/1000, Iterations 48160 | Train loss 2.17866\n",
      "Epoch 963/1000, Iterations 48180 | Train loss 2.16434\n",
      "Epoch 963/1000, Iterations 48200 | Train loss 2.08417\n",
      "Epoch 964/1000, Iterations 48220 | Train loss 2.17905\n",
      "Epoch 964/1000, Iterations 48240 | Train loss 2.19288\n",
      "Epoch 965/1000, Iterations 48260 | Train loss 2.19791\n",
      "Epoch 965/1000, Iterations 48280 | Train loss 2.19686\n",
      "Epoch 965/1000, Iterations 48300 | Train loss 2.06166\n",
      "Epoch 966/1000, Iterations 48320 | Train loss 2.15122\n",
      "Epoch 966/1000, Iterations 48340 | Train loss 2.22708\n",
      "Epoch 967/1000, Iterations 48360 | Train loss 2.14049\n",
      "Epoch 967/1000, Iterations 48380 | Train loss 2.17104\n",
      "Epoch 967/1000, Iterations 48400 | Train loss 2.09372\n",
      "Epoch 968/1000, Iterations 48420 | Train loss 2.12539\n",
      "Epoch 968/1000, Iterations 48440 | Train loss 2.16875\n",
      "Epoch 969/1000, Iterations 48460 | Train loss 2.20545\n",
      "Epoch 969/1000, Iterations 48480 | Train loss 2.16048\n",
      "Epoch 969/1000, Iterations 48500 | Train loss 2.08336\n",
      "Epoch 970/1000, Iterations 48520 | Train loss 2.19676\n",
      "Epoch 970/1000, Iterations 48540 | Train loss 2.15488\n",
      "Epoch 971/1000, Iterations 48560 | Train loss 2.19572\n",
      "Epoch 971/1000, Iterations 48580 | Train loss 2.25919\n",
      "Epoch 971/1000, Iterations 48600 | Train loss 2.09914\n",
      "Epoch 972/1000, Iterations 48620 | Train loss 2.17376\n",
      "Epoch 972/1000, Iterations 48640 | Train loss 2.19939\n",
      "Epoch 973/1000, Iterations 48660 | Train loss 2.20586\n",
      "Epoch 973/1000, Iterations 48680 | Train loss 2.12938\n",
      "Epoch 973/1000, Iterations 48700 | Train loss 2.06364\n",
      "Epoch 974/1000, Iterations 48720 | Train loss 2.18568\n",
      "Epoch 974/1000, Iterations 48740 | Train loss 2.20655\n",
      "Epoch 975/1000, Iterations 48760 | Train loss 2.15672\n",
      "Epoch 975/1000, Iterations 48780 | Train loss 2.16077\n",
      "Epoch 975/1000, Iterations 48800 | Train loss 2.06936\n",
      "Epoch 976/1000, Iterations 48820 | Train loss 2.19051\n",
      "Epoch 976/1000, Iterations 48840 | Train loss 2.21940\n",
      "Epoch 977/1000, Iterations 48860 | Train loss 2.14765\n",
      "Epoch 977/1000, Iterations 48880 | Train loss 2.20422\n",
      "Epoch 977/1000, Iterations 48900 | Train loss 2.06877\n",
      "Epoch 978/1000, Iterations 48920 | Train loss 2.16843\n",
      "Epoch 978/1000, Iterations 48940 | Train loss 2.18386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 979/1000, Iterations 48960 | Train loss 2.18501\n",
      "Epoch 979/1000, Iterations 48980 | Train loss 2.18324\n",
      "Epoch 979/1000, Iterations 49000 | Train loss 2.06587\n",
      "Epoch 980/1000, Iterations 49020 | Train loss 2.19685\n",
      "Epoch 980/1000, Iterations 49040 | Train loss 2.13610\n",
      "Epoch 981/1000, Iterations 49060 | Train loss 2.16487\n",
      "Epoch 981/1000, Iterations 49080 | Train loss 2.25273\n",
      "Epoch 981/1000, Iterations 49100 | Train loss 2.07395\n",
      "Epoch 982/1000, Iterations 49120 | Train loss 2.17334\n",
      "Epoch 982/1000, Iterations 49140 | Train loss 2.20684\n",
      "Epoch 983/1000, Iterations 49160 | Train loss 2.19345\n",
      "Epoch 983/1000, Iterations 49180 | Train loss 2.20983\n",
      "Epoch 983/1000, Iterations 49200 | Train loss 2.13374\n",
      "Epoch 984/1000, Iterations 49220 | Train loss 2.11584\n",
      "Epoch 984/1000, Iterations 49240 | Train loss 2.17281\n",
      "Epoch 985/1000, Iterations 49260 | Train loss 2.15907\n",
      "Epoch 985/1000, Iterations 49280 | Train loss 2.17706\n",
      "Epoch 985/1000, Iterations 49300 | Train loss 2.10040\n",
      "Epoch 986/1000, Iterations 49320 | Train loss 2.17389\n",
      "Epoch 986/1000, Iterations 49340 | Train loss 2.18728\n",
      "Epoch 987/1000, Iterations 49360 | Train loss 2.16056\n",
      "Epoch 987/1000, Iterations 49380 | Train loss 2.18376\n",
      "Epoch 987/1000, Iterations 49400 | Train loss 2.08253\n",
      "Epoch 988/1000, Iterations 49420 | Train loss 2.11222\n",
      "Epoch 988/1000, Iterations 49440 | Train loss 2.15758\n",
      "Epoch 989/1000, Iterations 49460 | Train loss 2.15058\n",
      "Epoch 989/1000, Iterations 49480 | Train loss 2.15662\n",
      "Epoch 989/1000, Iterations 49500 | Train loss 2.06500\n",
      "Epoch 990/1000, Iterations 49520 | Train loss 2.13587\n",
      "Epoch 990/1000, Iterations 49540 | Train loss 2.18720\n",
      "Epoch 991/1000, Iterations 49560 | Train loss 2.15656\n",
      "Epoch 991/1000, Iterations 49580 | Train loss 2.16218\n",
      "Epoch 991/1000, Iterations 49600 | Train loss 2.06691\n",
      "Epoch 992/1000, Iterations 49620 | Train loss 2.15642\n",
      "Epoch 992/1000, Iterations 49640 | Train loss 2.20131\n",
      "Epoch 993/1000, Iterations 49660 | Train loss 2.18044\n",
      "Epoch 993/1000, Iterations 49680 | Train loss 2.16569\n",
      "Epoch 993/1000, Iterations 49700 | Train loss 2.04531\n",
      "Epoch 994/1000, Iterations 49720 | Train loss 2.07651\n",
      "Epoch 994/1000, Iterations 49740 | Train loss 2.23260\n",
      "Epoch 995/1000, Iterations 49760 | Train loss 2.11583\n",
      "Epoch 995/1000, Iterations 49780 | Train loss 2.17962\n",
      "Epoch 995/1000, Iterations 49800 | Train loss 2.09567\n",
      "Epoch 996/1000, Iterations 49820 | Train loss 2.17169\n",
      "Epoch 996/1000, Iterations 49840 | Train loss 2.23118\n",
      "Epoch 997/1000, Iterations 49860 | Train loss 2.16538\n",
      "Epoch 997/1000, Iterations 49880 | Train loss 2.17026\n",
      "Epoch 997/1000, Iterations 49900 | Train loss 2.07788\n",
      "Epoch 998/1000, Iterations 49920 | Train loss 2.13148\n",
      "Epoch 998/1000, Iterations 49940 | Train loss 2.19407\n",
      "Epoch 999/1000, Iterations 49960 | Train loss 2.16072\n",
      "Epoch 999/1000, Iterations 49980 | Train loss 2.16239\n",
      "Epoch 999/1000, Iterations 50000 | Train loss 2.09555\n"
     ]
    }
   ],
   "source": [
    "model.train(train_x, train_y, n_epochs=1000, ckpt_dir=\"C:/Users/p0ng5/Desktop/rnn_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 5000), (32, 5000))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " << y_onehot >>  Tensor(\"one_hot_1:0\", shape=(1, 1, 65), dtype=float32)\n",
      "  << lstm_outputs  >>  Tensor(\"rnn/transpose:0\", shape=(1, 1, 128), dtype=float32)\n",
      " << logits >>  Tensor(\"logits/BiasAdd:0\", shape=(1, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model_sampling = CharRnn(n_classes=len(char_set), sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/p0ng5/Desktop/rnn_model/char_model-50000.ckpt\n"
     ]
    }
   ],
   "source": [
    "sample_text = model_sampling.sample(output_length=500, ckpt_dir=model.ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Snaretin hirs\n",
      "\n",
      "   Ham. Ild mell, you will nom to himsence?\n",
      "  Ham. The Maunerin withing that is io soued my\n",
      "hine\n",
      "Touee as the wirce of his like and the wine of his Seater,\n",
      "As by his Sande, and whencestie of him to my.\n",
      "Thet he do not that he shoued then world. I might better.\n",
      "All wise the worting sinte and the mateer path,\n",
      "Ah the euinke on my saces woued hime me alfene There\n",
      "It heares\n",
      "   Ham. Hill the Coopers, and shath my Lord\n",
      "\n",
      "   Ham. Why thue my Lord, I haue brauerit my Lord\n",
      "\n",
      "   Ham. Why see me \n"
     ]
    }
   ],
   "source": [
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Procembin to the Project (oftentess. of aresendesters, for woredis indited tores,ofints aroted toothe\n",
      "To tex that panders it tomsied forrance of this ancerier (of in it and fanmers of the prentecters/disites.  f our asents ones in if in our things and comsace of the pentrestiof\n",
      "of mast.\n",
      "\n",
      "Starse arices, and pookicinien arofe on copsed an on otsecitest to the sumbeste of areast parsocs if of mokestin on ar and thithen on on shosedanding.\n",
      "         *  *]]  TO  UUEDORERADAAGENG-TI YoU IT NOPIIN DOMEST\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  it thile so do sente,\n",
      "That tare sean anee in a torse at toue oo the soone soreet tie toores,\n",
      "A   King. That a dildothis asariou,\n",
      "Tee se a  ine the ener and homestie oon iole\n",
      "\n",
      "   Ham. Ilat ou hathese to doue himes and orese antere, An  hat, one ar hild shere, If moue, on than in thane,\n",
      "And shale the heare tit hes andot a thas arae a dang,\n",
      "and wol  it in santise oo tie sith,\n",
      "As  or whoue  onteet if mather\n",
      "Ant this is tee e miserses and the mate,\n",
      "Ard wo hou te har iot to mesthe at or heres to he me\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/p0ng5/Desktop/rnn_model/char_model-2700.ckpt'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
