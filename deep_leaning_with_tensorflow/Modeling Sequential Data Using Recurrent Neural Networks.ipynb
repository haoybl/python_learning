{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_path = 'C:/Users/p0ng5/OneDrive/movie_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reviews_path = 'C:/Users/LENOVO/Desktop/movie_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv(reviews_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nothing is fantastic! Simple as that! It's a f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This really was a waste of time...the movie ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This might sound weird, but I only got to see ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Considering this film was released 8 years bef...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very rarely does one come across an indie come...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Nothing is fantastic! Simple as that! It's a f...          1\n",
       "1  This really was a waste of time...the movie ha...          0\n",
       "2  This might sound weird, but I only got to see ...          0\n",
       "3  Considering this film was released 8 years bef...          1\n",
       "4  Very rarely does one come across an indie come...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I Sell the Dead is a big, sloppy horror comedy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>I know this sounds odd coming from someone bor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>OK I had higher hopes for this Carnosaur movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Laurence Fishburne is a fine actor, and deserv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I lived in Tokyo for 7 months. Knowing the rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  I Sell the Dead is a big, sloppy horror comedy...          1\n",
       "49996  I know this sounds odd coming from someone bor...          1\n",
       "49997  OK I had higher hopes for this Carnosaur movie...          0\n",
       "49998  Laurence Fishburne is a fine actor, and deserv...          1\n",
       "49999  I lived in Tokyo for 7 months. Knowing the rea...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count unique words\n",
    "counter = Counter()\n",
    "\n",
    "# clean text\n",
    "for i, review in enumerate(movie_data['review']):\n",
    "    \n",
    "    text = \"\".join(c if c not in punctuation else \" {} \".format(c) for c in review).lower()\n",
    "    movie_data.iloc[i, 0] = text\n",
    "    \n",
    "    counter.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2int dictionary\n",
    "word2int = {w:i for i, w in enumerate(sorted(counter, key=counter.get, reverse=True), start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# int-list reviews\n",
    "mapped_reviews = []\n",
    "for review in movie_data['review']:\n",
    "    mapped_reviews.append([word2int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 200\n",
    "\n",
    "# if length < sequence_length : left padd with zeros\n",
    "# if length > sequence_length : take last 'sequence_length' elements\n",
    "\n",
    "# padded sequence\n",
    "sequences = np.zeros(shape=(len(movie_data), sequence_length), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, mapped_review in enumerate(mapped_reviews):\n",
    "    n = len(mapped_review)\n",
    "    if n < sequence_length:\n",
    "        sequences[i, -n:] = mapped_review\n",
    "    else:\n",
    "        sequences[i, :] = mapped_review[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 200), (25000,), (25000, 200), (25000,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = sequences[:25000, :]\n",
    "y_train = movie_data.iloc[:25000, 1].values\n",
    "\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = movie_data.iloc[25000:, 1].values\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12500, 200), (12500,), (12500, 200), (12500,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = X_test[:12500]\n",
    "y_val = y_test[:12500]\n",
    "X_test = X_test[12500:]\n",
    "y_test = y_test[12500:]\n",
    "\n",
    "X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y=None, batch_size=64):\n",
    "    if y is not None:\n",
    "        assert len(X) == len(y)\n",
    "    n_batch = len(X) // batch_size\n",
    "    for i in range(n_batch):\n",
    "        a = i*batch_size\n",
    "        b = (i+1)*batch_size\n",
    "        \n",
    "        if y is not None:\n",
    "            yield X[a:b], y[a:b]\n",
    "        else:\n",
    "            yield X[a:b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN:\n",
    "    \n",
    "    def __init__(self, word_size, embed_size=200, lstm_size=256, num_layer=1,\n",
    "                seq_length=200, learning_rate=1e-4, batch_size=32):\n",
    "        # model hyper parameters\n",
    "        self.word_size = word_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layer = num_layer\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # build model graph\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            #tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    def build(self):\n",
    "        # placeholder for inputs\n",
    "        tf_x = tf.placeholder(dtype=tf.int32, \n",
    "                              shape=(self.batch_size, self.seq_length), \n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(dtype=tf.float32, \n",
    "                              shape=(self.batch_size), \n",
    "                              name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(dtype=tf.float32, shape=(), \n",
    "                                     name='tf_keepprob')\n",
    "        # embedding vector\n",
    "        W_embedding = tf.Variable(\n",
    "            tf.random_uniform(shape=(self.word_size, self.embed_size), minval=-1, maxval=1),\n",
    "            name='W_embedding')\n",
    "        \n",
    "        embed_x = tf.nn.embedding_lookup(W_embedding, tf_x, name='embed_x')\n",
    "        \n",
    "        # create rnn cell\n",
    "        cells = tf.contrib.rnn.MultiRNNCell([\n",
    "            tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                                         output_keep_prob=tf_keepprob)\n",
    "                                          for i in range(self.num_layer)\n",
    "        ])\n",
    "        \n",
    "        # define the initial state/ rnn steps\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print(\"  << initial state > \", self.initial_state)\n",
    "        \n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cell=cells, inputs=embed_x, \n",
    "                                                           initial_state=self.initial_state)\n",
    "        print(\"\\n << lstm_output >> \", lstm_outputs)\n",
    "        print(\"\\n << final state >> \", self.final_state)\n",
    "        \n",
    "        # dense layer -> logits\n",
    "        logits = tf.layers.dense(inputs=lstm_outputs[:, -1], units=1, \n",
    "                                 activation=None, name='logits')\n",
    "        \n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print('\\n  << logits     >> ', logits)\n",
    "        \n",
    "        # predictions -> prob. | labels\n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        y_labels = tf.cast(tf.round(y_proba), dtype=tf.int32, name='labels')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels': y_labels\n",
    "        }\n",
    "        print(\"\\n << predictions  >> \", predictions)\n",
    "        \n",
    "        # cost function\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, \n",
    "                                                                      logits=logits), \n",
    "                              name='cost')\n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "    \n",
    "    def train(self, X, y, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            \n",
    "            batch_total = 0\n",
    "            for epoch in range(num_epochs):\n",
    "                # reset cell&hidden states\n",
    "                state = sess.run(self.initial_state)\n",
    "                for x_batch, y_batch in batch_generator(X, y, batch_size=self.batch_size):\n",
    "                    feed = {'tf_x:0': x_batch,\n",
    "                            'tf_y:0': y_batch,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state: state}\n",
    "                    loss, _, state = sess.run(['cost:0', 'train_op', self.final_state], \n",
    "                                              feed_dict=feed)\n",
    "                    \n",
    "                    # update training every 20 batches\n",
    "                    batch_total += 1\n",
    "                    if (batch_total+1) % 20 == 0:\n",
    "                        print(\"Epoch {:3d}, Iterations {:4d} | Train loss: {:.4f}\".format(epoch+1, \n",
    "                                                                                            batch_total+1,\n",
    "                                                                                            loss))\n",
    "                # save every 10 epochs                                                                          \n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    self.saver.save(sess, \n",
    "                                    'model/sentiment-rnn-{}.ckpt'.format(epoch+1))\n",
    "    \n",
    "    def predict(self, X, predict_proba=False):\n",
    "        pred = []\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            # restore latest model\n",
    "            self.saver.restore(sess, \n",
    "                               tf.train.latest_checkpoint('./model/'))\n",
    "            \n",
    "            # init model states\n",
    "            state = sess.run(self.initial_state)\n",
    "            \n",
    "            for x_batch in batch_generator(X, y=None, batch_size=self.batch_size):\n",
    "                feed = {'tf_x:0': x_batch,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: state}\n",
    "                \n",
    "                if predict_proba:\n",
    "                    y_pred, state = sess.run(['probabilities:0', self.final_state], \n",
    "                                           feed_dict=feed)\n",
    "                else:\n",
    "                    y_pred, state = sess.run(['labels:0', self.final_state], \n",
    "                                            feed_dict=feed)\n",
    "                pred.append(y_pred)\n",
    "                \n",
    "        return np.concatenate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2int' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5d252e7a0607>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mword_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2int\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0membed_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mn_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word2int' is not defined"
     ]
    }
   ],
   "source": [
    "sequence_length = 200\n",
    "word_size = max(word2int.values()) + 1\n",
    "embed_size = 256\n",
    "hidden_size = 128\n",
    "n_layer = 1\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = SentimentRNN(word_size=word_size, embed_size=embed_size, lstm_size=hidden_size, \n",
    "                     num_layer=n_layer, seq_length=sequence_length, batch_size=batch_size, \n",
    "                     learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1, Iterations   20 | Train loss: 0.6689\n",
      "Epoch   1, Iterations   40 | Train loss: 0.5714\n",
      "Epoch   1, Iterations   60 | Train loss: 0.6164\n",
      "Epoch   1, Iterations   80 | Train loss: 0.6584\n",
      "Epoch   1, Iterations  100 | Train loss: 0.5446\n",
      "Epoch   1, Iterations  120 | Train loss: 0.4987\n",
      "Epoch   1, Iterations  140 | Train loss: 0.4725\n",
      "Epoch   1, Iterations  160 | Train loss: 0.4996\n",
      "Epoch   1, Iterations  180 | Train loss: 0.5622\n",
      "Epoch   1, Iterations  200 | Train loss: 0.5063\n",
      "Epoch   1, Iterations  220 | Train loss: 0.4509\n",
      "Epoch   1, Iterations  240 | Train loss: 0.4027\n",
      "Epoch   2, Iterations  260 | Train loss: 0.5039\n",
      "Epoch   2, Iterations  280 | Train loss: 0.3833\n",
      "Epoch   2, Iterations  300 | Train loss: 0.4192\n",
      "Epoch   2, Iterations  320 | Train loss: 0.4525\n",
      "Epoch   2, Iterations  340 | Train loss: 0.3851\n",
      "Epoch   2, Iterations  360 | Train loss: 0.2664\n",
      "Epoch   2, Iterations  380 | Train loss: 0.2420\n",
      "Epoch   2, Iterations  400 | Train loss: 0.3306\n",
      "Epoch   2, Iterations  420 | Train loss: 0.4068\n",
      "Epoch   2, Iterations  440 | Train loss: 0.2590\n",
      "Epoch   2, Iterations  460 | Train loss: 0.2982\n",
      "Epoch   2, Iterations  480 | Train loss: 0.3188\n",
      "Epoch   2, Iterations  500 | Train loss: 0.2969\n",
      "Epoch   3, Iterations  520 | Train loss: 0.2957\n",
      "Epoch   3, Iterations  540 | Train loss: 0.2063\n",
      "Epoch   3, Iterations  560 | Train loss: 0.2427\n",
      "Epoch   3, Iterations  580 | Train loss: 0.1807\n",
      "Epoch   3, Iterations  600 | Train loss: 0.2052\n",
      "Epoch   3, Iterations  620 | Train loss: 0.2329\n",
      "Epoch   3, Iterations  640 | Train loss: 0.1974\n",
      "Epoch   3, Iterations  660 | Train loss: 0.2323\n",
      "Epoch   3, Iterations  680 | Train loss: 0.2184\n",
      "Epoch   3, Iterations  700 | Train loss: 0.0925\n",
      "Epoch   3, Iterations  720 | Train loss: 0.1635\n",
      "Epoch   3, Iterations  740 | Train loss: 0.2146\n",
      "Epoch   4, Iterations  760 | Train loss: 0.3312\n",
      "Epoch   4, Iterations  780 | Train loss: 0.2712\n",
      "Epoch   4, Iterations  800 | Train loss: 0.2500\n",
      "Epoch   4, Iterations  820 | Train loss: 0.2972\n",
      "Epoch   4, Iterations  840 | Train loss: 0.2335\n",
      "Epoch   4, Iterations  860 | Train loss: 0.1219\n",
      "Epoch   4, Iterations  880 | Train loss: 0.2511\n",
      "Epoch   4, Iterations  900 | Train loss: 0.1651\n",
      "Epoch   4, Iterations  920 | Train loss: 0.2093\n",
      "Epoch   4, Iterations  940 | Train loss: 0.0833\n",
      "Epoch   4, Iterations  960 | Train loss: 0.1050\n",
      "Epoch   4, Iterations  980 | Train loss: 0.1646\n",
      "Epoch   4, Iterations 1000 | Train loss: 0.1692\n",
      "Epoch   5, Iterations 1020 | Train loss: 0.3880\n",
      "Epoch   5, Iterations 1040 | Train loss: 0.1531\n",
      "Epoch   5, Iterations 1060 | Train loss: 0.1094\n",
      "Epoch   5, Iterations 1080 | Train loss: 0.1156\n",
      "Epoch   5, Iterations 1100 | Train loss: 0.0811\n",
      "Epoch   5, Iterations 1120 | Train loss: 0.1029\n",
      "Epoch   5, Iterations 1140 | Train loss: 0.1153\n",
      "Epoch   5, Iterations 1160 | Train loss: 0.1198\n",
      "Epoch   5, Iterations 1180 | Train loss: 0.1971\n",
      "Epoch   5, Iterations 1200 | Train loss: 0.0472\n",
      "Epoch   5, Iterations 1220 | Train loss: 0.0997\n",
      "Epoch   5, Iterations 1240 | Train loss: 0.1078\n",
      "Epoch   6, Iterations 1260 | Train loss: 0.1904\n",
      "Epoch   6, Iterations 1280 | Train loss: 0.1304\n",
      "Epoch   6, Iterations 1300 | Train loss: 0.1088\n",
      "Epoch   6, Iterations 1320 | Train loss: 0.1487\n",
      "Epoch   6, Iterations 1340 | Train loss: 0.0859\n",
      "Epoch   6, Iterations 1360 | Train loss: 0.0600\n",
      "Epoch   6, Iterations 1380 | Train loss: 0.0749\n",
      "Epoch   6, Iterations 1400 | Train loss: 0.2307\n",
      "Epoch   6, Iterations 1420 | Train loss: 0.1084\n",
      "Epoch   6, Iterations 1440 | Train loss: 0.0221\n",
      "Epoch   6, Iterations 1460 | Train loss: 0.0541\n",
      "Epoch   6, Iterations 1480 | Train loss: 0.0610\n",
      "Epoch   6, Iterations 1500 | Train loss: 0.1082\n",
      "Epoch   7, Iterations 1520 | Train loss: 0.0614\n",
      "Epoch   7, Iterations 1540 | Train loss: 0.0430\n",
      "Epoch   7, Iterations 1560 | Train loss: 0.0963\n",
      "Epoch   7, Iterations 1580 | Train loss: 0.0732\n",
      "Epoch   7, Iterations 1600 | Train loss: 0.1442\n",
      "Epoch   7, Iterations 1620 | Train loss: 0.0456\n",
      "Epoch   7, Iterations 1640 | Train loss: 0.0486\n",
      "Epoch   7, Iterations 1660 | Train loss: 0.1843\n",
      "Epoch   7, Iterations 1680 | Train loss: 0.0579\n",
      "Epoch   7, Iterations 1700 | Train loss: 0.0193\n",
      "Epoch   7, Iterations 1720 | Train loss: 0.0657\n",
      "Epoch   7, Iterations 1740 | Train loss: 0.0229\n",
      "Epoch   8, Iterations 1760 | Train loss: 0.1392\n",
      "Epoch   8, Iterations 1780 | Train loss: 0.0941\n",
      "Epoch   8, Iterations 1800 | Train loss: 0.0624\n",
      "Epoch   8, Iterations 1820 | Train loss: 0.0382\n",
      "Epoch   8, Iterations 1840 | Train loss: 0.0512\n",
      "Epoch   8, Iterations 1860 | Train loss: 0.0371\n",
      "Epoch   8, Iterations 1880 | Train loss: 0.0656\n",
      "Epoch   8, Iterations 1900 | Train loss: 0.1075\n",
      "Epoch   8, Iterations 1920 | Train loss: 0.0511\n",
      "Epoch   8, Iterations 1940 | Train loss: 0.0230\n",
      "Epoch   8, Iterations 1960 | Train loss: 0.0330\n",
      "Epoch   8, Iterations 1980 | Train loss: 0.0138\n",
      "Epoch   8, Iterations 2000 | Train loss: 0.0339\n",
      "Epoch   9, Iterations 2020 | Train loss: 0.0189\n",
      "Epoch   9, Iterations 2040 | Train loss: 0.0208\n",
      "Epoch   9, Iterations 2060 | Train loss: 0.0467\n",
      "Epoch   9, Iterations 2080 | Train loss: 0.0128\n",
      "Epoch   9, Iterations 2100 | Train loss: 0.0269\n",
      "Epoch   9, Iterations 2120 | Train loss: 0.0125\n",
      "Epoch   9, Iterations 2140 | Train loss: 0.0117\n",
      "Epoch   9, Iterations 2160 | Train loss: 0.0078\n",
      "Epoch   9, Iterations 2180 | Train loss: 0.0150\n",
      "Epoch   9, Iterations 2200 | Train loss: 0.0038\n",
      "Epoch   9, Iterations 2220 | Train loss: 0.0188\n",
      "Epoch   9, Iterations 2240 | Train loss: 0.0445\n",
      "Epoch  10, Iterations 2260 | Train loss: 0.3463\n",
      "Epoch  10, Iterations 2280 | Train loss: 0.1023\n",
      "Epoch  10, Iterations 2300 | Train loss: 0.0405\n",
      "Epoch  10, Iterations 2320 | Train loss: 0.1190\n",
      "Epoch  10, Iterations 2340 | Train loss: 0.0401\n",
      "Epoch  10, Iterations 2360 | Train loss: 0.0867\n",
      "Epoch  10, Iterations 2380 | Train loss: 0.0572\n",
      "Epoch  10, Iterations 2400 | Train loss: 0.0195\n",
      "Epoch  10, Iterations 2420 | Train loss: 0.0215\n",
      "Epoch  10, Iterations 2440 | Train loss: 0.0304\n",
      "Epoch  10, Iterations 2460 | Train loss: 0.0065\n",
      "Epoch  10, Iterations 2480 | Train loss: 0.0201\n",
      "Epoch  10, Iterations 2500 | Train loss: 0.0957\n",
      "Epoch  11, Iterations 2520 | Train loss: 0.0110\n",
      "Epoch  11, Iterations 2540 | Train loss: 0.0108\n",
      "Epoch  11, Iterations 2560 | Train loss: 0.0401\n",
      "Epoch  11, Iterations 2580 | Train loss: 0.0084\n",
      "Epoch  11, Iterations 2600 | Train loss: 0.0130\n",
      "Epoch  11, Iterations 2620 | Train loss: 0.0226\n",
      "Epoch  11, Iterations 2640 | Train loss: 0.0074\n",
      "Epoch  11, Iterations 2660 | Train loss: 0.0082\n",
      "Epoch  11, Iterations 2680 | Train loss: 0.0050\n",
      "Epoch  11, Iterations 2700 | Train loss: 0.0029\n",
      "Epoch  11, Iterations 2720 | Train loss: 0.0303\n",
      "Epoch  11, Iterations 2740 | Train loss: 0.0026\n",
      "Epoch  12, Iterations 2760 | Train loss: 0.0838\n",
      "Epoch  12, Iterations 2780 | Train loss: 0.0924\n",
      "Epoch  12, Iterations 2800 | Train loss: 0.0153\n",
      "Epoch  12, Iterations 2820 | Train loss: 0.0120\n",
      "Epoch  12, Iterations 2840 | Train loss: 0.0210\n",
      "Epoch  12, Iterations 2860 | Train loss: 0.0094\n",
      "Epoch  12, Iterations 2880 | Train loss: 0.0072\n",
      "Epoch  12, Iterations 2900 | Train loss: 0.0061\n",
      "Epoch  12, Iterations 2920 | Train loss: 0.0074\n",
      "Epoch  12, Iterations 2940 | Train loss: 0.0027\n",
      "Epoch  12, Iterations 2960 | Train loss: 0.0089\n",
      "Epoch  12, Iterations 2980 | Train loss: 0.0014\n",
      "Epoch  12, Iterations 3000 | Train loss: 0.0064\n",
      "Epoch  13, Iterations 3020 | Train loss: 0.0146\n",
      "Epoch  13, Iterations 3040 | Train loss: 0.0070\n",
      "Epoch  13, Iterations 3060 | Train loss: 0.0805\n",
      "Epoch  13, Iterations 3080 | Train loss: 0.0057\n",
      "Epoch  13, Iterations 3100 | Train loss: 0.0045\n",
      "Epoch  13, Iterations 3120 | Train loss: 0.0353\n",
      "Epoch  13, Iterations 3140 | Train loss: 0.0122\n",
      "Epoch  13, Iterations 3160 | Train loss: 0.0062\n",
      "Epoch  13, Iterations 3180 | Train loss: 0.0064\n",
      "Epoch  13, Iterations 3200 | Train loss: 0.0009\n",
      "Epoch  13, Iterations 3220 | Train loss: 0.0217\n",
      "Epoch  13, Iterations 3240 | Train loss: 0.0005\n",
      "Epoch  14, Iterations 3260 | Train loss: 0.1401\n",
      "Epoch  14, Iterations 3280 | Train loss: 0.0209\n",
      "Epoch  14, Iterations 3300 | Train loss: 0.0090\n",
      "Epoch  14, Iterations 3320 | Train loss: 0.0113\n",
      "Epoch  14, Iterations 3340 | Train loss: 0.0009\n",
      "Epoch  14, Iterations 3360 | Train loss: 0.0015\n",
      "Epoch  14, Iterations 3380 | Train loss: 0.0010\n",
      "Epoch  14, Iterations 3400 | Train loss: 0.0011\n",
      "Epoch  14, Iterations 3420 | Train loss: 0.0069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14, Iterations 3440 | Train loss: 0.0023\n",
      "Epoch  14, Iterations 3460 | Train loss: 0.0097\n",
      "Epoch  14, Iterations 3480 | Train loss: 0.0064\n",
      "Epoch  14, Iterations 3500 | Train loss: 0.0044\n",
      "Epoch  15, Iterations 3520 | Train loss: 0.0028\n",
      "Epoch  15, Iterations 3540 | Train loss: 0.0016\n",
      "Epoch  15, Iterations 3560 | Train loss: 0.0060\n",
      "Epoch  15, Iterations 3580 | Train loss: 0.0148\n",
      "Epoch  15, Iterations 3600 | Train loss: 0.0145\n",
      "Epoch  15, Iterations 3620 | Train loss: 0.0009\n",
      "Epoch  15, Iterations 3640 | Train loss: 0.0011\n",
      "Epoch  15, Iterations 3660 | Train loss: 0.0178\n",
      "Epoch  15, Iterations 3680 | Train loss: 0.0008\n",
      "Epoch  15, Iterations 3700 | Train loss: 0.0006\n",
      "Epoch  15, Iterations 3720 | Train loss: 0.0062\n",
      "Epoch  15, Iterations 3740 | Train loss: 0.0729\n",
      "Epoch  16, Iterations 3760 | Train loss: 0.0598\n",
      "Epoch  16, Iterations 3780 | Train loss: 0.0007\n",
      "Epoch  16, Iterations 3800 | Train loss: 0.0042\n",
      "Epoch  16, Iterations 3820 | Train loss: 0.0054\n",
      "Epoch  16, Iterations 3840 | Train loss: 0.0128\n",
      "Epoch  16, Iterations 3860 | Train loss: 0.0031\n",
      "Epoch  16, Iterations 3880 | Train loss: 0.0539\n",
      "Epoch  16, Iterations 3900 | Train loss: 0.0007\n",
      "Epoch  16, Iterations 3920 | Train loss: 0.0104\n",
      "Epoch  16, Iterations 3940 | Train loss: 0.0007\n",
      "Epoch  16, Iterations 3960 | Train loss: 0.0060\n",
      "Epoch  16, Iterations 3980 | Train loss: 0.0850\n",
      "Epoch  16, Iterations 4000 | Train loss: 0.0106\n",
      "Epoch  17, Iterations 4020 | Train loss: 0.2108\n",
      "Epoch  17, Iterations 4040 | Train loss: 0.0200\n",
      "Epoch  17, Iterations 4060 | Train loss: 0.0061\n",
      "Epoch  17, Iterations 4080 | Train loss: 0.0091\n",
      "Epoch  17, Iterations 4100 | Train loss: 0.0039\n",
      "Epoch  17, Iterations 4120 | Train loss: 0.0053\n",
      "Epoch  17, Iterations 4140 | Train loss: 0.0019\n",
      "Epoch  17, Iterations 4160 | Train loss: 0.0011\n",
      "Epoch  17, Iterations 4180 | Train loss: 0.0043\n",
      "Epoch  17, Iterations 4200 | Train loss: 0.1066\n",
      "Epoch  17, Iterations 4220 | Train loss: 0.0276\n",
      "Epoch  17, Iterations 4240 | Train loss: 0.0032\n",
      "Epoch  18, Iterations 4260 | Train loss: 0.0814\n",
      "Epoch  18, Iterations 4280 | Train loss: 0.0027\n",
      "Epoch  18, Iterations 4300 | Train loss: 0.0012\n",
      "Epoch  18, Iterations 4320 | Train loss: 0.0036\n",
      "Epoch  18, Iterations 4340 | Train loss: 0.0063\n",
      "Epoch  18, Iterations 4360 | Train loss: 0.0011\n",
      "Epoch  18, Iterations 4380 | Train loss: 0.0041\n",
      "Epoch  18, Iterations 4400 | Train loss: 0.0027\n",
      "Epoch  18, Iterations 4420 | Train loss: 0.0091\n",
      "Epoch  18, Iterations 4440 | Train loss: 0.0006\n",
      "Epoch  18, Iterations 4460 | Train loss: 0.0014\n",
      "Epoch  18, Iterations 4480 | Train loss: 0.0043\n",
      "Epoch  18, Iterations 4500 | Train loss: 0.0085\n",
      "Epoch  19, Iterations 4520 | Train loss: 0.0044\n",
      "Epoch  19, Iterations 4540 | Train loss: 0.0013\n",
      "Epoch  19, Iterations 4560 | Train loss: 0.0402\n",
      "Epoch  19, Iterations 4580 | Train loss: 0.0026\n",
      "Epoch  19, Iterations 4600 | Train loss: 0.0054\n",
      "Epoch  19, Iterations 4620 | Train loss: 0.0170\n",
      "Epoch  19, Iterations 4640 | Train loss: 0.0186\n",
      "Epoch  19, Iterations 4660 | Train loss: 0.0012\n",
      "Epoch  19, Iterations 4680 | Train loss: 0.0086\n",
      "Epoch  19, Iterations 4700 | Train loss: 0.0005\n",
      "Epoch  19, Iterations 4720 | Train loss: 0.1440\n",
      "Epoch  19, Iterations 4740 | Train loss: 0.0176\n",
      "Epoch  20, Iterations 4760 | Train loss: 0.0838\n",
      "Epoch  20, Iterations 4780 | Train loss: 0.0026\n",
      "Epoch  20, Iterations 4800 | Train loss: 0.0018\n",
      "Epoch  20, Iterations 4820 | Train loss: 0.0526\n",
      "Epoch  20, Iterations 4840 | Train loss: 0.0012\n",
      "Epoch  20, Iterations 4860 | Train loss: 0.0132\n",
      "Epoch  20, Iterations 4880 | Train loss: 0.0013\n",
      "Epoch  20, Iterations 4900 | Train loss: 0.0236\n",
      "Epoch  20, Iterations 4920 | Train loss: 0.0021\n",
      "Epoch  20, Iterations 4940 | Train loss: 0.0006\n",
      "Epoch  20, Iterations 4960 | Train loss: 0.0087\n",
      "Epoch  20, Iterations 4980 | Train loss: 0.0003\n",
      "Epoch  20, Iterations 5000 | Train loss: 0.0224\n",
      "Epoch  21, Iterations 5020 | Train loss: 0.0137\n",
      "Epoch  21, Iterations 5040 | Train loss: 0.0016\n",
      "Epoch  21, Iterations 5060 | Train loss: 0.0008\n",
      "Epoch  21, Iterations 5080 | Train loss: 0.0079\n",
      "Epoch  21, Iterations 5100 | Train loss: 0.0205\n",
      "Epoch  21, Iterations 5120 | Train loss: 0.0057\n",
      "Epoch  21, Iterations 5140 | Train loss: 0.0007\n",
      "Epoch  21, Iterations 5160 | Train loss: 0.0003\n",
      "Epoch  21, Iterations 5180 | Train loss: 0.0939\n",
      "Epoch  21, Iterations 5200 | Train loss: 0.0006\n",
      "Epoch  21, Iterations 5220 | Train loss: 0.0019\n",
      "Epoch  21, Iterations 5240 | Train loss: 0.0029\n",
      "Epoch  22, Iterations 5260 | Train loss: 0.0724\n",
      "Epoch  22, Iterations 5280 | Train loss: 0.0020\n",
      "Epoch  22, Iterations 5300 | Train loss: 0.0046\n",
      "Epoch  22, Iterations 5320 | Train loss: 0.0005\n",
      "Epoch  22, Iterations 5340 | Train loss: 0.0803\n",
      "Epoch  22, Iterations 5360 | Train loss: 0.0165\n",
      "Epoch  22, Iterations 5380 | Train loss: 0.0021\n",
      "Epoch  22, Iterations 5400 | Train loss: 0.0564\n",
      "Epoch  22, Iterations 5420 | Train loss: 0.0016\n",
      "Epoch  22, Iterations 5440 | Train loss: 0.0018\n",
      "Epoch  22, Iterations 5460 | Train loss: 0.0009\n",
      "Epoch  22, Iterations 5480 | Train loss: 0.0008\n",
      "Epoch  22, Iterations 5500 | Train loss: 0.0002\n",
      "Epoch  23, Iterations 5520 | Train loss: 0.0003\n",
      "Epoch  23, Iterations 5540 | Train loss: 0.0005\n",
      "Epoch  23, Iterations 5560 | Train loss: 0.0020\n",
      "Epoch  23, Iterations 5580 | Train loss: 0.0002\n",
      "Epoch  23, Iterations 5600 | Train loss: 0.0050\n",
      "Epoch  23, Iterations 5620 | Train loss: 0.0014\n",
      "Epoch  23, Iterations 5640 | Train loss: 0.0004\n",
      "Epoch  23, Iterations 5660 | Train loss: 0.0004\n",
      "Epoch  23, Iterations 5680 | Train loss: 0.0001\n",
      "Epoch  23, Iterations 5700 | Train loss: 0.0001\n",
      "Epoch  23, Iterations 5720 | Train loss: 0.0007\n",
      "Epoch  23, Iterations 5740 | Train loss: 0.0001\n",
      "Epoch  24, Iterations 5760 | Train loss: 0.0362\n",
      "Epoch  24, Iterations 5780 | Train loss: 0.0001\n",
      "Epoch  24, Iterations 5800 | Train loss: 0.0006\n",
      "Epoch  24, Iterations 5820 | Train loss: 0.0006\n",
      "Epoch  24, Iterations 5840 | Train loss: 0.0002\n",
      "Epoch  24, Iterations 5860 | Train loss: 0.0004\n",
      "Epoch  24, Iterations 5880 | Train loss: 0.0014\n",
      "Epoch  24, Iterations 5900 | Train loss: 0.0003\n",
      "Epoch  24, Iterations 5920 | Train loss: 0.0002\n",
      "Epoch  24, Iterations 5940 | Train loss: 0.0001\n",
      "Epoch  24, Iterations 5960 | Train loss: 0.0002\n",
      "Epoch  24, Iterations 5980 | Train loss: 0.0005\n",
      "Epoch  24, Iterations 6000 | Train loss: 0.0001\n",
      "Epoch  25, Iterations 6020 | Train loss: 0.0003\n",
      "Epoch  25, Iterations 6040 | Train loss: 0.0002\n",
      "Epoch  25, Iterations 6060 | Train loss: 0.0004\n",
      "Epoch  25, Iterations 6080 | Train loss: 0.0001\n",
      "Epoch  25, Iterations 6100 | Train loss: 0.0003\n",
      "Epoch  25, Iterations 6120 | Train loss: 0.0001\n",
      "Epoch  25, Iterations 6140 | Train loss: 0.0002\n",
      "Epoch  25, Iterations 6160 | Train loss: 0.0014\n",
      "Epoch  25, Iterations 6180 | Train loss: 0.0001\n",
      "Epoch  25, Iterations 6200 | Train loss: 0.0004\n",
      "Epoch  25, Iterations 6220 | Train loss: 0.0003\n",
      "Epoch  25, Iterations 6240 | Train loss: 0.0000\n",
      "Epoch  26, Iterations 6260 | Train loss: 0.0011\n",
      "Epoch  26, Iterations 6280 | Train loss: 0.0000\n",
      "Epoch  26, Iterations 6300 | Train loss: 0.0000\n",
      "Epoch  26, Iterations 6320 | Train loss: 0.0004\n",
      "Epoch  26, Iterations 6340 | Train loss: 0.0001\n",
      "Epoch  26, Iterations 6360 | Train loss: 0.0001\n",
      "Epoch  26, Iterations 6380 | Train loss: 0.0004\n",
      "Epoch  26, Iterations 6400 | Train loss: 0.0002\n",
      "Epoch  26, Iterations 6420 | Train loss: 0.0008\n",
      "Epoch  26, Iterations 6440 | Train loss: 0.0000\n",
      "Epoch  26, Iterations 6460 | Train loss: 0.0001\n",
      "Epoch  26, Iterations 6480 | Train loss: 0.0001\n",
      "Epoch  26, Iterations 6500 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6520 | Train loss: 0.0000\n",
      "Epoch  27, Iterations 6540 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6560 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6580 | Train loss: 0.0002\n",
      "Epoch  27, Iterations 6600 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6620 | Train loss: 0.0000\n",
      "Epoch  27, Iterations 6640 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6660 | Train loss: 0.0001\n",
      "Epoch  27, Iterations 6680 | Train loss: 0.0000\n",
      "Epoch  27, Iterations 6700 | Train loss: 0.0000\n",
      "Epoch  27, Iterations 6720 | Train loss: 0.0005\n",
      "Epoch  27, Iterations 6740 | Train loss: 0.0000\n",
      "Epoch  28, Iterations 6760 | Train loss: 0.0343\n",
      "Epoch  28, Iterations 6780 | Train loss: 0.0000\n",
      "Epoch  28, Iterations 6800 | Train loss: 0.0003\n",
      "Epoch  28, Iterations 6820 | Train loss: 0.0071\n",
      "Epoch  28, Iterations 6840 | Train loss: 0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  28, Iterations 6860 | Train loss: 0.0004\n",
      "Epoch  28, Iterations 6880 | Train loss: 0.1193\n",
      "Epoch  28, Iterations 6900 | Train loss: 0.0111\n",
      "Epoch  28, Iterations 6920 | Train loss: 0.0125\n",
      "Epoch  28, Iterations 6940 | Train loss: 0.0076\n",
      "Epoch  28, Iterations 6960 | Train loss: 0.1508\n",
      "Epoch  28, Iterations 6980 | Train loss: 0.0128\n",
      "Epoch  28, Iterations 7000 | Train loss: 0.0163\n",
      "Epoch  29, Iterations 7020 | Train loss: 0.0052\n",
      "Epoch  29, Iterations 7040 | Train loss: 0.0028\n",
      "Epoch  29, Iterations 7060 | Train loss: 0.0023\n",
      "Epoch  29, Iterations 7080 | Train loss: 0.0222\n",
      "Epoch  29, Iterations 7100 | Train loss: 0.0033\n",
      "Epoch  29, Iterations 7120 | Train loss: 0.0136\n",
      "Epoch  29, Iterations 7140 | Train loss: 0.0175\n",
      "Epoch  29, Iterations 7160 | Train loss: 0.0210\n",
      "Epoch  29, Iterations 7180 | Train loss: 0.0033\n",
      "Epoch  29, Iterations 7200 | Train loss: 0.0223\n",
      "Epoch  29, Iterations 7220 | Train loss: 0.0043\n",
      "Epoch  29, Iterations 7240 | Train loss: 0.0011\n",
      "Epoch  30, Iterations 7260 | Train loss: 0.0226\n",
      "Epoch  30, Iterations 7280 | Train loss: 0.0024\n",
      "Epoch  30, Iterations 7300 | Train loss: 0.0300\n",
      "Epoch  30, Iterations 7320 | Train loss: 0.0009\n",
      "Epoch  30, Iterations 7340 | Train loss: 0.0018\n",
      "Epoch  30, Iterations 7360 | Train loss: 0.0003\n",
      "Epoch  30, Iterations 7380 | Train loss: 0.0008\n",
      "Epoch  30, Iterations 7400 | Train loss: 0.0039\n",
      "Epoch  30, Iterations 7420 | Train loss: 0.0005\n",
      "Epoch  30, Iterations 7440 | Train loss: 0.0002\n",
      "Epoch  30, Iterations 7460 | Train loss: 0.0005\n",
      "Epoch  30, Iterations 7480 | Train loss: 0.0002\n",
      "Epoch  30, Iterations 7500 | Train loss: 0.0004\n",
      "Epoch  31, Iterations 7520 | Train loss: 0.0006\n",
      "Epoch  31, Iterations 7540 | Train loss: 0.0001\n",
      "Epoch  31, Iterations 7560 | Train loss: 0.0005\n",
      "Epoch  31, Iterations 7580 | Train loss: 0.0005\n",
      "Epoch  31, Iterations 7600 | Train loss: 0.0019\n",
      "Epoch  31, Iterations 7620 | Train loss: 0.0003\n",
      "Epoch  31, Iterations 7640 | Train loss: 0.0007\n",
      "Epoch  31, Iterations 7660 | Train loss: 0.0007\n",
      "Epoch  31, Iterations 7680 | Train loss: 0.0005\n",
      "Epoch  31, Iterations 7700 | Train loss: 0.0001\n",
      "Epoch  31, Iterations 7720 | Train loss: 0.0002\n",
      "Epoch  31, Iterations 7740 | Train loss: 0.0001\n",
      "Epoch  32, Iterations 7760 | Train loss: 0.0021\n",
      "Epoch  32, Iterations 7780 | Train loss: 0.0004\n",
      "Epoch  32, Iterations 7800 | Train loss: 0.0001\n",
      "Epoch  32, Iterations 7820 | Train loss: 0.0003\n",
      "Epoch  32, Iterations 7840 | Train loss: 0.0002\n",
      "Epoch  32, Iterations 7860 | Train loss: 0.0008\n",
      "Epoch  32, Iterations 7880 | Train loss: 0.0002\n",
      "Epoch  32, Iterations 7900 | Train loss: 0.0008\n",
      "Epoch  32, Iterations 7920 | Train loss: 0.0001\n",
      "Epoch  32, Iterations 7940 | Train loss: 0.0001\n",
      "Epoch  32, Iterations 7960 | Train loss: 0.0002\n",
      "Epoch  32, Iterations 7980 | Train loss: 0.0001\n",
      "Epoch  32, Iterations 8000 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8020 | Train loss: 0.0002\n",
      "Epoch  33, Iterations 8040 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8060 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8080 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8100 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8120 | Train loss: 0.0002\n",
      "Epoch  33, Iterations 8140 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8160 | Train loss: 0.0005\n",
      "Epoch  33, Iterations 8180 | Train loss: 0.0000\n",
      "Epoch  33, Iterations 8200 | Train loss: 0.0000\n",
      "Epoch  33, Iterations 8220 | Train loss: 0.0001\n",
      "Epoch  33, Iterations 8240 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8260 | Train loss: 0.0002\n",
      "Epoch  34, Iterations 8280 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8300 | Train loss: 0.0000\n",
      "Epoch  34, Iterations 8320 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8340 | Train loss: 0.0000\n",
      "Epoch  34, Iterations 8360 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8380 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8400 | Train loss: 0.0000\n",
      "Epoch  34, Iterations 8420 | Train loss: 0.0000\n",
      "Epoch  34, Iterations 8440 | Train loss: 0.0000\n",
      "Epoch  34, Iterations 8460 | Train loss: 0.0001\n",
      "Epoch  34, Iterations 8480 | Train loss: 0.0002\n",
      "Epoch  34, Iterations 8500 | Train loss: 0.0001\n",
      "Epoch  35, Iterations 8520 | Train loss: 0.0001\n",
      "Epoch  35, Iterations 8540 | Train loss: 0.0002\n",
      "Epoch  35, Iterations 8560 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8580 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8600 | Train loss: 0.0001\n",
      "Epoch  35, Iterations 8620 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8640 | Train loss: 0.0001\n",
      "Epoch  35, Iterations 8660 | Train loss: 0.0001\n",
      "Epoch  35, Iterations 8680 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8700 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8720 | Train loss: 0.0000\n",
      "Epoch  35, Iterations 8740 | Train loss: 0.0001\n",
      "Epoch  36, Iterations 8760 | Train loss: 0.0003\n",
      "Epoch  36, Iterations 8780 | Train loss: 0.0000\n",
      "Epoch  36, Iterations 8800 | Train loss: 0.0000\n",
      "Epoch  36, Iterations 8820 | Train loss: 0.0002\n",
      "Epoch  36, Iterations 8840 | Train loss: 0.0001\n",
      "Epoch  36, Iterations 8860 | Train loss: 0.0001\n",
      "Epoch  36, Iterations 8880 | Train loss: 0.0001\n",
      "Epoch  36, Iterations 8900 | Train loss: 0.0001\n",
      "Epoch  36, Iterations 8920 | Train loss: 0.0004\n",
      "Epoch  36, Iterations 8940 | Train loss: 0.0000\n",
      "Epoch  36, Iterations 8960 | Train loss: 0.0000\n",
      "Epoch  36, Iterations 8980 | Train loss: 0.0000\n",
      "Epoch  36, Iterations 9000 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9020 | Train loss: 0.0001\n",
      "Epoch  37, Iterations 9040 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9060 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9080 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9100 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9120 | Train loss: 0.0001\n",
      "Epoch  37, Iterations 9140 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9160 | Train loss: 0.0002\n",
      "Epoch  37, Iterations 9180 | Train loss: 0.0001\n",
      "Epoch  37, Iterations 9200 | Train loss: 0.0000\n",
      "Epoch  37, Iterations 9220 | Train loss: 0.0001\n",
      "Epoch  37, Iterations 9240 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9260 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9280 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9300 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9320 | Train loss: 0.0001\n",
      "Epoch  38, Iterations 9340 | Train loss: 0.0001\n",
      "Epoch  38, Iterations 9360 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9380 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9400 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9420 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9440 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9460 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9480 | Train loss: 0.0000\n",
      "Epoch  38, Iterations 9500 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9520 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9540 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9560 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9580 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9600 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9620 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9640 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9660 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9680 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9700 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9720 | Train loss: 0.0000\n",
      "Epoch  39, Iterations 9740 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9760 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9780 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9800 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9820 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9840 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9860 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9880 | Train loss: 0.0002\n",
      "Epoch  40, Iterations 9900 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9920 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9940 | Train loss: 0.0000\n",
      "Epoch  40, Iterations 9960 | Train loss: 0.0001\n",
      "Epoch  40, Iterations 9980 | Train loss: 0.0001\n",
      "Epoch  40, Iterations 10000 | Train loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/sentiment-rnn-40.ckpt\n"
     ]
    }
   ],
   "source": [
    "# val accuracy\n",
    "val_preds = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SentimentRNN(word_size=word_size, embed_size=embed_size, lstm_size=hidden_size, \n",
    "                     num_layer=n_layer, seq_length=sequence_length, batch_size=batch_size, \n",
    "                     learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Project 2 - implementing an RNN for character-level language modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pg2265.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178707"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[15858:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2int = {c:i for i,c in enumerate(char_set)}\n",
    "int2char = {i:c for i,c in enumerate(char_set)}\n",
    "\n",
    "assert len(char2int) == len(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ints = np.array([char2int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162849,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_date(sequence, batch_size, num_steps):\n",
    "    '''generate x, y  sequences from the given sequence\n",
    "    according to batch_size and num_steps\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    sequence : 1-d array\n",
    "        a sequence of integers (converted from char2int)\n",
    "    batch_size : int\n",
    "        size of batch in each iteration\n",
    "    num_steps : int\n",
    "        length of sequence in each batch\n",
    "    '''\n",
    "    total_batch_length = batch_size * num_steps\n",
    "    n_batch = len(sequence) // total_batch_length\n",
    "    \n",
    "    # trim original sequence fit to batches\n",
    "    x = sequence[0:n_batch*total_batch_length]\n",
    "    y = sequence[1:n_batch*total_batch_length+1]\n",
    "    assert len(x) == len(y)\n",
    "    \n",
    "    # reshape to [batch_size, n_batch*num_steps] matrix\n",
    "    x = np.asarray(x).reshape(batch_size, n_batch*num_steps)\n",
    "    y = np.asarray(y).reshape(batch_size, n_batch*num_steps)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, total_length = data_x.shape\n",
    "    n_batch = total_length // num_steps\n",
    "    for i in range(n_batch):\n",
    "        a, b = i*num_steps, (i+1)*num_steps\n",
    "        yield data_x[a:b], data_y[a:b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = reshape_date(text_ints, batch_size=64, num_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 2500), (64, 2500))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54, 12, 28, 63, 54,  4, 41, 56, 28, 51],\n",
       "       [56, 28, 63, 58, 12, 28, 28, 42, 63, 31],\n",
       "       [28, 42, 63, 18, 12, 28,  4, 28, 63, 20],\n",
       "       [63, 18, 20, 31, 28, 31, 58, 63, 31, 57],\n",
       "       [12, 20, 49, 28, 42, 13, 37, 49, 51, 63]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 28, 63, 54,  4, 41, 56, 28, 51, 20],\n",
       "       [28, 63, 58, 12, 28, 28, 42, 63, 31,  3],\n",
       "       [42, 63, 18, 12, 28,  4, 28, 63, 20, 58],\n",
       "       [18, 20, 31, 28, 31, 58, 63, 31, 57,  4],\n",
       "       [20, 49, 28, 42, 13, 37, 49, 51, 63, 58]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRnn:\n",
    "    \n",
    "    def __init__(self, n_classes, lstm_size=128, n_layers=1, \n",
    "                 batch_size=64, n_steps=100, learning_rate=1e-3,\n",
    "                keep_prob=.5, grad_clip=5, sampling=False, random_state=None):\n",
    "        # model parameters\n",
    "        self.n_classes = n_classes\n",
    "        self.lstm_size = lstm_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_steps = n_steps\n",
    "        self.n_layers = n_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            self.build(sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            # sampling mode\n",
    "            batch_size, n_step = 1, 1\n",
    "        else:\n",
    "            # traing mode\n",
    "            batch_size, n_step = self.batch_size, self.n_steps\n",
    "        \n",
    "        # init placeholders\n",
    "        tf_x = tf.placeholder(dtype=tf.int32, shape=(batch_size, n_step), name='tf_x')\n",
    "        tf_y = tf.placeholder(dtype=tf.int32, shape=(batch_size, n_step), name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(dtype=tf.float32, shape=(), name='tf_keepprob')\n",
    "        \n",
    "        # one-hot encoding\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.n_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.n_classes)\n",
    "        print(\" << y_onehot >> \", y_onehot)\n",
    "        # build multi-layers RNN cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.BasicLSTMCell(num_units=self.lstm_size), \n",
    "            input_keep_prob=tf_keepprob) for i in range(self.n_layers)])\n",
    "        \n",
    "        # define initial state\n",
    "        self.initial_state = cells.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "        \n",
    "        # run sequence step through the RNN\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, \n",
    "                                                      inputs=x_onehot, \n",
    "                                                      initial_state=self.initial_state)\n",
    "        print(\"  << lstm_outputs  >> \", lstm_outputs)\n",
    "        # reshape output 3-d [batch_size, steps, lstm_size] -> 2-d [batch_size*steps, lstm_size]\n",
    "        seq_outputs = tf.reshape(lstm_outputs, shape=[-1, self.lstm_size], name='seq_outputs_reshape')\n",
    "        \n",
    "        # dense layer -> out shape [seq_length, n_classes]\n",
    "        logits = tf.layers.dense(seq_outputs, units=self.n_classes, \n",
    "                                 activation=None, name='logits')\n",
    "        print(\" << logits >> \", logits)\n",
    "        proba = tf.nn.softmax(logits, axis=-1, name='probabilities')\n",
    "        \n",
    "        y_reshaped = tf.reshape(y_onehot, shape=(-1, self.n_classes), name='y_reshaped')\n",
    "        \n",
    "        # cost function\n",
    "        cost = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                                                    labels=y_reshaped, \n",
    "                                                    logits=logits), \n",
    "                    name='cost')\n",
    "        \n",
    "        # gradient clipping to avoiding exploding gradients\n",
    "        train_vars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(cost, train_vars), \n",
    "                                          clip_norm=self.grad_clip)\n",
    "        # optimizer \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars=zip(grads, train_vars), \n",
    "                                             name='train_op')\n",
    "        \n",
    "    def train(self, x_train, y_train, n_epochs, ckpt_dir='C:\\\\Users\\\\LENOVO\\\\OneDrive\\\\tmp\\\\model'):\n",
    "        \n",
    "        # check valid ckpt_dir\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.best_loss_ = float('inf')\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            \n",
    "            n_batch = x_train.shape[1]//self.n_steps\n",
    "            total_iterations = 0\n",
    "            for i in range(n_epochs):\n",
    "                # init zero_state\n",
    "                state = sess.run(self.initial_state)\n",
    "                avg_loss = 0.0\n",
    "                for j, (x_batch, y_batch) in enumerate(batch_generator(x_train, y_train, \n",
    "                                                                       num_steps=self.n_steps)):\n",
    "                    feed = {'tf_x:0': x_batch,\n",
    "                            'tf_y:0': y_batch,\n",
    "                            'tf_keepprob:0': self.keep_prob, \n",
    "                             self.initial_state: state}\n",
    "                    loss, _, state = sess.run(['cost:0', 'train_op', self.final_state], \n",
    "                                              feed_dict=feed)\n",
    "                    \n",
    "                    avg_loss += loss            \n",
    "                    # display every 20 iterations\n",
    "                    total_iterations += 1\n",
    "                    if (total_iterations % 20 == 0):\n",
    "                        print(\"Epoch {:2d}/{}, Iterations {:4d} | Train loss {:.5f}\".format(i, n_epochs, \n",
    "                                                                                            total_iterations,\n",
    "                                                                                            loss))\n",
    "                # one pass finish\n",
    "                avg_loss /= (j+1)\n",
    "                # save model if better than best_loss\n",
    "                if avg_loss < self.best_loss_:\n",
    "                    self.saver.save(sess, os.path.join(self.ckpt_dir, \n",
    "                                                       'char_model-{}.ckpt'.format(total_iterations)))\n",
    "    \n",
    "                        \n",
    "    def sample(self, output_length, ckpt_dir='C:\\\\Users\\\\LENOVO\\\\OneDrive\\\\tmp\\\\model',\n",
    "               starter_seq='The '):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " << y_onehot >>  Tensor(\"one_hot_1:0\", shape=(64, 100, 65), dtype=float32)\n",
      "  << lstm_outputs  >>  Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n",
      " << logits >>  Tensor(\"logits/BiasAdd:0\", shape=(6400, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = CharRnn(n_classes=len(char_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 < float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
