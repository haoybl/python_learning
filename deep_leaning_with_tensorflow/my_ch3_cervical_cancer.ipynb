{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFilter, ImageStat, Image, ImageDraw\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resize the image to 64 x 64 x 3\n",
    "def get_im_cv2(filepath):\n",
    "    im = cv2.imread(filepath)\n",
    "    im = cv2.resize(im, (64, 64), cv2.INTER_LINEAR)\n",
    "    return im\n",
    "\n",
    "# load trainin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'C://Users//p0ng5//Desktop//data//cervical_cancer'\n",
    "\n",
    "def load_train():\n",
    "    start = time.time()\n",
    "    X_train = []\n",
    "    X_id = []\n",
    "    y_train = []\n",
    "\n",
    "    train_folders = ['Type_1', 'Type_2', 'Type_3']\n",
    "\n",
    "    for fld in train_folders:\n",
    "        path = os.path.join(data_dir, 'train', fld, '*.jpg')\n",
    "        for file in glob.glob(path):\n",
    "            img = get_im_cv2(file)\n",
    "            X_train.append(img)\n",
    "            X_id.append(os.path.basename(file))\n",
    "            y_train.append(train_folders.index(fld))\n",
    "    print('Read train data time: {} seconds'.format(round(time.time() - start, 2)))\n",
    "    return X_train, X_id, y_train\n",
    "\n",
    "\n",
    "def load_test():\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    start = time.time()\n",
    "    path = os.path.join(data_dir, 'test', '*.jpg')\n",
    "    for file in sorted(glob.glob(path)):\n",
    "        img = get_im_cv2(file)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(os.path.basename(file))\n",
    "    print(\"Read test data time: {} seconds\".format(round(time.time() - start, 2)))\n",
    "    return X_test, X_test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize data\n",
    "def read_and_normalize_train_data():\n",
    "    \n",
    "    train_data, train_id, train_target = load_train()\n",
    "    train_data = np.array(train_data, dtype=np.float32)\n",
    "    train_target = np.array(train_target, dtype=np.float32)\n",
    "    \n",
    "    train_data /= 255\n",
    "    train_target = np_utils.to_categorical(train_target, 3)\n",
    "    print(\"Train shape: {}\".format(train_data.shape))\n",
    "    print(\"Target shape: []\".format(train_target.shape))\n",
    "    return train_data, train_target, train_id\n",
    "\n",
    "def read_and_normalize_test_data():\n",
    "    \n",
    "    test_data, test_id = load_test()\n",
    "    test_data = np.array(test_data, dtype=np.float32)\n",
    "    \n",
    "    test_data /= 255\n",
    "    print(\"Test shape: {}\".format(test_data.shape))\n",
    "    \n",
    "    return test_data, test_id\n",
    "\n",
    "\n",
    "def get_batch(X, batch_size, random_state=None):\n",
    "    n_samples = len(X)\n",
    "    n_batches = int(np.ceil(n_samples / batch_size))\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    rand_idx = np.random.permutation(range(n_samples))\n",
    "    for i in range(n_batches):\n",
    "        yield rand_idx[i*batch_size: min((i+1)*batch_size, n_samples)]\n",
    "        \n",
    "        \n",
    "# helper function for layer creation\n",
    "def conv2d(X, W, b, strides=1):\n",
    "    '''relu(X*W + b)'''\n",
    "    X = tf.nn.conv2d(X, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    X = tf.nn.bias_add(X, b)\n",
    "    return tf.nn.relu(X)\n",
    "    \n",
    "def maxpool2d(X, stride=2):\n",
    "    return tf.nn.max_pool(X, ksize=(1, stride, stride, 1), padding='VALID',\n",
    "                          strides=(1, stride, stride, 1))\n",
    "\n",
    "# init parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "in_H = 64\n",
    "in_W = 64\n",
    "in_channel = 3\n",
    "filter_1 = 64\n",
    "filter_2 = 128\n",
    "filter_3 = 256\n",
    "filter_H = 3\n",
    "filter_W = 3\n",
    "\n",
    "fc_flat = (in_H//8) * (in_W//8) * filter_3\n",
    "hidden_1 = 512\n",
    "hidden_2 = 512\n",
    "n_classes = 3\n",
    "\n",
    "p_dropout = .5\n",
    "\n",
    "\n",
    "\n",
    "def make_model(X, weights, biases, dropout):\n",
    "    \n",
    "    # conv1\n",
    "    X = conv2d(X, weights['wc1'], biases['bc1'])\n",
    "    X = maxpool2d(X)\n",
    "    \n",
    "    # conv2\n",
    "    X = conv2d(X, weights['wc2'], biases['bc2'])\n",
    "    X = maxpool2d(X)\n",
    "    \n",
    "    # conv3\n",
    "    X = conv2d(X, weights['wc3'], biases['bc3'])\n",
    "    X = maxpool2d(X)\n",
    "    \n",
    "    # FC1\n",
    "    X = tf.reshape(X, shape=[-1, fc_flat])\n",
    "    X = tf.add(tf.matmul(X, weights['wd1']), biases['bd1'])\n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    # add dropout\n",
    "    X = tf.nn.dropout(X, dropout)\n",
    "    \n",
    "    # FC2\n",
    "    X = tf.add(tf.matmul(X, weights['wd2']), biases['bd2'])\n",
    "    X = tf.nn.relu(X)\n",
    "    X = tf.nn.dropout(X, dropout)\n",
    "    \n",
    "    # output\n",
    "    out = tf.add(tf.matmul(X, weights['out']), biases['out'])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train data time: 303.56 seconds\n",
      "Train shape: (1481, 64, 64, 3)\n",
      "Target shape: []\n",
      "Read test data time: 106.89 seconds\n",
      "Test shape: (512, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, train_id = read_and_normalize_train_data()\n",
    "X_test, test_id = read_and_normalize_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: cost: 513346.6250 accuracy: 0.3862\n",
      "Epoch 11: cost: 50.7139 accuracy: 0.5348\n",
      "Epoch 21: cost: 0.9943 accuracy: 0.5307\n",
      "Epoch 31: cost: 0.9965 accuracy: 0.5294\n",
      "Epoch 41: cost: 2.5273 accuracy: 0.5287\n",
      "Epoch 51: cost: 1.4421 accuracy: 0.5280\n",
      "Epoch 61: cost: 0.9977 accuracy: 0.5287\n",
      "Epoch 71: cost: 0.9978 accuracy: 0.5287\n",
      "Epoch 81: cost: 0.9977 accuracy: 0.5287\n",
      "Epoch 91: cost: 0.9986 accuracy: 0.5280\n",
      "Epoch 101: cost: 0.9994 accuracy: 0.5280\n",
      "Epoch 111: cost: 0.9985 accuracy: 0.5280\n",
      "Epoch 121: cost: 0.9998 accuracy: 0.5273\n",
      "Epoch 131: cost: 0.9998 accuracy: 0.5273\n",
      "Epoch 141: cost: 0.9998 accuracy: 0.5273\n",
      "Epoch 151: cost: 0.9998 accuracy: 0.5273\n",
      "Epoch 161: cost: 0.9997 accuracy: 0.5273\n",
      "Epoch 171: cost: 0.9998 accuracy: 0.5273\n",
      "Epoch 181: cost: 0.9998 accuracy: 0.5273\n",
      "Epoch 191: cost: 0.9998 accuracy: 0.5273\n",
      "Epoch 200: cost: 0.9998 accuracy: 0.5273\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# init variable\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, in_H, in_W, in_channel))\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=(None, n_classes))\n",
    "dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "# init weights & biases\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([filter_H, filter_W, in_channel, filter_1])),\n",
    "    'wc2': tf.Variable(tf.random_normal([filter_H, filter_W, filter_1, filter_2])),\n",
    "    'wc3': tf.Variable(tf.random_normal([filter_H, filter_W, filter_2, filter_3])),\n",
    "    'wd1': tf.Variable(tf.random_normal([fc_flat, hidden_1])),\n",
    "    'wd2': tf.Variable(tf.random_normal([hidden_1, hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.zeros([filter_1])),\n",
    "    'bc2': tf.Variable(tf.zeros([filter_2])),\n",
    "    'bc3': tf.Variable(tf.zeros([filter_3])),\n",
    "    'bd1': tf.Variable(tf.zeros([hidden_1])),\n",
    "    'bd2': tf.Variable(tf.zeros([hidden_2])),\n",
    "    'out': tf.Variable(tf.zeros([n_classes]))\n",
    "}\n",
    "\n",
    "pred = make_model(X, weights, biases, dropout)\n",
    "\n",
    "# cost & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# accuracy\n",
    "correct = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        for idx in get_batch(X_train, batch_size, random_state=i):\n",
    "            X_batch, Y_batch = X_train[idx], Y_train[idx]\n",
    "            sess.run(optimizer, feed_dict={X:X_batch, Y:Y_batch, dropout:p_dropout})\n",
    "            \n",
    "        if (i%10==0) or (i+1 == epochs):\n",
    "            acc, loss = sess.run([accuracy, cost], feed_dict={X:X_train, Y:Y_train, dropout:1.0})\n",
    "            print(\"Epoch {}: cost: {:.4f} accuracy: {:.4f}\".format(i+1, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.slim import nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
