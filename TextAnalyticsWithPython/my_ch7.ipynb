{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exploring WordNet\n",
    "### Understanding Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term = 'fruit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "synsets = wn.synsets(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('fruit.n.01'),\n",
       " Synset('yield.n.03'),\n",
       " Synset('fruit.n.03'),\n",
       " Synset('fruit.v.01'),\n",
       " Synset('fruit.v.02')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset:Synset('fruit.n.01')\n",
      "Part of speech:noun.plant\n",
      "Definition:the ripened reproductive body of a seed plant\n",
      "Lemmas:['fruit']\n",
      "Examples:[]\n",
      "--------------------------------------------------\n",
      "Synset:Synset('yield.n.03')\n",
      "Part of speech:noun.artifact\n",
      "Definition:an amount of a product\n",
      "Lemmas:['yield', 'fruit']\n",
      "Examples:[]\n",
      "--------------------------------------------------\n",
      "Synset:Synset('fruit.n.03')\n",
      "Part of speech:noun.event\n",
      "Definition:the consequence of some effort or action\n",
      "Lemmas:['fruit']\n",
      "Examples:['he lived long enough to see the fruit of his policies']\n",
      "--------------------------------------------------\n",
      "Synset:Synset('fruit.v.01')\n",
      "Part of speech:verb.creation\n",
      "Definition:cause to bear fruit\n",
      "Lemmas:['fruit']\n",
      "Examples:[]\n",
      "--------------------------------------------------\n",
      "Synset:Synset('fruit.v.02')\n",
      "Part of speech:verb.creation\n",
      "Definition:bear fruit\n",
      "Lemmas:['fruit']\n",
      "Examples:['the trees fruited early this year']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets:\n",
    "    print(\"Synset:{}\\nPart of speech:{}\\nDefinition:{}\\nLemmas:{}\\nExamples:{}\".format(synset,\n",
    "                                                                                           synset.lexname(),\n",
    "                                                                                           synset.definition(),\n",
    "                                                                                           synset.lemma_names(),\n",
    "                                                                                           synset.examples()))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Lexical Semantic Relations\n",
    "### Entailments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('walk.v.01') --entails--> [Synset('step.v.01')]\n",
      "Synset('eat.v.01') --entails--> [Synset('chew.v.01'), Synset('swallow.v.01')]\n",
      "Synset('digest.v.01') --entails--> [Synset('consume.v.02')]\n"
     ]
    }
   ],
   "source": [
    "for action in ['walk', 'eat', 'digest']:\n",
    "    action_syn = wn.synsets(action, pos='v')[0]\n",
    "    print(\"{} --entails--> {}\".format(action_syn, action_syn.entailments()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homonyms and Homographs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank.n.01 - sloping land (especially the slope beside a body of water)\n",
      "depository_financial_institution.n.01 - a financial institution that accepts deposits and channels the money into lending activities\n",
      "bank.n.03 - a long ridge or pile\n",
      "bank.n.04 - an arrangement of similar objects in a row or in tiers\n",
      "bank.n.05 - a supply or stock held in reserve for future use (especially in emergencies)\n",
      "bank.n.06 - the funds held by a gambling house or the dealer in some gambling games\n",
      "bank.n.07 - a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "savings_bank.n.02 - a container (usually with a slot in the top) for keeping money at home\n",
      "bank.n.09 - a building in which the business of banking transacted\n",
      "bank.n.10 - a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "bank.v.01 - tip laterally\n",
      "bank.v.02 - enclose with a bank\n",
      "bank.v.03 - do business with a bank or keep an account at a bank\n",
      "bank.v.04 - act as the banker in a game or in gambling\n",
      "bank.v.05 - be in the banking business\n",
      "deposit.v.02 - put into a bank account\n",
      "bank.v.07 - cover with ashes so to control the rate of burning\n",
      "trust.v.01 - have confidence or faith in\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('bank'):\n",
    "    print(\"{} - {}\".format(synset.name(), synset.definition()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym: large.a.01\n",
      "Definition: above average in size or number or quantity or magnitude or extent\n",
      "Antonym: small.a.01\n",
      "Definition: limited or below average in number or quantity or magnitude or extent\n"
     ]
    }
   ],
   "source": [
    "term = 'large'\n",
    "synsets = wn.synsets(term, pos='a')\n",
    "\n",
    "synset_adj = synsets[0]\n",
    "adj_synonym = synset_adj.lemmas()[1].synset()\n",
    "adj_antonym = synset_adj.lemmas()[1].antonyms()[0].synset()\n",
    "\n",
    "print(\"Synonym: {}\".format(adj_synonym.name()))\n",
    "print(\"Definition: {}\".format(adj_synonym.definition()))\n",
    "print('Antonym: {}'.format(adj_antonym.name()))\n",
    "print(\"Definition: {}\".format(adj_antonym.definition()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('distrust.v.01.distrust'), Lemma('distrust.v.01.mistrust')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset.lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('large.a.01.large'), Lemma('large.a.01.big')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_adj.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym: rich_people.n.01\n",
      "Definition: people who have possessions and wealth (considered as a group)\n",
      "Synonym: poor_people.n.01\n",
      "Definition: people without possessions or wealth (considered as a group)\n",
      "--------------------------------------------------\n",
      "Synonym: rich.a.01\n",
      "Definition: possessing material wealth\n",
      "Synonym: poor.a.02\n",
      "Definition: having little money or few possessions\n",
      "--------------------------------------------------\n",
      "Synonym: rich.a.02\n",
      "Definition: having an abundant supply of desirable qualities or substances (especially natural resources)\n",
      "Synonym: poor.a.04\n",
      "Definition: lacking in specific resources, qualities or substances\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "term = 'rich'\n",
    "synsets = wn.synsets(term)[:3]\n",
    "\n",
    "for synset in synsets:\n",
    "    \n",
    "    lemma = synset.lemmas()[0]\n",
    "    synonym = lemma.synset()\n",
    "    antonym = lemma.antonyms()[0].synset()\n",
    "    \n",
    "    print(\"Synonym: {}\".format(synonym.name()))\n",
    "    print(\"Definition: {}\".format(synonym.definition()))\n",
    "    print(\"Synonym: {}\".format(antonym.name()))\n",
    "    print(\"Definition: {}\".format(antonym.definition()))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyponyms and Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tree.n.01\n",
      "Definition: a tall perennial woody plant having a main trunk and branches forming a distinct elevated crown; includes both gymnosperms and angiosperms\n"
     ]
    }
   ],
   "source": [
    "term = 'tree'\n",
    "ss = wn.synsets(term)[0]\n",
    "\n",
    "print(\"Name:\", ss.name())\n",
    "print(\"Definition:\", ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Hyponyms: 180\n",
      "\n",
      "aalii.n.01 - a small Hawaiian tree with hard dark wood\n",
      "\n",
      "acacia.n.01 - any of various spiny trees or shrubs of the genus Acacia\n",
      "\n",
      "african_walnut.n.01 - tropical African timber tree with wood that resembles mahogany\n",
      "\n",
      "albizzia.n.01 - any of numerous trees of the genus Albizia\n",
      "\n",
      "alder.n.02 - north temperate shrubs or trees having toothed leaves and conelike fruit; bark is used in tanning and dyeing and the wood is rot-resistant\n",
      "\n",
      "angelim.n.01 - any of several tropical American trees of the genus Andira\n",
      "\n",
      "angiospermous_tree.n.01 - any tree having seeds and ovules contained in the ovary\n",
      "\n",
      "anise_tree.n.01 - any of several evergreen shrubs and small trees of the genus Illicium\n",
      "\n",
      "arbor.n.01 - tree (as opposed to shrub)\n",
      "\n",
      "aroeira_blanca.n.01 - small resinous tree or shrub of Brazil\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hyponyms = ss.hyponyms()\n",
    "print(\"Total Hyponyms:\", len(hyponyms))\n",
    "print()\n",
    "for h in hyponyms[:10]:\n",
    "    print(\"{} - {}\".format(h.name(), h.definition()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypernym = ss.hypernyms()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('woody_plant.n.01')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entity.n.01 -> physical_entity.n.01 -> object.n.01 -> whole.n.02 -> living_thing.n.01 -> organism.n.01 -> plant.n.02 -> vascular_plant.n.01 -> woody_plant.n.01 -> tree.n.01'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" -> \".join(s.name() for s in ss.hypernym_paths()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holonyms and Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term = 'tree'\n",
    "tree = wn.synsets(term)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest.n.01 - the trees and other plants in a large densely wooded area\n"
     ]
    }
   ],
   "source": [
    "tree_holonym = tree.member_holonyms()[0]\n",
    "print(\"{} - {}\".format(tree_holonym.name(), tree_holonym.definition()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burl.n.02 - a large rounded outgrowth on the trunk or branch of a tree\n",
      "\n",
      "crown.n.07 - the upper branches and leaves of a tree or other plant\n",
      "\n",
      "limb.n.02 - any of the main branches arising from the trunk or a bough of a tree\n",
      "\n",
      "stump.n.01 - the base part of a tree that remains standing after the tree has been felled\n",
      "\n",
      "trunk.n.01 - the main stem of a tree; usually covered with bark; the bole is usually the part that is commercially useful for lumber\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_meronym = tree.part_meronyms()\n",
    "for m in tree_meronym:\n",
    "    print(\"{} - {}\".format(m.name(), m.definition()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total substance meronyms: 2\n",
      "\n",
      "heartwood.n.01 - the older inactive central wood of a tree or woody plant; usually darker and denser than the surrounding sapwood\n",
      "\n",
      "sapwood.n.01 - newly formed outer wood lying between the cambium and the heartwood of a tree or woody plant; usually light colored; active in water conduction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "substance_meronyms = tree.substance_meronyms()\n",
    "print('Total substance meronyms:', len(substance_meronyms))\n",
    "print()\n",
    "for s in substance_meronyms:\n",
    "    print(\"{} - {}\".format(s.name(), s.definition()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Relationships and Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = wn.synset('tree.n.01')\n",
    "lion = wn.synset('lion.n.01')\n",
    "tiger = wn.synset('tiger.n.02')\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities = [tree, lion, tiger, dog, cat]\n",
    "names = [e.name().split('.')[0] for e in entities]\n",
    "definitions = [e.definition() for e in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('tree.n.01'),\n",
       " Synset('lion.n.01'),\n",
       " Synset('tiger.n.02'),\n",
       " Synset('dog.n.01'),\n",
       " Synset('cat.n.01')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tree', 'lion', 'tiger', 'dog', 'cat']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a tall perennial woody plant having a main trunk and branches forming a distinct elevated crown; includes both gymnosperms and angiosperms',\n",
       " 'large gregarious predatory feline of Africa and India having a tawny coat with a shaggy mane in the male',\n",
       " 'large feline of forests in most of Asia having a tawny coat with black stripes; endangered',\n",
       " 'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds',\n",
       " 'feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_hypernyms = [[e1.lowest_common_hypernyms(e2)[0].name().split('.')[0] for e2 in entities]\n",
    "                      for e1 in entities ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tree', 'organism', 'organism', 'organism', 'organism'],\n",
       " ['organism', 'lion', 'big_cat', 'carnivore', 'feline'],\n",
       " ['organism', 'big_cat', 'tiger', 'carnivore', 'feline'],\n",
       " ['organism', 'carnivore', 'carnivore', 'dog', 'carnivore'],\n",
       " ['organism', 'feline', 'feline', 'carnivore', 'cat']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree</th>\n",
       "      <th>lion</th>\n",
       "      <th>tiger</th>\n",
       "      <th>dog</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>tree</td>\n",
       "      <td>organism</td>\n",
       "      <td>organism</td>\n",
       "      <td>organism</td>\n",
       "      <td>organism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>organism</td>\n",
       "      <td>lion</td>\n",
       "      <td>big_cat</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>feline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>organism</td>\n",
       "      <td>big_cat</td>\n",
       "      <td>tiger</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>feline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>organism</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>dog</td>\n",
       "      <td>carnivore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>organism</td>\n",
       "      <td>feline</td>\n",
       "      <td>feline</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tree       lion      tiger        dog        cat\n",
       "tree       tree   organism   organism   organism   organism\n",
       "lion   organism       lion    big_cat  carnivore     feline\n",
       "tiger  organism    big_cat      tiger  carnivore     feline\n",
       "dog    organism  carnivore  carnivore        dog  carnivore\n",
       "cat    organism     feline     feline  carnivore        cat"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(common_hypernyms, index=names, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarities = [[e1.path_similarity(e2) for e2 in entities] for e1 in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree</th>\n",
       "      <th>lion</th>\n",
       "      <th>tiger</th>\n",
       "      <th>dog</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tree  lion  tiger   dog   cat\n",
       "tree   1.00  0.07   0.07  0.12  0.08\n",
       "lion   0.07  1.00   0.33  0.17  0.25\n",
       "tiger  0.07  0.33   1.00  0.17  0.25\n",
       "dog    0.12  0.17   0.17  1.00  0.20\n",
       "cat    0.08  0.25   0.25  0.20  1.00"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(similarities, index=names, columns=names).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "    ('The fruits on that plant have ripened', 'n'),\n",
    "    ('He finally reaped the fruit of his hard word as he won the race', 'n')\n",
    "]\n",
    "\n",
    "word = 'fruit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The fruits on that plant have ripened\n",
      "Word synset: Synset('fruit.n.01')\n",
      "Corresponding definition: the ripened reproductive body of a seed plant\n",
      "--------------------------------------------------\n",
      "Sentence: He finally reaped the fruit of his hard word as he won the race\n",
      "Word synset: Synset('fruit.n.03')\n",
      "Corresponding definition: the consequence of some effort or action\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sent, tag in samples:\n",
    "    word_syn = lesk(word_tokenize(sent.lower()), word, pos=tag)\n",
    "    print(\"Sentence: {}\".format(sent))\n",
    "    print(\"Word synset: {}\".format(word_syn))\n",
    "    print(\"Corresponding definition: {}\".format(word_syn.definition()))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "    ('Lead is a very soft, malleable metal', 'n'),\n",
    "    ('John is the actor who plays the lead in that movie', 'n'),\n",
    "    ('This road leads to nowhere', 'v')\n",
    "]\n",
    "\n",
    "word = 'lead'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Lead is a very soft, malleable metal\n",
      "Word synset: Synset('lead.n.02')\n",
      "Corresponding definition: a soft heavy toxic malleable metallic element; bluish white when freshly cut but tarnishes readily to dull grey\n",
      "--------------------------------------------------\n",
      "Sentence: John is the actor who plays the lead in that movie\n",
      "Word synset: Synset('star.n.04')\n",
      "Corresponding definition: an actor who plays a principal role\n",
      "--------------------------------------------------\n",
      "Sentence: This road leads to nowhere\n",
      "Word synset: Synset('run.v.23')\n",
      "Corresponding definition: cause something to pass or lead somewhere\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sent, tag in samples:\n",
    "    word_syn = lesk(word_tokenize(sent.lower()), word, pos=tag)\n",
    "    print(\"Sentence: {}\".format(sent))\n",
    "    print(\"Word synset: {}\".format(word_syn))\n",
    "    print(\"Corresponding definition: {}\".format(word_syn.definition()))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "def parse_document(doc):\n",
    "    doc = re.sub('\\n', ' ', doc)\n",
    "    return [s.strip() for s in nltk.sent_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "text = \"\"\"\n",
    "Bayern Munich, or FC Bayern, is a German sports club based in Munich, \n",
    "Bavaria, Germany. It is best known for its professional football team, \n",
    "which plays in the Bundesliga, the top tier of the German football \n",
    "league system, and is the most successful club in German football \n",
    "history, having won a record 26 national titles and 18 national cups. \n",
    "FC Bayern was founded in 1900 by eleven football players led by Franz John. \n",
    "Although Bayern won its first national championship in 1932, the club \n",
    "was not selected for the Bundesliga at its inception in 1963. The club \n",
    "had its period of greatest success in the middle of the 1970s when, \n",
    "under the captaincy of Franz Beckenbauer, it won the European Cup three \n",
    "times in a row (1974-76). Overall, Bayern has reached ten UEFA Champions \n",
    "League finals, most recently winning their fifth title in 2013 as part \n",
    "of a continental treble. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "sentences = parse_document(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sentences = [nltk.pos_tag(tokens) for tokens in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ne_chunked_sents = [nltk.ne_chunk(tag_tokens) for tag_tokens in tagged_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('GPE', [('Bayern', 'NNP')]), Tree('ORGANIZATION', [('Munich', 'NNP')]), (',', ','), ('or', 'CC'), Tree('ORGANIZATION', [('FC', 'NNP'), ('Bayern', 'NNP')]), (',', ','), ('is', 'VBZ'), ('a', 'DT'), Tree('GPE', [('German', 'JJ')]), ('sports', 'NNS'), ('club', 'NN'), ('based', 'VBN'), ('in', 'IN'), Tree('GPE', [('Munich', 'NNP')]), (',', ','), Tree('GPE', [('Bavaria', 'NNP')]), (',', ','), Tree('GPE', [('Germany', 'NNP')]), ('.', '.')])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_chunked_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ne_chunked_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPE Bayern/NNP) True\n",
      "[('Bayern', 'NNP')] GPE\n",
      "(ORGANIZATION Munich/NNP) True\n",
      "[('Munich', 'NNP')] ORGANIZATION\n",
      "(',', ',') False\n",
      "('or', 'CC') False\n",
      "(ORGANIZATION FC/NNP Bayern/NNP) True\n",
      "[('FC', 'NNP'), ('Bayern', 'NNP')] ORGANIZATION\n",
      "(',', ',') False\n",
      "('is', 'VBZ') False\n",
      "('a', 'DT') False\n",
      "(GPE German/JJ) True\n",
      "[('German', 'JJ')] GPE\n",
      "('sports', 'NNS') False\n",
      "('club', 'NN') False\n",
      "('based', 'VBN') False\n",
      "('in', 'IN') False\n",
      "(GPE Munich/NNP) True\n",
      "[('Munich', 'NNP')] GPE\n",
      "(',', ',') False\n",
      "(GPE Bavaria/NNP) True\n",
      "[('Bavaria', 'NNP')] GPE\n",
      "(',', ',') False\n",
      "(GPE Germany/NNP) True\n",
      "[('Germany', 'NNP')] GPE\n",
      "('.', '.') False\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i, hasattr(i, 'label'))\n",
    "    if hasattr(i, 'label'):\n",
    "        print(i.leaves(), i.label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_entities = []\n",
    "\n",
    "for tagged_tree in ne_chunked_sents:\n",
    "    for node in tagged_tree:\n",
    "        if hasattr(node, 'label'):\n",
    "            name = \" \".join(c[0] for c in node.leaves())\n",
    "            label = node.label()\n",
    "            name_entities.append((name, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bayern', 'GPE'),\n",
       " ('Munich', 'ORGANIZATION'),\n",
       " ('FC Bayern', 'ORGANIZATION'),\n",
       " ('German', 'GPE'),\n",
       " ('Munich', 'GPE'),\n",
       " ('Bavaria', 'GPE'),\n",
       " ('Germany', 'GPE'),\n",
       " ('Bundesliga', 'ORGANIZATION'),\n",
       " ('German', 'GPE'),\n",
       " ('German', 'GPE'),\n",
       " ('Bayern', 'PERSON'),\n",
       " ('Franz John', 'PERSON'),\n",
       " ('Bayern', 'PERSON'),\n",
       " ('Bundesliga', 'ORGANIZATION'),\n",
       " ('Franz Beckenbauer', 'PERSON'),\n",
       " ('European', 'ORGANIZATION'),\n",
       " ('Overall', 'GPE'),\n",
       " ('Bayern', 'GPE'),\n",
       " ('UEFA', 'ORGANIZATION')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stanford_parser import stanford_ne_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sn = stanford_ne_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ne_annotated_sentences = [sn.tag(sent) for sent in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_entities = []\n",
    "\n",
    "for tag_sent in ne_annotated_sentences:\n",
    "    current_name = ''\n",
    "    current_name_tag = None\n",
    "    for name, tag in tag_sent:\n",
    "        if tag != 'O':\n",
    "            current_name = \" \".join([current_name, name]).strip()\n",
    "            current_name_tag = (current_name, tag)\n",
    "        else:\n",
    "            if current_name_tag:\n",
    "                name_entities.append(current_name_tag)\n",
    "                current_name_tag = None\n",
    "                current_name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity Name</th>\n",
       "      <th>Entity Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FC Bayern</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Munich</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bavaria</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FC Bayern</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Franz John</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bayern</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Franz Beckenbauer</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bayern</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Entity Name   Entity Type\n",
       "0      Bayern Munich  ORGANIZATION\n",
       "1          FC Bayern  ORGANIZATION\n",
       "2             Munich      LOCATION\n",
       "3            Bavaria      LOCATION\n",
       "4            Germany      LOCATION\n",
       "5          FC Bayern  ORGANIZATION\n",
       "6         Franz John        PERSON\n",
       "7             Bayern  ORGANIZATION\n",
       "8  Franz Beckenbauer        PERSON\n",
       "9             Bayern  ORGANIZATION"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(name_entities, columns=['Entity Name', 'Entity Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Semantic Representations\n",
    "\n",
    "#### Propositional Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P = 'P'\n",
    "Q = 'Q'\n",
    "P_prop = 'He is hungry'\n",
    "Q_prop = 'He will eat a sandwich'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_status = [True, False]\n",
    "q_status = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conjunction = 'P & Q'\n",
    "disjunction = \"P | Q\"\n",
    "implication = 'P -> Q'\n",
    "equivalence = 'P <-> Q'\n",
    "\n",
    "expressions = [conjunction, disjunction, implication, equivalence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for p_val, q_val in product(p_status, q_status):\n",
    "    \n",
    "    valu = nltk.Valuation([(P, p_val), (Q, q_val)])\n",
    "    dom = set()\n",
    "    assignment = nltk.Assignment(dom)\n",
    "    model = nltk.Model(dom, valu)\n",
    "    row = [p_val, q_val]\n",
    "    for expr in expressions:\n",
    "        row.append(model.evaluate(expr, assignment))\n",
    "    results.append(row)\n",
    "    \n",
    "cols = [P, Q] + expressions\n",
    "\n",
    "result_df = pd.DataFrame(results, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>P &amp; Q</th>\n",
       "      <th>P | Q</th>\n",
       "      <th>P -&gt; Q</th>\n",
       "      <th>P &lt;-&gt; Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       P      Q  P & Q  P | Q  P -> Q  P <-> Q\n",
       "0   True   True   True   True    True     True\n",
       "1   True  False  False   True   False    False\n",
       "2  False   True  False   True    True    False\n",
       "3  False  False  False  False    True     True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Order Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init theorem provers\n",
    "prover_path = \"/home/pongsakorn/Desktop/python_learning/TextAnalyticsWithPython/LADR-2009-11A/bin\"\n",
    "os.environ['PROVER9'] = prover_path\n",
    "\n",
    "prover = nltk.Prover9()\n",
    "\n",
    "# for reading FOL expressions\n",
    "read_expr = nltk.sem.Expression.fromstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# `If an entity jumps over another entity, the reverse cannot happen`\n",
    "\n",
    "# set the rule expression\n",
    "rule = read_expr('all x. all y. (jumps_over(x, y) -> -jumps_over(y, x))')\n",
    "# set the event occured\n",
    "event = read_expr('jumps_over(fox, dog)')\n",
    "\n",
    "# set the outcome to evaluate\n",
    "test_outcome = read_expr(\"jumps_over(dog, fox)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prover.prove(goal=test_outcome, assumptions=[event, rule], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "# aclimdb/[train|test\\]/[pos|neg]\n",
    "folder = './aclImdb/'\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "for d1 in ['train', 'test']:\n",
    "    for d2 in ['pos', 'neg']:\n",
    "        path = os.path.join(folder, d1, d2)\n",
    "        for txt_file in os.listdir(path):\n",
    "            with open(os.path.join(path, txt_file), 'r') as f:\n",
    "                review = f.read().strip()              \n",
    "                row = [review, d2]\n",
    "                dataset = dataset.append([row], ignore_index=True)\n",
    "\n",
    "idx = np.random.permutation(len(dataset))\n",
    "\n",
    "dataset.columns = ['review', 'sentiment']\n",
    "\n",
    "dataset = dataset.iloc[idx]\n",
    "\n",
    "dataset.reset_index(drop=True, inplace=True)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-573a86964f34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from html import unescape\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "contractions = pickle.load(open('contraction_map.pkl', 'rb'))\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    '''class to remove html tag and unescape html characters'''\n",
    "    \n",
    "    def __init__(self, convert_charrefs=False):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "        self.convert_charrefs = convert_charrefs\n",
    "        \n",
    "    def handle_data(self, txt):\n",
    "        self.fed.append(unescape(txt))\n",
    "        \n",
    "    def get_data(self):\n",
    "        return \" \".join(self.fed)\n",
    "\n",
    "def expand_contraction(text, contractions):\n",
    "    \n",
    "    pattern = re.compile(r\"({})\".format('|'.join(contractions.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand(match_obj):\n",
    "        \n",
    "        txt = match_obj.group(0)\n",
    "        new_txt = contractions.get(txt, contractions.get(txt.lower()))\n",
    "        return txt[0] + new_txt[1:]\n",
    "    \n",
    "    text = pattern.sub(expand, text)\n",
    "    text = re.sub(\"'\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_pos_tag(text):\n",
    "    \n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    return [(w, penn_to_wn_tags(tag)) for w,tag in tagged_tokens]\n",
    "\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    tagged_tokens = text_pos_tag(text)\n",
    "    \n",
    "    return \" \".join(wnl.lemmatize(w, tag) if tag else w for w, tag in tagged_tokens)\n",
    "\n",
    "\n",
    "def keep_only_text(text):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    for w in tokens:\n",
    "        if re.search(\"[a-zA-Z]\", w):\n",
    "            filtered_tokens.append(w.strip())\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "\n",
    "def remove_special_char(text):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pattern = re.compile(r'[{}]'.format(re.escape(string.punctuation)))\n",
    "    text = \" \".join(filter(None, [pattern.sub(' ', t).strip() for t in tokens]))\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join(w for w in tokens if w not in stop_words)\n",
    "\n",
    "def strip_html(text):\n",
    "    html_stripper = MLStripper()\n",
    "    html_stripper.feed(text)\n",
    "    return html_stripper.get_data()\n",
    "\n",
    "\n",
    "def normalize_accented_characters(text):\n",
    "    text = unicodedata.normalize('NFKD', \n",
    "                                 text.decode('utf-8')).encode('ascii', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, lemmanize=True, \n",
    "                     tokenize=False, only_text_chars=False):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        text = strip_html(text)\n",
    "        #text = normalize_accented_characters(text)\n",
    "        \n",
    "        # expand contraction\n",
    "        text = expand_contraction(text, contractions)\n",
    "        \n",
    "        if lemmanize:\n",
    "            text = lemmatize_text(text).lower()\n",
    "        else:\n",
    "            text = text.lower()\n",
    "            \n",
    "        if only_text_chars:\n",
    "            text = keep_only_text(text)\n",
    "        \n",
    "        # remove special char\n",
    "        text = remove_special_char(text)\n",
    "        # remove stopwords\n",
    "        text = remove_stopwords(text)\n",
    "        \n",
    "        if tokenize:\n",
    "            text = nltk.word_tokenize(text)\n",
    "        \n",
    "        normalized_corpus.append(text)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "def build_feature_matrix(corpus, feature_type='frequency', \n",
    "                         min_df=0.0, max_df=1.0, ngram=(1, 1)):\n",
    "    \n",
    "    if feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df, \n",
    "                                     max_df=max_df, ngram_range=ngram)\n",
    "    elif feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=min_df, \n",
    "                                     max_df=max_df, ngram_range=ngram)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram)\n",
    "    else:\n",
    "        raise Exception('Invalid feature type: possible choice [{}]'.format(\"|\".join(['frequency', \n",
    "                                                                                      'binary',\n",
    "                                                                                     'tfidf'])))\n",
    "    feature_matrix = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                                f1_score, recall_score, precision_score)\n",
    "\n",
    "\n",
    "def display_classification_report(y_true, y_pred, labels=None, target_names=None):\n",
    "    report = classification_report(y_true, y_pred, \n",
    "                                   labels=labels, target_names=target_names)\n",
    "    print(report)\n",
    "    \n",
    "    \n",
    "def display_confusion_matrix(y_true, y_pred, labels=[1, 0]):\n",
    "    con_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    idx_cols = pd.MultiIndex.from_product([['Prediction'], labels])\n",
    "    idx_rows = pd.MultiIndex.from_product([['Actual'], labels])\n",
    "    \n",
    "    return pd.DataFrame(con_mat, index=idx_rows, columns=idx_cols, dtype=int)\n",
    "\n",
    "\n",
    "def display_evaluation_metrics(y_true, y_pred, pos_label=1):\n",
    "    \n",
    "    result = \"Accuracy: {acc:.3f}\\nPrecision: {prec:.3f}\\n\\\n",
    "    Recall: {recall:.3f}\\nF1 Score: {f1:.3f}\"\n",
    "    \n",
    "    print(result.format(acc=accuracy_score(y_true, y_pred),\n",
    "                        prec=precision_score(y_true, y_pred),\n",
    "                        recall=recall_score(y_true, y_pred),\n",
    "                        f1=f1_score(y_true, y_pred, pos_label=pos_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset.to_csv('movie_reviews.csv', index=False)\n",
    "dataset = pd.read_csv('movie_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For me, it just didn't seem like GI Joe at all...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Back in August, '81 there was a country-ish bu...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This 1973 TV remake of the Billy Wilder classi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie really shows its age. The print I s...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As others have noted, this should have been an...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  For me, it just didn't seem like GI Joe at all...       neg\n",
       "1  Back in August, '81 there was a country-ish bu...       pos\n",
       "2  This 1973 TV remake of the Billy Wilder classi...       neg\n",
       "3  This movie really shows its age. The print I s...       neg\n",
       "4  As others have noted, this should have been an...       neg"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000,), (35000,), (15000,), (15000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = 35000\n",
    "\n",
    "train = dataset.iloc[:n_train]\n",
    "test = dataset.iloc[n_train:]\n",
    "\n",
    "review_train = train['review'].values\n",
    "review_test = test['review'].values\n",
    "sentiment_train = train['sentiment'].values\n",
    "sentiment_test = test['sentiment'].values\n",
    "\n",
    "review_train.shape, sentiment_train.shape, review_test.shape, sentiment_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8,), (8,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = 8\n",
    "\n",
    "sample_idx = np.random.permutation(range(len(test)))[:n_sample]\n",
    "\n",
    "review_sample = review_test[sample_idx]\n",
    "sentiment_sample = sentiment_test[sample_idx]\n",
    "\n",
    "review_sample.shape, sentiment_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30min 6s, sys: 3.39 s, total: 30min 9s\n",
      "Wall time: 30min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# normalized dataset\n",
    "\n",
    "norm_train_reviews = normalize_corpus(review_train, lemmanize=True, only_text_chars=True)\n",
    "norm_test_reviews = normalize_corpus(review_test, lemmanize=True, only_text_chars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.53 s, sys: 132 ms, total: 9.66 s\n",
      "Wall time: 9.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# feature extraction\n",
    "vectorizer, train_features = build_feature_matrix(norm_train_reviews, feature_type='tfidf',\n",
    "                                                 min_df=0.0, max_df=1.0, ngram=(1, 1))\n",
    "\n",
    "test_features = vectorizer.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SGDClassifier(loss='hinge', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=1000, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(train_features, sentiment_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.94      0.91      0.93     17504\n",
      "        pos       0.92      0.94      0.93     17496\n",
      "\n",
      "avg / total       0.93      0.93      0.93     35000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_classification_report(sentiment_train, svc.predict(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.90      0.88      0.89      7496\n",
      "        pos       0.88      0.91      0.89      7504\n",
      "\n",
      "avg / total       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_classification_report(sentiment_test, svc.predict(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Actual</th>\n",
       "      <th>pos</th>\n",
       "      <td>16443</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>1521</td>\n",
       "      <td>15983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Prediction       \n",
       "                  pos    neg\n",
       "Actual pos      16443   1053\n",
       "       neg       1521  15983"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_confusion_matrix(sentiment_train, svc.predict(train_features), labels=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Actual</th>\n",
       "      <th>pos</th>\n",
       "      <td>6815</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>936</td>\n",
       "      <td>6560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Prediction      \n",
       "                  pos   neg\n",
       "Actual pos       6815   689\n",
       "       neg        936  6560"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_confusion_matrix(sentiment_test, svc.predict(test_features), labels=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews:\n",
      "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!\n",
      "\n",
      "Predict: neg\n",
      "Actual: pos\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "I saw one of the stage performances in Denver and have never been less impressed. The word \"vagina\" says it all. A body part. Nothing shocking here. I could say to my doctor, \"My left arm has been hurting a bit after tennis\" or \"My vagina hurts after cycling\" with equal or more social commentary. It could be the \"Tricep Monologues\" for all the entertainment or radical comment I heard. The monologues were dull but delivered with drama, the topics were outdated, and I was alternately bored and annoyed. Once I think I laughed but apparently it wasn't when I was supposed to. Surely this isn't really a hit. Oh, and spoilers: there was a LESBIAN! - oh, wait, maybe not, come to think of it. And Inappropriate Fondling! And a Crack Mama! That about covers it.\n",
      "\n",
      "Predict: neg\n",
      "Actual: neg\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "Wow! Wow! Wow! I have never seen a non-preachy documentary on globalization until I saw MARDI GRAS: MADE IN CHINA. This film has zero narration and combines verite footage with sensitive interviews with four teenage workers in China who live inside a factory compound. They play with toys, jump rope, and dance. Yet, the majority of their days and nights consist of work, work, and work -- but the footage of their work is illuminating and mesmerizing to watch. The owner of the factory in China is amazingly open, so much so that he hits home the effects of globalization while he \"punishes\" the workers. Astutely following Mardi Gras beads from China to the Carnival, the film reveals how the local is connected to the global through humor and interesting, compelling footage from both cultures. One of the most interesting parts in this film is the cross cultural introduction of factory workers and Mardi Gras revelers to each other through pictures. Here, the film comes full circle and shows how images can be a point of communication and transformation. The film is never preachy, is not guilt driven, and allows everyone's point of view to be present. At the end, we -- the viewers -- make up our own conclusions about the complexity of the film, and globalization.\n",
      "\n",
      "Predict: pos\n",
      "Actual: pos\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "I remember when this came out it was the first kung fu film ever seen around our way and we were all excited about seeing it for sure .Although the action was mediocre at best it gave us our first taste of kung fu and our first taste of bad dubbing as well as bad film making or more precisely the way Chinese people were making films at the time . They were admittedly inferior wlthout question but there was entertainment value here and that caught on for sure . The kung fu craze had begun and Bruce Lee and ''The Chinese Connection'' would soon follow either that or ''The Chinese Boxer'' with Jimmy Wang Yu . In any case this film was chosen to lead the way .\n",
      "\n",
      "Predict: pos\n",
      "Actual: neg\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "Sheesh! What a dreadful movie. Dodgy camera work, a script with more corn than Kellogg's, and acting so hammy you could open a pig farm with it. <br /><br />To cap it all, it doesn't know which audience to aim at - we have Cornel Wilde - or is that Corny Wilde? - getting on his soap box about the hazards of smoking any time someone lights a cigarette, dear oh dear, and in another awkward scene we have the baddie, Lobo, forcing his, ahem, if you will, 'male friend' to do a striptease dressed in a bikini. Try explaining that one to the kids...<br /><br />Throw in an overly contrived Treasure Island-cum-Jaws type storyline, and the result is a film so unintentionally funny, it's enjoyable - I shouldn't expect a Special Edition DVD any time soon, though.\n",
      "\n",
      "Predict: neg\n",
      "Actual: neg\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "Actually, Goldie Hawn is from Washington (Takoma Park, Maryland), but I digress. This is sort of a Mr. Smith goes to Washington type of movie, with some variations but the same premise. I taped this movie off of cable years ago because I had a huge crush on Goldie Hawn. The story is interesting, but it's highly unlikely that some cocktail waitress will get an important job in the government just because she saved some big shot's life. It made me laugh and made me mad at the same time. It made me laugh because some of the situations she found herself in were so ridiculous, I had to laugh. (POSSIBLE SPOILER AHEAD). It made me mad to think that our government would set up an average citizen in the manner she was set up. And the speech she made at the end...beautiful. Too bad not many people have guts like that in real life.\n",
      "\n",
      "Predict: pos\n",
      "Actual: pos\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "I'm a bit spooked by some of these reviews praising A.K.A. Not only do they sound as if they were written by the same person, but they contain all kinds of insider information that surely you could only find by reading the press book from cover to cover. Please don't tell me that the director is writing his own reviews as that would just be too sad to contemplate.<br /><br /> Afraid I'm another one of those who hated the film and was surprised by its unapologetic amateurism. Great idea, shame about the execution. And it was most disconcerting to watch so many good actors (as well as some very bad ones including the leaden lead) all apparently thinking that they were appearing in a series of very different films.<br /><br /> I wish that A.K.A. had been audacious, innovative or just simply interesting. Sadly it was like watching an unintentionally hysterical home video with arty aspirations. A missed opportunity.\n",
      "\n",
      "Predict: neg\n",
      "Actual: neg\n",
      "--------------------------------------------------\n",
      "Reviews:\n",
      "This must be the dumbest movie I've ever seen and therefor it deserves a special award. OK, so it begins pretty good, logical, humor and then rapidly changes to complete and utter uh f@rt,@ss, fck stupidity that makes total sense. The strange thing is that i hardly laughed at it, though i guess it is supposed to be funny, with a very serious message. After a moment of thought why i didn't laugh, the answer was pretty clear... this movie is a lot more realistic then i would like it to be. So it is extremely dumb, yet very wise if the object was to reach people which aren't 'blessed' with a whole lot of neurons and synapses and electrolytes (huh, electrolytes?). This movie might actually be understood at some level by a big audience!<br /><br />Although they probably just laugh.<br /><br />Ah well, nice try...<br /><br />I am now editing (well adding to) my comment, cause i read some of the other comments. First i noticed some people see racism in this movie. Well, that did not at all cross my mind when watching it. However, i can imagine if you are looking from that perspective, you will see some. We could of course debate the link between intelligence and race, but i think this is not the place to do so. I wonder, if we were to rate the Huxtable family here, would you complain about them all being black? I don't think so, but i guess you could... (aside from that, from the 2 wisest people in the world, half, the woman, doesn't seem totally caucasian to me, so 'what the problem is?')<br /><br />Secondly the movie gets very mixed ratings; from 1's to 10's and everything in between, but a lot of polarisation there, which means the movie does its job very well! It stirrs people up, makes them want to speak about it and judge it. That means it is a good movie, whether you like it or hate it. After 5 minutes it becomes very predictive and boring even, yet you keep watching this utter crap. I guess you keep hoping that it isn't so. Or maybe it's our sick way to look at terrible accidents. Well, this is one for sure.\n",
      "\n",
      "Predict: neg\n",
      "Actual: pos\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "reviews = review_test[sample_idx]\n",
    "preds = svc.predict(test_features)[sample_idx]\n",
    "trues = sentiment_test[sample_idx]\n",
    "\n",
    "for r, p, t in zip(reviews, preds, trues):\n",
    "    print('Reviews:\\n{}\\n\\nPredict: {}\\nActual: {}'.format(r, p, t))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sk = StratifiedKFold(n_splits=10, shuffle=True, random_state=123)\n",
    "gb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tr_idx, ts_idx in sk.split(train_features, sentiment_train):\n",
    "    data_X = train_features[ts_idx].toarray()\n",
    "    data_y = sentiment_train[ts_idx]\n",
    "    \n",
    "    gb.partial_fit(data_X, data_y, classes=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "for row in test_features:\n",
    "    y_preds.append(gb.predict(row.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.62      0.78      0.69      7496\n",
      "        pos       0.71      0.53      0.60      7504\n",
      "\n",
      "avg / total       0.67      0.66      0.65     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_classification_report(sentiment_test, np.array(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Actual</th>\n",
       "      <th>pos</th>\n",
       "      <td>3960</td>\n",
       "      <td>3544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>1631</td>\n",
       "      <td>5865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Prediction      \n",
       "                  pos   neg\n",
       "Actual pos       3960  3544\n",
       "       neg       1631  5865"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_confusion_matrix(sentiment_test, np.array(y_preds), labels=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pysrt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pysrt.open('../../Ted 2.srt', encoding='TIS-620')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt = a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n \\n\\n\\n<i> </i>\\n<i> </i>\\n<i></i>\\n<i> </i>\\n \\n   \\n \\n\\n \\n  \\n\\n\\n \\n  \\n\\n\\n\\n\\n\\n \\n \\n- <i></i>\\n- <i></i>\\n<i> </i>\\n<i> </i>\\n<i>  </i>\\n- <i></i>\\n- <i></i>\\n<i> </i>\\n<i> </i>\\n<i> </i>\\n<i> \"\"</i>'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pat = re.compile(r'<i>(.+)</i>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " 'Downtown Abbey',\n",
       " '',\n",
       " '',\n",
       " '   ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " 'I Am Legend',\n",
       " 'Sister, Sister',\n",
       " '  3-17 ',\n",
       " '',\n",
       " ' .',\n",
       " '',\n",
       " 'Frozen',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " 'Kramer vs. Kramer, Alien vs. Predator',\n",
       " '',\n",
       " 'Ernest Goes to Camp,',\n",
       " 'Ernest Goes to Jail',\n",
       " 'The Importance of Being Earnest',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '  ',\n",
       " 'Rocky III.',\n",
       " 'Rocky',\n",
       " 'Rocky',\n",
       " 'Rocky',\n",
       " 'Rocky',\n",
       " 'The Great Gatsby',\n",
       " '',\n",
       " '  ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' 1980',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " ' \"\" ',\n",
       " '  \"\"',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '  ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '..',\n",
       " \"Where's Waldo\",\n",
       " ' ',\n",
       " ' ',\n",
       " '',\n",
       " ' ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '  ',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' \"\"']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat.findall(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pysrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '../../python_learning/ted_subtitles/SaraSeager_2015.en.th.srt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subs = pysrt.open(filename, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = [sub.text for sub in subs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm here to tell you about the real search for alien life.\\n \",\n",
       " 'Not little green humanoids arriving in shiny UFOs,\\n ',\n",
       " 'although that would be nice.\\n',\n",
       " \"But it's the search for planets orbiting stars far away.\\n \",\n",
       " 'Every star in our sky is a sun.\\n',\n",
       " 'And if our sun has planets --\\n',\n",
       " 'Mercury, Venus, Earth, Mars, etc.,\\n    ',\n",
       " 'surely those other stars should have planets also,\\n ',\n",
       " 'and they do.\\n ',\n",
       " 'And in the last two decades,\\n',\n",
       " 'astronomers have found thousands of exoplanets.\\n ',\n",
       " 'Our night sky is literally teeming with exoplanets.\\n  ',\n",
       " 'We know, statistically speaking,\\n',\n",
       " 'that every star has at least one planet.\\n',\n",
       " 'And in the search for planets,\\n',\n",
       " 'and in the future, planets that might be like Earth,\\n ',\n",
       " \"we're able to help address\\n\",\n",
       " 'some of the most amazing and mysterious questions\\n',\n",
       " 'that have faced humankind for centuries.\\n',\n",
       " 'Why are we here?\\n',\n",
       " 'Why does our universe exist?\\n',\n",
       " 'How did Earth form and evolve?\\n',\n",
       " 'How and why did life originate and populate our planet?\\n ',\n",
       " 'The second question that we often think about is:\\n',\n",
       " 'Are we alone?\\n',\n",
       " 'Is there life out there?\\n',\n",
       " 'Who is out there?\\n',\n",
       " 'You know, this question has been around for thousands of years,\\n',\n",
       " 'since at least the time of the Greek philosophers.\\n',\n",
       " \"But I'm here to tell you just how close we're getting\\n \",\n",
       " 'to finding out the answer to this question.\\n',\n",
       " \"It's the first time in human history that this really is within reach for us.\\n \",\n",
       " 'Now when I think about the possibilities for life out there,\\n ',\n",
       " 'I think of the fact that our sun is but one of many stars.\\n  ',\n",
       " 'This is a photograph of a real galaxy,\\n',\n",
       " 'we think our Milky Way looks like this galaxy.\\n ',\n",
       " \"It's a collection of bound stars.\\n\",\n",
       " 'But our [sun] is one of hundreds of billions of stars\\n ',\n",
       " 'and our galaxy is one of upwards of hundreds of billions of galaxies.\\n ',\n",
       " 'Knowing that small planets are very common,\\n ',\n",
       " 'you can just do the math.\\n',\n",
       " 'And there are just so many stars and so many planets out there,\\n ',\n",
       " 'that surely, there must be life somewhere out there.\\n ',\n",
       " 'Well, the biologists get furious with me for saying that,\\n ',\n",
       " 'because we have absolutely no evidence for life beyond Earth yet.\\n ',\n",
       " 'Well, if we were able to look at our galaxy from the outside\\n  ',\n",
       " 'and zoom in to where our sun is,\\n',\n",
       " 'we see a real map of the stars.\\n',\n",
       " 'And the highlighted stars are those with known exoplanets.\\n ',\n",
       " 'This is really just the tip of the iceberg.\\n',\n",
       " 'Here, this animation is zooming in onto our solar system.\\n  ',\n",
       " \"And you'll see here the planets\\n\",\n",
       " 'as well as some spacecraft that are also orbiting our sun.\\n ',\n",
       " 'Now if we can imagine going to the West Coast of North America,\\n  ',\n",
       " 'and looking out at the night sky,\\n',\n",
       " \"here's what we'd see on a spring night.\\n\",\n",
       " 'And you can see the constellations overlaid\\n',\n",
       " 'and again, so many stars with planets.\\n',\n",
       " \"There's a special patch of the sky where we have thousands of planets.\\n \",\n",
       " 'This is where the Kepler Space Telescope focused for many years.\\n ',\n",
       " \"Let's zoom in and look at one of the favorite exoplanets.\\n \",\n",
       " 'This star is called Kepler-186f.\\n -186',\n",
       " \"It's a system of about five planets.\\n\",\n",
       " \"And by the way, most of these exoplanets, we don't know too much about.\\n  \",\n",
       " 'We know their size, and their orbit and things like that.\\n ',\n",
       " \"But there's a very special planet here called Kepler-186f.\\n  -186\",\n",
       " 'This planet is in a zone that is not too far from the star,\\n',\n",
       " 'so that the temperature may be just right for life.\\n',\n",
       " \"Here, the artist's conception is just zooming in\\n \",\n",
       " 'and showing you what that planet might be like.\\n ',\n",
       " 'So, many people have this romantic notion of astronomers\\n ',\n",
       " 'going to the telescope on a lonely mountaintop\\n ',\n",
       " 'and looking at the spectacular night sky through a big telescope.\\n ',\n",
       " 'But actually, we just work on our computers like everyone else,\\n  ',\n",
       " 'and we get our data by email or downloading from a database.\\n  ',\n",
       " 'So instead of coming here to tell you\\n ',\n",
       " 'about the somewhat tedious nature of the data and data analysis\\n ',\n",
       " 'and the complex computer models we make,\\n',\n",
       " 'I have a different way to try to explain to you\\n',\n",
       " \"some of the things that we're thinking about exoplanets.\\n \",\n",
       " \"Here's a travel poster:\\n\",\n",
       " '\"Kepler-186f:\\n-186',\n",
       " 'Where the grass is always redder on the other side.\"\\n  ()',\n",
       " \"That's because Kepler-186f orbits a red star,\\n -186 \",\n",
       " \"and we're just speculating that perhaps the plants there,\\n \",\n",
       " 'if there is vegetation that does photosynthesis,\\n',\n",
       " 'it has different pigments and looks red.\\n ',\n",
       " '\"Enjoy the gravity on HD 40307g,\\n 40307',\n",
       " 'a Super-Earth.\"\\n',\n",
       " 'This planet is more massive than Earth\\n',\n",
       " 'and has a higher surface gravity.\\n',\n",
       " '\"Relax on Kepler-16b,\\n -16',\n",
       " 'where your shadow always has company.\"\\n',\n",
       " '(Laughter)\\n()',\n",
       " 'We know of a dozen planets that orbit two stars,\\n ',\n",
       " \"and there's likely many more out there.\\n\",\n",
       " 'If we could visit one of those planets,\\n',\n",
       " 'you literally would see two sunsets\\n',\n",
       " 'and have two shadows.\\n',\n",
       " 'So actually, science fiction got some things right.\\n ',\n",
       " 'Tatooine from Star Wars.\\n ',\n",
       " 'And I have a couple of other favorite exoplanets\\n',\n",
       " 'to tell you about.\\n',\n",
       " 'This one is Kepler-10b,\\n -10',\n",
       " \"it's a hot, hot planet.\\n \",\n",
       " 'It orbits over 50 times closer to its star\\n',\n",
       " 'than our Earth does to our sun.\\n 50 ',\n",
       " \"And actually, it's so hot,\\n\",\n",
       " \"we can't visit any of these planets, but if we could,\\n \",\n",
       " 'we would melt long before we got there.\\n',\n",
       " 'We think the surface is hot enough to melt rock\\n',\n",
       " 'and has liquid lava lakes.\\n',\n",
       " 'Gliese 1214b.\\n 1214 ',\n",
       " 'This planet, we know the mass and the size\\n ',\n",
       " 'and it has a fairly low density.\\n',\n",
       " \"It's somewhat warm.\\n\",\n",
       " \"We actually don't know really anything about this planet,\\n  \",\n",
       " \"but one possibility is that it's a water world,\\n \",\n",
       " \"like a scaled-up version of one of Jupiter's icy moons\\n \",\n",
       " 'that might be 50 percent water by mass.\\n 50',\n",
       " 'And in this case, it would have a thick steam atmosphere\\n',\n",
       " 'overlaying an ocean,\\n',\n",
       " 'not of liquid water,\\n',\n",
       " 'but of an exotic form of water, a superfluid --\\n ',\n",
       " 'not quite a gas, not quite a liquid.\\n ',\n",
       " \"And under that wouldn't be rock,\\n\",\n",
       " 'but a form of high-pressure ice,\\n',\n",
       " 'like ice IX.\\n  IX',\n",
       " 'So out of all these planets out there,\\n ',\n",
       " 'and the variety is just simply astonishing,\\n',\n",
       " 'we mostly want to find the planets that are Goldilocks planets, we call them.\\n',\n",
       " 'Not too big, not too small,\\n',\n",
       " 'not too hot, not too cold --\\n ',\n",
       " 'but just right for life.\\n',\n",
       " \"But to do that, we'd have to be able to look\\n\",\n",
       " \"at the planet's atmosphere,\\n \",\n",
       " 'because the atmosphere acts like a blanket trapping heat --\\n ',\n",
       " 'the greenhouse effect.\\n ',\n",
       " 'We have to be able to assess the greenhouse gases\\n',\n",
       " 'on other planets.\\n',\n",
       " 'Well, science fiction got some things wrong.\\n',\n",
       " 'The Star Trek Enterprise\\n ',\n",
       " 'had to travel vast distances at incredible speeds\\n',\n",
       " 'to orbit other planets\\n',\n",
       " 'so that First Officer Spock could analyze the atmosphere\\n',\n",
       " 'to see if the planet was habitable\\n',\n",
       " 'or if there were lifeforms there.\\n',\n",
       " \"Well, we don't need to travel at warp speeds\\n \",\n",
       " 'to see other planet atmospheres,\\n',\n",
       " \"although I don't want to dissuade any budding engineers\\n\",\n",
       " 'from figuring out how to do that.\\n',\n",
       " 'We actually can and do study planet atmospheres\\n ',\n",
       " 'from here, from Earth orbit.\\n ',\n",
       " 'This is a picture, a photograph of the Hubble Space Telescope\\n ',\n",
       " 'taken by the shuttle Atlantis as it was departing\\n ',\n",
       " 'after the last human space flight to Hubble.\\n ',\n",
       " 'They installed a new camera, actually,\\n',\n",
       " 'that we use for exoplanet atmospheres.\\n',\n",
       " \"And so far, we've been able to study dozens of exoplanet atmospheres,\\n \",\n",
       " 'about six of them in great detail.\\n',\n",
       " 'But those are not small planets like Earth.\\n  ',\n",
       " \"They're big, hot planets that are easy to see.\\n \",\n",
       " \"We're not ready,\\n\",\n",
       " \"we don't have the right technology yet to study small exoplanets.\\n \",\n",
       " 'But nevertheless,\\n',\n",
       " 'I wanted to try to explain to you how we study exoplanet atmospheres.\\n ',\n",
       " 'I want you to imagine, for a moment, a rainbow.\\n  ',\n",
       " 'And if we could look at this rainbow closely,\\n',\n",
       " 'we would see that some dark lines are missing.\\n',\n",
       " \"And here's our sun,\\n\",\n",
       " 'the white light of our sun split up,\\n',\n",
       " 'not by raindrops, but by a spectrograph.\\n ',\n",
       " 'And you can see all these dark, vertical lines.\\n',\n",
       " 'Some are very narrow, some are wide,\\n ',\n",
       " 'some are shaded at the edges.\\n',\n",
       " 'And this is actually how astronomers have studied objects in the heavens,\\n ',\n",
       " 'literally, for over a century.\\n',\n",
       " 'So here, each different atom and molecule\\n ',\n",
       " 'has a special set of lines,\\n ',\n",
       " 'a fingerprint, if you will.\\n',\n",
       " \"And that's how we study exoplanet atmospheres.\\n \",\n",
       " \"And I'll just never forget when I started working\\n \",\n",
       " 'on exoplanet atmospheres 20 years ago,\\n  20 ',\n",
       " 'how many people told me,\\n',\n",
       " '\"This will never happen.\\n',\n",
       " 'We\\'ll never be able to study them. Why are you bothering?\"\\n ',\n",
       " \"And that's why I'm pleased to tell you about all the atmospheres studied now,\\n  \",\n",
       " 'and this is really a field of its own.\\n',\n",
       " 'So when it comes to other planets, other Earths,\\n ',\n",
       " 'in the future when we can observe them,\\n',\n",
       " 'what kind of gases would we be looking for?\\n',\n",
       " 'Well, you know, our own Earth has oxygen in the atmosphere\\n  ',\n",
       " 'to 20 percent by volume.\\n 20 ',\n",
       " \"That's a lot of oxygen.\\n\",\n",
       " 'But without plants and photosynthetic life,\\n ',\n",
       " 'there would be no oxygen,\\n',\n",
       " 'virtually no oxygen in our atmosphere.\\n ',\n",
       " 'So oxygen is here because of life.\\n ',\n",
       " 'And our goal then is to look for gases in other planet atmospheres,\\n ',\n",
       " \"gases that don't belong,\\n\",\n",
       " 'that we might be able to attribute to life.\\n',\n",
       " 'But which molecules should we search for?\\n',\n",
       " 'I actually told you how diverse exoplanets are.\\n ',\n",
       " 'We expect that to continue in the future\\n',\n",
       " 'when we find other Earths.\\n',\n",
       " \"And that's one of the main things I'm working on now,\\n \",\n",
       " 'I have a theory about this.\\n',\n",
       " 'It reminds me that nearly every day,\\n ',\n",
       " 'I receive an email or emails\\n',\n",
       " 'from someone with a crazy theory about physics of gravity\\n ',\n",
       " 'or cosmology or some such.\\n ',\n",
       " \"So, please don't email me one of your crazy theories.\\n  \",\n",
       " '(Laughter)\\n()',\n",
       " 'Well, I had my own crazy theory.\\n',\n",
       " 'But, who does the MIT professor go to?\\n',\n",
       " 'Well, I emailed a Nobel Laureate in Physiology or Medicine\\n ',\n",
       " 'and he said, \"Sure, come and talk to me.\"\\n \" \"',\n",
       " 'So I brought my two biochemistry friends\\n',\n",
       " 'and we went to talk to him about our crazy theory.\\n ',\n",
       " 'And that theory was that life produces all small molecules,\\n  ',\n",
       " 'so many molecules.\\n',\n",
       " 'Like, everything I could think of, but not being a chemist.\\n ',\n",
       " 'Think about it:\\n',\n",
       " 'carbon dioxide, carbon monoxide,\\n ',\n",
       " 'molecular hydrogen, molecular nitrogen,\\n ',\n",
       " 'methane, methyl chloride --\\n ',\n",
       " 'so many gases.\\n',\n",
       " 'They also exist for other reasons,\\n',\n",
       " 'but just life even produces ozone.\\n',\n",
       " 'So we go to talk to him about this,\\n',\n",
       " 'and immediately, he shot down the theory.\\n',\n",
       " \"He found an example that didn't exist.\\n\",\n",
       " 'So, we went back to the drawing board\\n',\n",
       " 'and we think we have found something very interesting in another field.\\n',\n",
       " 'But back to exoplanets,\\n',\n",
       " 'the point is that life produces so many different types of gases,\\n ',\n",
       " 'literally thousands of gases.\\n ',\n",
       " \"And so what we're doing now is just trying to figure out\\n\",\n",
       " 'on which types of exoplanets,\\n',\n",
       " 'which gases could be attributed to life.\\n',\n",
       " 'And so when it comes time when we find gases\\n',\n",
       " 'in exoplanet atmospheres\\n',\n",
       " \"that we won't know if they're being produced\\n\",\n",
       " 'by intelligent aliens or by trees,\\n ',\n",
       " 'or a swamp,\\n',\n",
       " 'or even just by simple, single-celled microbial life.\\n',\n",
       " 'So working on the models\\n',\n",
       " 'and thinking about biochemistry,\\n',\n",
       " \"it's all well and good.\\n\",\n",
       " 'But a really big challenge ahead of us is: how?\\n  ',\n",
       " 'How are we going to find these planets?\\n',\n",
       " 'There are actually many ways to find planets,\\n',\n",
       " 'several different ways.\\n',\n",
       " \"But the one that I'm most focused on is how can we open a gateway\\n \",\n",
       " 'so that in the future,\\n',\n",
       " 'we can find hundreds of Earths.\\n ',\n",
       " 'We have a real shot at finding signs of life.\\n ',\n",
       " 'And actually, I just finished leading a two-year project\\n  ',\n",
       " 'in this very special phase\\n',\n",
       " 'of a concept we call the starshade.\\n ',\n",
       " 'And the starshade is a very specially shaped screen\\n',\n",
       " 'and the goal is to fly that starshade\\n',\n",
       " 'so it blocks out the light of a star\\n',\n",
       " 'so that the telescope can see the planets directly.\\n ',\n",
       " 'Here, you can see myself and two team members\\n ',\n",
       " 'holding up one small part of the starshade.\\n ',\n",
       " \"It's shaped like a giant flower,\\n\",\n",
       " 'and this is one of the prototype petals.\\n',\n",
       " 'The concept is that a starshade and telescope could launch together,\\n  ',\n",
       " 'with the petals unfurling from the stowed position.\\n',\n",
       " 'The central truss would expand,\\n',\n",
       " 'with the petals snapping into place.\\n',\n",
       " 'Now, this has to be made very precisely,\\n',\n",
       " 'literally, the petals to microns\\n ',\n",
       " 'and they have to deploy to millimeters.\\n',\n",
       " 'And this whole structure would have to fly\\n',\n",
       " 'tens of thousands of kilometers away from the telescope.\\n ',\n",
       " \"It's about tens of meters in diameter.\\n\",\n",
       " 'And the goal is to block out the starlight to incredible precision\\n ',\n",
       " \"so that we'd be able to see the planets directly.\\n\",\n",
       " 'And it has to be a very special shape,\\n',\n",
       " 'because of the physics of defraction.\\n',\n",
       " 'Now this is a real project that we worked on,\\n  ',\n",
       " 'literally, you would not believe how hard.\\n',\n",
       " \"Just so you believe it's not just in movie format,\\n\",\n",
       " \"here's a real photograph\\n\",\n",
       " 'of a second-generation starshade deployment test bed in the lab.\\n ',\n",
       " 'And in this case, I just wanted you to know\\n',\n",
       " 'that that central truss has heritage left over\\n',\n",
       " 'from large radio deployables in space.\\n',\n",
       " 'So after all of that hard work\\n',\n",
       " 'where we try to think of all the crazy gases that might be out there,\\n ',\n",
       " 'and we build the very complicated space telescopes\\n',\n",
       " 'that might be out there,\\n',\n",
       " 'what are we going to find?\\n',\n",
       " 'Well, in the best case,\\n',\n",
       " 'we will find an image of another exo-Earth.\\n',\n",
       " 'Here is Earth as a pale blue dot.\\n',\n",
       " 'And this is actually a real photograph of Earth\\n  ',\n",
       " 'taken by the Voyager 1 spacecraft,\\n 1',\n",
       " 'four billion miles away.\\n',\n",
       " 'And that red light is just scattered light in the camera optics.\\n',\n",
       " \"But what's so awesome to consider\\n\",\n",
       " 'is that if there are intelligent aliens\\n',\n",
       " 'orbiting on a planet around a star near to us\\n  ',\n",
       " 'and they build complicated space telescopes\\n',\n",
       " \"of the kind that we're trying to build,\\n\",\n",
       " \"all they'll see is this pale blue dot,\\n \",\n",
       " 'a pinprick of light.\\n',\n",
       " 'And so sometimes, when I pause to think\\n ',\n",
       " 'about my professional struggle and huge ambition,\\n ',\n",
       " \"it's hard to think about that\\n\",\n",
       " 'in contrast to the vastness of the universe.\\n',\n",
       " 'But nonetheless, I am devoting the rest of my life\\n ',\n",
       " 'to finding another Earth.\\n',\n",
       " 'And I can guarantee\\n',\n",
       " 'that in the next generation of space telescopes,\\n',\n",
       " 'in the second generation,\\n',\n",
       " 'we will have the capability to find and identity other Earths.\\n  ',\n",
       " 'And the capability to split up the starlight\\n',\n",
       " 'so that we can look for gases\\n',\n",
       " 'and assess the greenhouse gases in the atmosphere,\\n',\n",
       " 'estimate the surface temperature,\\n',\n",
       " 'and look for signs of life.\\n',\n",
       " \"But there's more.\\n\",\n",
       " 'In this case of searching for other planets like Earth,\\n ',\n",
       " 'we are making a new kind of map\\n',\n",
       " 'of the nearby stars and of the planets orbiting them,\\n  ',\n",
       " 'including [planets] that actually might be inhabitable by humans.\\n ',\n",
       " 'And so I envision that our descendants,\\n',\n",
       " 'hundreds of years from now,\\n',\n",
       " 'will embark on an interstellar journey to other worlds.\\n ',\n",
       " 'And they will look back at all of us\\n',\n",
       " 'as the generation who first found the Earth-like worlds.\\n',\n",
       " 'Thank you.\\n',\n",
       " '(Applause)\\n()',\n",
       " 'June Cohen: And I give you, for a question,\\n  :  ',\n",
       " 'Rosetta Mission Manager Fred Jansen.\\n  ',\n",
       " 'Fred Jansen: You mentioned halfway through\\n  : ',\n",
       " 'that the technology to actually look at the spectrum\\n ',\n",
       " 'of an exoplanet like Earth is not there yet.\\n',\n",
       " 'When do you expect this will be there,\\n',\n",
       " \"and what's needed?\\n\",\n",
       " 'Actually, what we expect is what we call our next-generation Hubble telescope.\\n  ',\n",
       " 'And this is called the James Webb Space Telescope,\\n ',\n",
       " 'and that will launch in 2018,\\n .. 2018',\n",
       " \"and that's what we're going to do,\\n\",\n",
       " \"we're going to look at a special kind of planet\\n  \",\n",
       " 'called transient exoplanets,\\n  (transient exoplanet)',\n",
       " 'and that will be our first shot at studying small planets\\n ',\n",
       " 'for gases that might indicate the planet is habitable.\\n ',\n",
       " \"JC: I'm going to ask you one follow-up question, too, Sara,\\n.. :    \",\n",
       " 'as the generalist.\\n',\n",
       " 'So I am really struck by the notion in your career\\n ',\n",
       " 'of the opposition you faced,\\n',\n",
       " 'that when you began thinking about exoplanets,\\n ',\n",
       " 'there was extreme skepticism in the scientific community\\n ',\n",
       " 'that they existed,\\n',\n",
       " 'and you proved them wrong.\\n',\n",
       " 'What did it take to take that on?\\n',\n",
       " 'SS: Well, the thing is that as scientists,\\n',\n",
       " \"we're supposed to be skeptical,\\n\",\n",
       " 'because our job to make sure that what the other person is saying\\n ',\n",
       " 'actually makes sense or not.\\n',\n",
       " 'But being a scientist,\\n',\n",
       " \"I think you've seen it from this session,\\n\",\n",
       " \"it's like being an explorer.\\n\",\n",
       " 'You have this immense curiosity,\\n',\n",
       " 'this stubbornness,\\n',\n",
       " 'this sort of resolute will that you will go forward\\n',\n",
       " 'no matter what other people say.\\n',\n",
       " 'JC: I love that. Thank you, Sara.\\n.. :   ',\n",
       " '(Applause)\\n()']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
